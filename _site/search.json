[
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the modules of our proposed Shiny application (Group Project) and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for our Shiny application are supported in R CRAN,\nTo prepare and test that the specific R codes can run and returns the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications,\nTo select the appropriate Shiny UI components for exposing the parameters determined above, and\nTo include a section called UI design for the different components of the UIs for the proposed design."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#loading-r-packages",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#loading-r-packages",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.2.1 Loading R packages",
    "text": "4.2.1 Loading R packages\nThe below R packages will be used in this exercise and for the Shiny application\n\npacman::p_load(sf, tidyverse, tmap, dplyr,\n               spatstat, spdep,\n               lubridate, leaflet,\n               plotly, DT, viridis,\n               ggplot2, sfdep)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#importing-and-loading-the-acled-data",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#importing-and-loading-the-acled-data",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.2.2 Importing and loading the ACLED data",
    "text": "4.2.2 Importing and loading the ACLED data\nCountry specific data from the Armed Conflict Location & Event Data Project (ACLED) can be downloaded at https://acleddata.com/data-export-tool/\nLoading the ACLED data set for Myanmar.\n\nACLED_MMR &lt;- read_csv(\"data/MMR.csv\")"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#downloading-and-loading-the-shape-files-for-country",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#downloading-and-loading-the-shape-files-for-country",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.2.3 Downloading and loading the shape files for country",
    "text": "4.2.3 Downloading and loading the shape files for country\nShape files were downloaded from the Myanmmar Information Management Unit (MIMU) website.\nI chose this source over GADM and GeoBoundaries due to its updated administrative region information and map levels.\n\n\n\n\n\n\nNote - Data Quality Issues\n\n\n\nACLED captures event data from national, sub-national and other media sources, and populates event locations based on the last known information.\n\nHowever, some names of administrative areas were found to have changed; either disaggregated into new administrative areas or previously active but now defunct. Some administrative areas were also aggregated into higher administrative areas.\nAs part of our data cleaning and preparation process, I had to identify discrepancies in both admin1 & 2 (administrative levels) and re-name some administrative areas to sync with the downloaded shape files from MIMU."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#loading-admin-1-2-administrative-regionarea-shape-files",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#loading-admin-1-2-administrative-regionarea-shape-files",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.3.1 Loading Admin 1 & 2 (administrative region/area) shape files",
    "text": "4.3.1 Loading Admin 1 & 2 (administrative region/area) shape files\n\nmmr_shp_mimu_1 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda2_adm1_250k_mimu_1\")\n\nReading layer `mmr_polbnda2_adm1_250k_mimu_1' from data source \n  `C:\\imranmi\\ISSS608-VAA\\Take-home-ex\\Take-home-Ex4a\\data\\geospatial3' \n  using driver `ESRI Shapefile'\nSimple feature collection with 18 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\nmmr_shp_mimu_2 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\imranmi\\ISSS608-VAA\\Take-home-ex\\Take-home-Ex4a\\data\\geospatial3' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nclass(mmr_shp_mimu_2)\n\n[1] \"sf\"         \"data.frame\"\n\n\nThe Shape file for admin2 level map, is an SF object, with geometry type: Multipolygon.\n\nst_geometry(mmr_shp_mimu_2)\n\nGeometry set for 80 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\n\nst_crs(mmr_shp_mimu_2)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]]]\n\n\nNext, I will check the unique district names in this shape file (admin2)\n\nunique_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\nunique_regions_mimu2\n\n [1] \"Hinthada\"                        \"Labutta\"                        \n [3] \"Maubin\"                          \"Myaungmya\"                      \n [5] \"Pathein\"                         \"Pyapon\"                         \n [7] \"Bago\"                            \"Taungoo\"                        \n [9] \"Pyay\"                            \"Thayarwady\"                     \n[11] \"Falam\"                           \"Hakha\"                          \n[13] \"Matupi\"                          \"Mindat\"                         \n[15] \"Bhamo\"                           \"Mohnyin\"                        \n[17] \"Myitkyina\"                       \"Puta-O\"                         \n[19] \"Bawlake\"                         \"Loikaw\"                         \n[21] \"Hpa-An\"                          \"Hpapun\"                         \n[23] \"Kawkareik\"                       \"Myawaddy\"                       \n[25] \"Gangaw\"                          \"Magway\"                         \n[27] \"Minbu\"                           \"Pakokku\"                        \n[29] \"Thayet\"                          \"Kyaukse\"                        \n[31] \"Maungdaw\"                        \"Mrauk-U\"                        \n[33] \"Sittwe\"                          \"Thandwe\"                        \n[35] \"Hkamti\"                          \"Kale\"                           \n[37] \"Kanbalu\"                         \"Katha\"                          \n[39] \"Kawlin\"                          \"Mawlaik\"                        \n[41] \"Monywa\"                          \"Naga Self-Administered Zone\"    \n[43] \"Sagaing\"                         \"Shwebo\"                         \n[45] \"Tamu\"                            \"Yinmarbin\"                      \n[47] \"Kengtung\"                        \"Monghsat\"                       \n[49] \"Tachileik\"                       \"Hopang\"                         \n[51] \"Kokang Self-Administered Zone\"   \"Kyaukme\"                        \n[53] \"Lashio\"                          \"Matman\"                         \n[55] \"Mongmit\"                         \"Muse\"                           \n[57] \"Pa Laung Self-Administered Zone\" \"Danu Self-Administered Zone\"    \n[59] \"Langkho\"                         \"Loilen\"                         \n[61] \"Pa-O Self-Administered Zone\"     \"Taunggyi\"                       \n[63] \"Dawei\"                           \"Kawthoung\"                      \n[65] \"Mandalay\"                        \"Meiktila\"                       \n[67] \"Myingyan\"                        \"Nyaung-U\"                       \n[69] \"Pyinoolwin\"                      \"Yamethin\"                       \n[71] \"Mawlamyine\"                      \"Thaton\"                         \n[73] \"Det Khi Na\"                      \"Oke Ta Ra\"                      \n[75] \"Kyaukpyu\"                        \"Myeik\"                          \n[77] \"Yangon (East)\"                   \"Yangon (North)\"                 \n[79] \"Yangon (South)\"                  \"Yangon (West)\"                  \n\n\nThere are 80 admin2 levels or districts in mmr_shp_mimu_2\nLets compare with our admin2 levels in our main dataset ACLED_MMR\n\nunique_acled_regions2 &lt;- unique(ACLED_MMR$admin2)\n\nunique_acled_regions2\n\n [1] \"Maungdaw\"                        \"Bago\"                           \n [3] \"Shwebo\"                          \"Kyaukme\"                        \n [5] \"Pyinoolwin\"                      \"Muse\"                           \n [7] \"Sittwe\"                          \"Yinmarbin\"                      \n [9] \"Thaton\"                          \"Yangon-North\"                   \n[11] \"Pa-O Self-Administered Zone\"     \"Hpapun\"                         \n[13] \"Kyaukpyu\"                        \"Yangon-West\"                    \n[15] \"Mongmit\"                         \"Bhamo\"                          \n[17] \"Mrauk-U\"                         \"Yangon-East\"                    \n[19] \"Yangon-South\"                    \"Monywa\"                         \n[21] \"Gangaw\"                          \"Pathein\"                        \n[23] \"Katha\"                           \"Taungoo\"                        \n[25] \"Kanbalu\"                         \"Lashio\"                         \n[27] \"Mawlamyine\"                      \"Myitkyina\"                      \n[29] \"Kawkareik\"                       \"Loilen\"                         \n[31] \"Mandalay\"                        \"Kawlin\"                         \n[33] \"Kyaukse\"                         \"Magway\"                         \n[35] \"Meiktila\"                        \"Pakokku\"                        \n[37] \"Taunggyi\"                        \"Tamu\"                           \n[39] \"Nay Pyi Taw\"                     \"Mohnyin\"                        \n[41] \"Kale\"                            \"Det Khi Na\"                     \n[43] \"Myingyan\"                        \"Loikaw\"                         \n[45] \"Matupi\"                          \"Pyay\"                           \n[47] \"Sagaing\"                         \"Myeik\"                          \n[49] \"Dawei\"                           \"Thayarwady\"                     \n[51] \"Thandwe\"                         \"Mawlaik\"                        \n[53] \"Bawlake\"                         \"Pyapon\"                         \n[55] \"Hinthada\"                        \"Thayet\"                         \n[57] \"Pa Laung Self-Administered Zone\" \"Mindat\"                         \n[59] \"Hkamti\"                          \"Kokang Self-Administered Zone\"  \n[61] \"Hpa-An\"                          \"Danu Self-Administered Zone\"    \n[63] \"Myawaddy\"                        \"Maubin\"                         \n[65] \"Hakha\"                           \"Falam\"                          \n[67] \"Minbu\"                           \"Monghsat\"                       \n[69] \"Puta-O\"                          \"Hopang\"                         \n[71] \"Nyaung-U\"                        \"Kawthoung\"                      \n[73] \"Yamethin\"                        \"Yangon\"                         \n[75] \"Myaungmya\"                       \"Mong Pawk (Wa SAD)\"             \n[77] \"Oke Ta Ra\"                       \"Matman\"                         \n[79] \"Kengtung\"                        \"Naga Self-Administered Zone\"    \n[81] \"Labutta\"                         \"Langkho\"                        \n[83] \"Tachileik\"                      \n\n\nI will write a simple function below to identify the discrepancies between the shape file and our state/district names in our main dataset.\n\n# Find the unique region names that are in 'unique_acled_regions2' but not in 'unique_regions_mimu2'\n\nmismatched_admin2 &lt;- setdiff(unique_acled_regions2, unique_regions_mimu2)\n\nif (length(mismatched_admin2) &gt; 0) {\n  print(\"The following region names from 'acled_mmr' do not match any in 'mimu2':\")\n  print(mismatched_admin2)\n} else {\n  print(\"All unique region names in 'acled_mmr' match the unique region names in 'mimu2.'\")\n}\n\n[1] \"The following region names from 'acled_mmr' do not match any in 'mimu2':\"\n[1] \"Yangon-North\"       \"Yangon-West\"        \"Yangon-East\"       \n[4] \"Yangon-South\"       \"Nay Pyi Taw\"        \"Yangon\"            \n[7] \"Mong Pawk (Wa SAD)\"\n\n\nLets harmonize the names in both data files. I will re-save it to a new data set called ACLED_MMR_1\nFixing our admin 1 names.\n\nACLED_MMR_1 &lt;- ACLED_MMR %&gt;%\n  mutate(admin1 = case_when(\n    admin1 == \"Bago-East\" ~ \"Bago (East)\",\n    admin1 == \"Bago-West\" ~ \"Bago (West)\",\n    admin1 == \"Shan-North\" ~ \"Shan (North)\",\n    admin1 == \"Shan-South\" ~ \"Shan (South)\",\n    admin1 == \"Shan-East\" ~ \"Shan (East)\",\n    TRUE ~ as.character(admin1)\n  ))\n\nFixing our admin 2 names.\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  mutate(admin2 = case_when(\n    admin2 == \"Yangon-East\" ~ \"Yangon (East)\",\n    admin2 == \"Yangon-West\" ~ \"Yangon (West)\",\n    admin2 == \"Yangon-North\" ~ \"Yangon (North)\",\n    admin2 == \"Yangon-South\" ~ \"Yangon (South)\",\n    admin2 == \"Mong Pawk (Wa SAD)\" ~ \"Tachileik\",\n    admin2 == \"Nay Pyi Taw\" ~ \"Det Khi Na\",\n    admin2 == \"Yangon\" ~ \"Yangon (West)\",\n    TRUE ~ as.character(admin2)\n  ))\n\nChecking if our changes are successful.\n\n# Get unique admin 2 district names from 'ACLED_MMR_1'\nunique_acled_regions2 &lt;- unique(ACLED_MMR_1$admin2)\n\n# Get unique district names from 'mmr_shp_mimu_2'\nunique_map_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\n# Find the unique district names that are in 'unique_acled_regions2' but not in 'unique_map_regions_mimu2'\n\nmismatched_regions2 &lt;- setdiff(unique_acled_regions2, unique_map_regions_mimu2)\n\nif (length(mismatched_regions2) &gt; 0) {\n  print(\"The following district names from 'acled_mmr_1' do not match any in 'mmr_shp_mimu_2':\")\n  print(mismatched_regions2)\n} else {\n  print(\"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\")\n}\n\n[1] \"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\"\n\n\nLets do a sample plot to see how our country map looks like at the admin2 (districts) level.\n\nplot(mmr_shp_mimu_2)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#data-wrangling",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#data-wrangling",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.3.2 Data Wrangling",
    "text": "4.3.2 Data Wrangling\nFor the purposes of plotting choropleth maps, I will first create attributes subsets to summarise the number of incidents and fatalities, grouped by year, admin region, event type and sub event type.\n\nData2 &lt;- ACLED_MMR_1 %&gt;%\n    group_by(year, admin2, event_type, sub_event_type) %&gt;%\n    summarise(Incidents = n(),\n              Fatalities = sum(fatalities, na.rm = TRUE)) %&gt;%\n              \n    ungroup()\n\nChecking the total sum of incidents and fatalities\n\ntotal_incidents2 &lt;- sum(Data2$Incidents)\ntotal_fatalities2 &lt;- sum(Data2$Fatalities)\n\ntotal_incidents2\n\n[1] 57198\n\ntotal_fatalities2\n\n[1] 57593\n\n\nNext, I will do a spatial join between my shape files and attribute files\n\nACLED_MMR_admin2 &lt;- left_join(mmr_shp_mimu_2, Data2,\n                            by = c(\"DT\" = \"admin2\"))\n\nRemoving the variables I don’t require.\n\nACLED_MMR_admin2 &lt;- ACLED_MMR_admin2 %&gt;%\n                      select(-OBJECTID, -ST, -ST_PCODE)\n\n\nclass(ACLED_MMR_admin2)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNext, I will just double check that total sum of incidents and fatalities in our SF files are correct as per our original datasets.\n\ntotal_incidents_check &lt;- sum(ACLED_MMR_admin2$Incidents)\ntotal_fatalities_check &lt;- sum(ACLED_MMR_admin2$Fatalities)\n\ntotal_incidents_check \n\n[1] 57198\n\ntotal_fatalities_check\n\n[1] 57593"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.4.1 Choropleth map of Incidents & Fatalities by Admin2 level (by District)",
    "text": "4.4.1 Choropleth map of Incidents & Fatalities by Admin2 level (by District)\nThe below codes will be used to create the choropleth maps.\n\nFatalities in Battles in 2023, by Districts (Quantile)Incidents of Violence against civilians in 2021, by Districts (Equal)\n\n\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Battles\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2021, event_type == \"Violence against civilians\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Incidents\",\n          n = 5,\n          style = \"equal\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nAdding Interactivity by using tmap_leaftlet()\n\ndata_filtered &lt;- ACLED_MMR_admin2 %&gt;%\n  filter(year == 2022, event_type == \"Battles\")\n\ntm_map &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(data_filtered) +\n  tm_fill(col = \"Incidents\", n = 5, style = \"equal\", palette = \"Reds\", title = \"Incidents\") +\n  tm_borders(alpha = 0.5)\n\ntmap_leaflet(tm_map)\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nFrom the codes above, below are the variables we can expose as user inputs:-\n\nYear\nEvent type (Battles, Violence against civilians, protests, riots, explosions/remote violence)\nCount type: number of Incidents or Fatalities\nData classification type: eg quantile, equal, jenks, kmeans, pretty etc"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#visualising-the-location-of-conflict-events",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#visualising-the-location-of-conflict-events",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.5.1 Visualising the location of conflict events",
    "text": "4.5.1 Visualising the location of conflict events\nUsing the leaflet package, I will use the Geometry points from our SF data sets to plot the points of event types in the maps.\nIn this case, I will use admin 1 level regions, to achieve a better aesthetics for users.\nThis is because, visually dividing the country map into more smaller districts (admin2) would likely make the map look “too busy”.\n\nBattles from 2010 to presentViolence against civilians from 2010 to presentProtests from 2010 to present\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Battles) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Battles&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Violence_CV) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Violence on Civillians&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Protests) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Protests&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nThe plots above are sufficiently interactive as users can hover to any “circle” to get more information on the event and location.\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#filtering-the-event-and-year-event-type-battles-in-2023",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#filtering-the-event-and-year-event-type-battles-in-2023",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.7.1 Filtering the Event and Year (Event type = Battles, in 2023)",
    "text": "4.7.1 Filtering the Event and Year (Event type = Battles, in 2023)\nThe below subset will serve as our reference data subset for our subsequent codes.\n\nBattles_2023 &lt;- Events_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Battles\")"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-contiguity-spatial-weights",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-contiguity-spatial-weights",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.7.2 Computing Contiguity Spatial Weights",
    "text": "4.7.2 Computing Contiguity Spatial Weights\nBefore we can compute any spatial statistics, we need to construct spatial weights of the study area.\nThe spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. admin2) in the study area (Myanmar).\nIn the code below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\nBy default this function will return a list of first order neighbours using the Queen criteria.\nHowever, we can also pass a “queen” argument that takes TRUE or FALSE as options.\n\nwm_q &lt;- poly2nb(Battles_2023, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 356 \nPercentage nonzero weights: 6.501096 \nAverage number of links: 4.810811 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8 10 \n 3  8 11 11 15  9  9  6  2 \n3 least connected regions:\n2 16 58 with 1 link\n2 most connected regions:\n19 56 with 10 links\n\n\nThe summary report above shows that there are 74 area units for this subset (Battles occurring in 2023).\n\nThere are 2 most connected area units with 10 neighbours, and there are 3 area units with only 1 neighbour."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#row-standardised-weights-matrix",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#row-standardised-weights-matrix",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.7.3 Row-standardised weights matrix",
    "text": "4.7.3 Row-standardised weights matrix\nNext, we assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring admin2 (district) and then summing the weighted income values.\nThis has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons and thus potentially over or under estimating the true nature of the spatial autocorrelation in the data.\nHowever, for this example, I will stick with the style=“W” option for simplicity’s sake. Other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 356 \nPercentage nonzero weights: 6.501096 \nAverage number of links: 4.810811 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 74 5476 74 36.81702 310.0455"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-local-morans-i",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-local-morans-i",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.8.1 Computing Local Moran’s I",
    "text": "4.8.1 Computing Local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a list object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips &lt;- order(Battles_2023$DT)\nlocalMI &lt;- localmoran(Battles_2023$Incidents, rswm_q)\nhead(localMI)\n\n           Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.46274887 -9.006432e-03 0.1247560902  1.3356292      0.1816705\n2  0.71317847 -1.016877e-02 0.7448368248  0.8381394      0.4019524\n3  0.69218038 -9.386041e-03 0.3392457987  1.2045132      0.2283913\n4  0.68518101 -9.386041e-03 0.3392457987  1.1924960      0.2330668\n5  0.00627746 -1.483579e-05 0.0001702656  0.4822206      0.6296493\n6 -0.23004674 -4.814215e-03 0.0464274459 -1.0453066      0.2958813\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code below lists the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=Battles_2023$DT[fips]),\n  check.names=FALSE)\n\n                                         Ii        E.Ii      Var.Ii        Z.Ii\nBago                             6.2775e-03 -1.4836e-05  1.7027e-04  4.8222e-01\nBawlake                         -1.7909e-02 -4.6941e-04  1.1252e-02 -1.6441e-01\nBhamo                            2.4621e-01 -1.7367e-03  1.9897e-02  1.7577e+00\nDanu Self-Administered Zone      3.3657e-01 -8.2707e-03  1.9670e-01  7.7752e-01\nDawei                            9.4559e-01 -1.4130e-02  5.0825e-01  1.3462e+00\nDet Khi Na                       2.3024e-01 -6.5686e-03  6.3234e-02  9.4170e-01\nFalam                           -7.5203e-03 -2.4381e-03  5.8326e-02 -2.1044e-02\nGangaw                           6.2609e-01 -6.9289e-03  6.6679e-02  2.4514e+00\nHakha                           -1.1347e-02 -6.1003e-05  1.0815e-03 -3.4318e-01\nHinthada                         4.6275e-01 -9.0064e-03  1.2476e-01  1.3356e+00\nHkamti                          -5.3716e-02 -3.0598e-03  3.5009e-02 -2.7074e-01\nHopang                          -2.3647e-01 -9.7735e-03  3.5311e-01 -3.8150e-01\nHpa-An                          -1.3231e-01 -3.0598e-03  1.9751e-02 -9.1968e-01\nHpapun                          -4.1969e-02 -4.2526e-03  5.9189e-02 -1.5503e-01\nKale                             1.2532e-01 -7.7384e-04  6.4571e-03  1.5692e+00\nKanbalu                         -3.2905e-02 -3.4002e-05  3.9022e-04 -1.6640e+00\nKatha                           -8.8073e-03 -1.8682e-02  1.5309e-01  2.5238e-02\nKawkareik                        2.9062e-02 -1.3663e-02  3.2318e-01  7.5156e-02\nKawlin                          -7.2541e-02 -1.4063e-03  3.3678e-02 -3.8762e-01\nKawthoung                       -6.7102e-01 -3.2826e-03  2.4212e-01 -1.3570e+00\nKokang Self-Administered Zone    9.6449e-04 -1.1447e-08  2.7452e-07  1.8408e+00\nKyaukme                         -6.9939e-02 -9.0471e-03  8.6877e-02 -2.0659e-01\nKyaukpyu                         4.7922e-01 -6.8933e-03  1.2137e-01  1.3953e+00\nKyaukse                         -1.4472e-01 -9.0064e-03  7.4533e-02 -4.9713e-01\nLabutta                          7.1318e-01 -1.0169e-02  7.4484e-01  8.3814e-01\nLangkho                         -9.6769e-03 -8.6347e-03  2.0528e-01 -2.3004e-03\nLashio                           2.0219e-01 -4.2805e-03  4.8917e-02  9.3352e-01\nLoikaw                          -4.7652e-01 -2.3869e-02  3.2568e-01 -7.9318e-01\nLoilen                          -2.8804e-02 -5.6413e-03  7.8408e-02 -8.2718e-02\nMagway                           9.4077e-02 -5.9426e-03  4.9330e-02  4.5033e-01\nMandalay                        -2.2955e-01 -3.0598e-03  7.3153e-02 -8.3742e-01\nMatupi                          -1.7986e-02 -3.2117e-04  4.4878e-03 -2.6370e-01\nMaungdaw                         1.2575e-01 -2.6375e-03  6.3084e-02  5.1117e-01\nMawlaik                         -3.2557e-02 -2.6375e-03  3.0190e-02 -1.7220e-01\nMawlamyine                       2.4766e-01 -3.3072e-03  5.8440e-02  1.0381e+00\nMeiktila                         3.8976e-01 -8.2707e-03  7.9484e-02  1.4118e+00\nMinbu                           -1.9321e-01 -6.2516e-03  6.0203e-02 -7.6195e-01\nMindat                           2.2211e-02 -8.7518e-04  1.5503e-02  1.8541e-01\nMohnyin                          1.1002e-01 -8.8788e-04  1.5727e-02  8.8440e-01\nMongmit                         -4.9751e-01 -8.6347e-03  1.1965e-01 -1.4133e+00\nMonywa                           3.7540e+00 -3.3925e-02  5.8106e-01  4.9692e+00\nMrauk-U                          2.1574e-01 -3.9983e-03  4.5705e-02  1.0278e+00\nMuse                             2.6578e-01 -1.6313e-01  2.4204e+00  2.7569e-01\nMyawaddy                         1.8035e-02 -6.4391e-05  2.3492e-03  3.7341e-01\nMyeik                            3.6057e-01 -2.5739e-02  9.1496e-01  4.0386e-01\nMyingyan                         4.3732e-02 -1.0008e-04  1.3987e-03  1.1720e+00\nMyitkyina                       -1.9413e-01 -6.2855e-03  8.7306e-02 -6.3572e-01\nNaga Self-Administered Zone     -8.8212e-02 -1.0169e-02  3.6725e-01 -1.2878e-01\nNyaung-U                        -5.4115e-01 -8.6347e-03  1.5176e-01 -1.3669e+00\nOke Ta Ra                        5.7519e-01 -9.0064e-03  1.5824e-01  1.4686e+00\nPa-O Self-Administered Zone      1.9941e-01 -5.6413e-03  5.4359e-02  8.7950e-01\nPa Laung Self-Administered Zone -5.3309e-01 -5.0623e-03  7.0402e-02 -1.9901e+00\nPakokku                          1.9341e+00 -2.2766e-01  1.4683e+00  1.7840e+00\nPathein                          6.9218e-01 -9.3860e-03  3.3925e-01  1.2045e+00\nPuta-O                          -4.8052e-01 -6.8933e-03  5.0659e-01 -6.6543e-01\nPyapon                           6.8518e-01 -9.3860e-03  3.3925e-01  1.1925e+00\nPyay                             1.0545e-01 -1.2618e-03  1.7615e-02  8.0407e-01\nPyinoolwin                       4.6151e-01 -2.7025e-02  2.1958e-01  1.0426e+00\nSagaing                          9.5824e-01 -1.0212e-02  9.7948e-02  3.0944e+00\nShwebo                           2.1412e+00 -5.0075e-02  5.4593e-01  2.9657e+00\nSittwe                           2.3135e-01 -3.0598e-03  1.1130e-01  7.0266e-01\nTamu                             3.2174e-02 -1.8902e-04  3.3506e-03  5.5911e-01\nTaunggyi                        -1.0168e-01 -1.1395e-03  7.3697e-03 -1.1711e+00\nTaungoo                         -2.3005e-01 -4.8142e-03  4.6427e-02 -1.0453e+00\nThandwe                          5.6456e-01 -1.0169e-02  1.4069e-01  1.5322e+00\nThaton                          -6.7767e-02 -3.0835e-03  5.4499e-02 -2.7708e-01\nThayarwady                       9.0973e-03 -1.4836e-05  2.0737e-04  6.3277e-01\nThayet                           2.9530e-01 -5.3479e-03  5.1546e-02  1.3242e+00\nYamethin                         4.3920e-01 -9.7735e-03  1.3528e-01  1.2207e+00\nYangon (East)                    6.1100e-01 -7.5664e-03  1.8008e-01  1.4576e+00\nYangon (North)                   4.4954e-01 -9.3860e-03  1.0671e-01  1.4049e+00\nYangon (South)                   5.2023e-01 -8.6347e-03  1.1965e-01  1.5289e+00\nYangon (West)                    6.6585e-01 -9.7735e-03  2.3209e-01  1.4024e+00\nYinmarbin                        4.5788e+00 -9.9115e-02  1.2481e+00  4.1872e+00\n                                Pr.z....E.Ii..\nBago                                    0.6296\nBawlake                                 0.8694\nBhamo                                   0.0788\nDanu Self-Administered Zone             0.4369\nDawei                                   0.1782\nDet Khi Na                              0.3463\nFalam                                   0.9832\nGangaw                                  0.0142\nHakha                                   0.7315\nHinthada                                0.1817\nHkamti                                  0.7866\nHopang                                  0.7028\nHpa-An                                  0.3577\nHpapun                                  0.8768\nKale                                    0.1166\nKanbalu                                 0.0961\nKatha                                   0.9799\nKawkareik                               0.9401\nKawlin                                  0.6983\nKawthoung                               0.1748\nKokang Self-Administered Zone           0.0656\nKyaukme                                 0.8363\nKyaukpyu                                0.1629\nKyaukse                                 0.6191\nLabutta                                 0.4020\nLangkho                                 0.9982\nLashio                                  0.3506\nLoikaw                                  0.4277\nLoilen                                  0.9341\nMagway                                  0.6525\nMandalay                                0.4024\nMatupi                                  0.7920\nMaungdaw                                0.6092\nMawlaik                                 0.8633\nMawlamyine                              0.2992\nMeiktila                                0.1580\nMinbu                                   0.4461\nMindat                                  0.8529\nMohnyin                                 0.3765\nMongmit                                 0.1576\nMonywa                                  0.0000\nMrauk-U                                 0.3040\nMuse                                    0.7828\nMyawaddy                                0.7088\nMyeik                                   0.6863\nMyingyan                                0.2412\nMyitkyina                               0.5250\nNaga Self-Administered Zone             0.8975\nNyaung-U                                0.1716\nOke Ta Ra                               0.1419\nPa-O Self-Administered Zone             0.3791\nPa Laung Self-Administered Zone         0.0466\nPakokku                                 0.0744\nPathein                                 0.2284\nPuta-O                                  0.5058\nPyapon                                  0.2331\nPyay                                    0.4214\nPyinoolwin                              0.2972\nSagaing                                 0.0020\nShwebo                                  0.0030\nSittwe                                  0.4823\nTamu                                    0.5761\nTaunggyi                                0.2415\nTaungoo                                 0.2959\nThandwe                                 0.1255\nThaton                                  0.7817\nThayarwady                              0.5269\nThayet                                  0.1854\nYamethin                                0.2222\nYangon (East)                           0.1449\nYangon (North)                          0.1601\nYangon (South)                          0.1263\nYangon (West)                           0.1608\nYinmarbin                               0.0000"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#mapping-the-local-morans-i",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#mapping-the-local-morans-i",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.8.2 Mapping the Local Moran’s I",
    "text": "4.8.2 Mapping the Local Moran’s I\nBefore mapping the local Moran’s I map, I will need to append the local Moran’s I dataframe (i.e. localMI) onto the Battles_2023’s SF DataFrame.\n\nBattles_2023.localMI &lt;- cbind(Battles_2023,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\nBattles_2023.localMI\n\nSimple feature collection with 74 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           DT   DT_PCODE    DT_MMR PCode_V year event_type Incidents Fatalities\n1    Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023    Battles         4          3\n2     Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023    Battles         1          1\n3     Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023    Battles         3          1\n4      Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023    Battles         3          2\n5        Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023    Battles        50        270\n6     Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023    Battles        87        493\n7        Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023    Battles        34         59\n8  Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023    Battles        50         93\n9       Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023    Battles        27         89\n10      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023    Battles        48        210\n             Ii          E.Ii       Var.Ii        Z.Ii     Pr.Ii\n1   0.462748867 -9.006432e-03 0.1247560902  1.33562921 0.1816705\n2   0.713178468 -1.016877e-02 0.7448368248  0.83813940 0.4019524\n3   0.692180375 -9.386041e-03 0.3392457987  1.20451317 0.2283913\n4   0.685181011 -9.386041e-03 0.3392457987  1.19249602 0.2330668\n5   0.006277460 -1.483579e-05 0.0001702656  0.48222056 0.6296493\n6  -0.230046740 -4.814215e-03 0.0464274459 -1.04530664 0.2958813\n7   0.105454317 -1.261774e-03 0.0176145487  0.80407054 0.4213562\n8   0.009097304 -1.483579e-05 0.0002073682  0.63277490 0.5268807\n9  -0.007520292 -2.438086e-03 0.0583263538 -0.02104359 0.9832109\n10 -0.011346560 -6.100301e-05 0.0010814666 -0.34317564 0.7314663\n                         geometry\n1  MULTIPOLYGON (((95.12637 18...\n2  MULTIPOLYGON (((95.04462 15...\n3  MULTIPOLYGON (((94.27572 15...\n4  MULTIPOLYGON (((95.20798 15...\n5  MULTIPOLYGON (((95.90674 18...\n6  MULTIPOLYGON (((96.17964 19...\n7  MULTIPOLYGON (((95.70458 19...\n8  MULTIPOLYGON (((95.85173 18...\n9  MULTIPOLYGON (((93.36931 24...\n10 MULTIPOLYGON (((93.35213 23..."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#mapping-local-morans-i-values",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#mapping-local-morans-i-values",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.8.3 Mapping Local Moran’s I values",
    "text": "4.8.3 Mapping Local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code below.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#mapping-local-morans-i-p-values",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#mapping-local-morans-i-p-values",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.8.4 Mapping Local Moran’s I p-values",
    "text": "4.8.4 Mapping Local Moran’s I p-values\nThe choropleth above shows there is evidence for both positive and negative Ii values. However, we will also need to consider the p-values for each of these values.\nThe code below produces a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#data-table-for-the-morans-i-values",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#data-table-for-the-morans-i-values",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.8.6 Data table for the Moran’s I values",
    "text": "4.8.6 Data table for the Moran’s I values\nFor the sake of readability, it may also be a good idea to add a data table of the values, for users to make sense of both maps.\nThe below code will be used to generate the data table.\n\ndatatable(Battles_2023.localMI)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#plotting-the-moran-scatter-plot",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#plotting-the-moran-scatter-plot",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.10.1 Plotting the Moran Scatter plot",
    "text": "4.10.1 Plotting the Moran Scatter plot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code below plots the Moran scatterplot of Battles in 2023 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(Battles_2023$Incidents, rswm_q,\n                  labels=as.character(Battles_2023$DT), \n                  xlab=\"Battles_2023\", \n                  ylab=\"Spatially Lagged Events,Year\")\n\n\n\n\nThe plot is split in 4 quadrants. The top right corner belongs to areas that have high incidents of events and are surrounded by other areas that have higher than the average level/number of battles This is the high-high locations.\n\n\n\n\n\n\nNote\n\n\n\nThe Moran scatterplot is divided into four areas, with each quadrant corresponding with one of four categories: (1) High-High (HH) in the top-right quadrant; (2) High-Low (HL) in the bottom right quadrant; (3) Low-High (LH) in the top-left quadrant; (4) Low- Low (LL) in the bottom left quadrant."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.10.2 Plotting Moran scatterplot with standardised variable",
    "text": "4.10.2 Plotting Moran scatterplot with standardised variable\nFirst, I will use scale() to centre and scale the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centred) variable by their standard deviations.\n\nBattles_2023$Z.Incidents &lt;- scale(Battles_2023$Incidents) %&gt;% \n  as.vector \n\nThe as.vector() is added to the end is to make sure that the data type is a vector, that maps neatly into our dataframe.\nNext, we plot the Moran scatterplot again by using the code below.\n\nnci2 &lt;- moran.plot(Battles_2023$Z.Incidents, rswm_q,\n                   labels=as.character(Battles_2023$DT),\n                   xlab=\"z-Battles in 2023\", \n                   ylab=\"Spatially Lag z-Battles in 2023\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n1) High-High (HH): indicates high spatial correlation where incidents of Battles are clustered closely together.\n2) High-Low (HL): where areas of high frequency of incidents of Battles occurred are located next to areas where there is low frequency of incidents of Battles occurred.\n3) Low-High (LH): these are areas of low frequency of incidents where Battles occurred that are located next to areas where high frequency of Battles.\n4) Low-Low (LL): these are clusters of low frequency of incidents of Battles occurred."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#preparing-lisa-map-classes",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#preparing-lisa-map-classes",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.10.3 Preparing LISA map classes",
    "text": "4.10.3 Preparing LISA map classes\nAccording to Anselin (1995), LISA can be used to locate “hot spots” or local spatial clusters where the occurrence of Event types is statistically significant.\nIn addition to the four categories described in the Moran Scatterplot, the LISA analysis includes an additional category: (5) Insignificant: where there are no spatial autocorrelation or clusters where event types have occurred.\nThe code below shows the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we derive the spatially lagged variable of interest (i.e. Incidents) and centre the spatially lagged variable around its mean.\n\nBattles_2023$lag_Incidents &lt;- lag.listw(rswm_q, Battles_2023$Incidents)\nDV &lt;- Battles_2023$lag_Incidents - mean(Battles_2023$lag_Incidents)     \n\nThis is followed by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThe code below define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, we place non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#plotting-lisa-map",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#plotting-lisa-map",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.10.4 Plotting LISA Map",
    "text": "4.10.4 Plotting LISA Map\nThe below code is used to create the LISA map.\n\nBattles_2023.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"lightyellow\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#deriving-distance-based-weight-matrix",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#deriving-distance-based-weight-matrix",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.12.1 Deriving distance-based weight matrix",
    "text": "4.12.1 Deriving distance-based weight matrix\nWhist the spatial autocorrelation in the previous section considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n4.12.1.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph.\nWe need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length.\nOur input vector will be the geometry column of Battles_2023 dataset. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid() function over the geometry column of our Battles_2023 dataset and access the longitude value through double bracket notation [[]] and 1.\nThis allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[1]])\n\n\nclass(longitude)\n\n[1] \"numeric\"\n\n\n\nlongitude\n\n [1] 95.19035 94.99369 94.74008 95.53705 96.58767 96.34709 95.30186 95.75119\n [9] 93.71488 93.56355 93.22544 93.81953 97.16218 96.49805 97.41891 97.78205\n[17] 97.42880 97.33311 97.45091 97.36866 98.22657 98.51991 94.18202 95.31595\n[25] 94.45732 94.73063 95.05170 96.22693 92.48276 93.35447 92.90601 94.50024\n[33] 95.47237 94.40944 95.49546 96.05576 95.53962 94.74604 95.26376 95.66196\n[41] 95.64449 95.40740 94.38739 94.71565 98.96838 98.65407 97.13646 98.19561\n[49] 96.66042 98.01229 97.21860 96.52970 98.02872 97.92760 97.03704 96.92822\n[57] 98.47076 98.77515 96.16459 95.97341 95.54920 95.15564 96.25009 96.06338\n[65] 97.84218 97.26005 96.16788 96.13921 93.92534 98.93770 96.24776 96.03014\n[73] 96.28511 96.13431\n\n\nWe do the same for latitude by accessing the second value per centroid with [[2]].\n\nlatitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\nclass(coords)\n\n[1] \"matrix\" \"array\" \n\n\n\ncoords\n\n      longitude latitude\n [1,]  95.19035 17.87515\n [2,]  94.99369 16.16236\n [3,]  94.74008 16.86214\n [4,]  95.53705 16.17363\n [5,]  96.58767 17.74287\n [6,]  96.34709 18.81260\n [7,]  95.30186 18.76076\n [8,]  95.75119 18.10024\n [9,]  93.71488 23.42199\n[10,]  93.56355 22.51244\n[11,]  93.22544 21.57539\n[12,]  93.81953 21.26333\n[13,]  97.16218 24.25814\n[14,]  96.49805 25.30215\n[15,]  97.41891 26.01108\n[16,]  97.78205 27.27547\n[17,]  97.42880 18.91417\n[18,]  97.33311 19.48546\n[19,]  97.45091 17.75720\n[20,]  97.36866 18.07659\n[21,]  98.22657 16.00305\n[22,]  98.51991 16.54942\n[23,]  94.18202 21.80335\n[24,]  95.31595 20.26548\n[25,]  94.45732 20.38303\n[26,]  94.73063 21.43135\n[27,]  95.05170 19.43717\n[28,]  96.22693 21.61640\n[29,]  92.48276 21.02760\n[30,]  93.35447 20.47696\n[31,]  92.90601 20.40055\n[32,]  94.50024 18.53159\n[33,]  95.47237 25.34589\n[34,]  94.40944 23.07978\n[35,]  95.49546 23.32423\n[36,]  96.05576 24.25502\n[37,]  95.53962 23.93661\n[38,]  94.74604 23.96919\n[39,]  95.26376 22.25730\n[40,]  95.66196 26.41610\n[41,]  95.64449 21.98775\n[42,]  95.40740 22.74644\n[43,]  94.38739 24.17969\n[44,]  94.71565 22.24865\n[45,]  98.96838 23.02589\n[46,]  98.65407 23.83364\n[47,]  97.13646 22.50271\n[48,]  98.19561 22.76825\n[49,]  96.66042 23.45728\n[50,]  98.01229 23.68574\n[51,]  97.21860 23.21080\n[52,]  96.52970 21.22388\n[53,]  98.02872 20.26507\n[54,]  97.92760 21.39398\n[55,]  97.03704 20.46234\n[56,]  96.92822 20.85826\n[57,]  98.47076 14.08664\n[58,]  98.77515 10.98880\n[59,]  96.16459 21.96431\n[60,]  95.97341 20.95960\n[61,]  95.54920 21.47201\n[62,]  95.15564 20.94652\n[63,]  96.25009 22.62753\n[64,]  96.06338 20.49193\n[65,]  97.84218 15.79499\n[66,]  97.26005 17.13186\n[67,]  96.16788 19.63929\n[68,]  96.13921 20.04539\n[69,]  93.92534 19.67628\n[70,]  98.93770 12.36708\n[71,]  96.24776 16.89759\n[72,]  96.03014 17.26904\n[73,]  96.28511 16.66221\n[74,]  96.13431 16.82926\n\n\n\n\n4.12.1.2 Determining the cut-off distance\nFor the fixed distance weights, first, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\n\n\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  14.26   49.33   66.03   71.79   82.19  196.85 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 196.85 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\nwm_d197 &lt;- dnearneigh(coords, 0, 197, longlat = TRUE)\nwm_d197\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm197_lw &lt;- nb2listw(wm_d197, style = 'B')\nsummary(wm197_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 3  1  2  3  3  5  3  3  7  3  4  4  6  5  5  4  1  5  4  3 \n3 least connected regions:\n16 57 58 with 1 link\n3 most connected regions:\n39 42 62 with 20 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 828 1656 45160"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-adaptive-distance-weight-matrix",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-adaptive-distance-weight-matrix",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.12.2 Computing Adaptive distance weight matrix",
    "text": "4.12.2 Computing Adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours.\nHaving many neighbours smoothes the neighbour relationship across more neighbours.\nHowever, it is also possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n74 \n74 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n74 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 592 1036 19636"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-gi-statistics---fixed-distance",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-gi-statistics---fixed-distance",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.12.3 Computing GI statistics - Fixed distance",
    "text": "4.12.3 Computing GI statistics - Fixed distance\n\ngi.fixed &lt;- localG(Battles_2023$Incidents, wm197_lw)\ngi.fixed\n\n [1] -2.16251076 -2.26945742 -2.36608309 -2.28060756 -1.40482297 -1.73401341\n [7] -1.78091543 -2.09517182  1.58404920  3.74739192  1.77357701  1.43327980\n[13]  1.76425408  0.23868264 -0.46980525  0.66543351 -0.74168975 -1.33781214\n[19] -0.51148627 -0.52707475  0.62973486  0.75874852  2.45091315 -0.79794591\n[25] -0.46714759  0.78360503 -2.26619716  1.90728203 -0.19741884  0.53880930\n[31] -1.05349256 -1.42802753 -0.14931467  3.57984196  2.13682831  0.42422030\n[37]  1.15690841  2.06391206  2.02017424  0.24781966  2.12585948  2.66825096\n[43]  0.59177302  2.14376526  1.90967745  1.33161281  1.56642737  0.66429027\n[49]  3.18467135 -0.08013071  2.14013808  0.26220228 -0.29653380 -0.70655951\n[55] -1.52151868 -1.55549239  1.38510282  1.35703308  1.80376535  0.80349295\n[61]  1.58567210  0.37028741  1.42615140 -0.19363169  0.39364611 -0.86461442\n[67] -1.28913769 -1.59126895 -1.85739540  0.40386411 -2.04421053 -1.82676914\n[73] -1.82291155 -2.01994548\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)       Z(Gi) Pr(z != E(Gi))\n [1,] 0.068229167 0.17808219 0.0025805215 -2.16251076   0.0305788284\n [2,] 0.007285974 0.09589041 0.0015242874 -2.26945742   0.0232405238\n [3,] 0.020046863 0.12328767 0.0019038942 -2.36608309   0.0179774092\n [4,] 0.006769071 0.09589041 0.0015270818 -2.28060756   0.0225716798\n [5,] 0.094095941 0.16438356 0.0025033091 -1.40482297   0.1600739255\n [6,] 0.110194304 0.20547945 0.0030195729 -1.73401341   0.0829157041\n [7,] 0.065091864 0.15068493 0.0023098862 -1.78091543   0.0749262673\n [8,] 0.091196626 0.20547945 0.0029752444 -2.09517182   0.0361557216\n [9,] 0.193083573 0.12328767 0.0019414335  1.58404920   0.1131825243\n[10,] 0.289515279 0.12328767 0.0019676510  3.74739192   0.0001786828\n[11,] 0.202220460 0.12328767 0.0019806822  1.77357701   0.0761331435\n[12,] 0.267664828 0.19178082 0.0028030997  1.43327980   0.1517778927\n[13,] 0.219305224 0.13698630 0.0021770936  1.76425408   0.0776892110\n[14,] 0.091077575 0.08219178 0.0013859604  0.23868264   0.8113516776\n[15,] 0.040245203 0.05479452 0.0009590683 -0.46980525   0.6384941605\n[16,] 0.023995827 0.01369863 0.0002394576  0.66543351   0.5057732553\n[17,] 0.090454904 0.12328767 0.0019596135 -0.74168975   0.4582753304\n[18,] 0.074313409 0.13698630 0.0021946699 -1.33781214   0.1809576830\n[19,] 0.139005236 0.16438356 0.0024618296 -0.51148627   0.6090105985\n[20,] 0.125490196 0.15068493 0.0022849420 -0.52707475   0.5981416802\n[21,] 0.058130190 0.04109589 0.0007317001  0.62973486   0.5288680718\n[22,] 0.078141499 0.05479452 0.0009468162  0.75874852   0.4480030085\n[23,] 0.340266667 0.20547945 0.0030244162  2.45091315   0.0142494332\n[24,] 0.200730880 0.24657534 0.0033008582 -0.79794591   0.4249018788\n[25,] 0.167275574 0.19178082 0.0027517563 -0.46714759   0.6403942853\n[26,] 0.303858068 0.26027397 0.0030935821  0.78360503   0.4332719050\n[27,] 0.062418386 0.17808219 0.0026049511 -2.26619716   0.0234393145\n[28,] 0.355729167 0.24657534 0.0032752773  1.90728203   0.0564840763\n[29,] 0.061812467 0.06849315 0.0011451559 -0.19741884   0.8434997910\n[30,] 0.146966527 0.12328767 0.0019313068  0.53880930   0.5900184491\n[31,] 0.043455497 0.08219178 0.0013519884 -1.05349256   0.2921153017\n[32,] 0.030184751 0.08219178 0.0013263280 -1.42802753   0.1532839350\n[33,] 0.076701571 0.08219178 0.0013519884 -0.14931467   0.8813053367\n[34,] 0.363684489 0.17808219 0.0026880602  3.57984196   0.0003438021\n[35,] 0.322002635 0.20547945 0.0029736197  2.13682831   0.0326119589\n[36,] 0.171367177 0.15068493 0.0023769086  0.42422030   0.6714051589\n[37,] 0.252951981 0.19178082 0.0027957315  1.15690841   0.2473097820\n[38,] 0.232058669 0.13698630 0.0021219065  2.06391206   0.0390260550\n[39,] 0.396593674 0.27397260 0.0036842794  2.02017424   0.0433653170\n[40,] 0.047619048 0.04109589 0.0006928579  0.24781966   0.8042739429\n[41,] 0.387329591 0.26027397 0.0035720591  2.12585948   0.0335149615\n[42,] 0.435444414 0.27397260 0.0036621834  2.66825096   0.0076247282\n[43,] 0.134509081 0.10958904 0.0017733202  0.59177302   0.5540025915\n[44,] 0.370217451 0.24657534 0.0033264297  2.14376526   0.0320517005\n[45,] 0.132483082 0.06849315 0.0011228022  1.90967745   0.0561747560\n[46,] 0.113924051 0.06849315 0.0011639833  1.33161281   0.1829874538\n[47,] 0.307425214 0.21917808 0.0031738082  1.56642737   0.1172486006\n[48,] 0.137802607 0.10958904 0.0018038490  0.66429027   0.5065045498\n[49,] 0.339932274 0.17808219 0.0025828347  3.18467135   0.0014491849\n[50,] 0.092809365 0.09589041 0.0014784223 -0.08013071   0.9361332989\n[51,] 0.287356322 0.17808219 0.0026070606  2.14013808   0.0323436091\n[52,] 0.261594581 0.24657534 0.0032811258  0.26220228   0.7931654986\n[53,] 0.071372753 0.08219178 0.0013311533 -0.29653380   0.7668224621\n[54,] 0.080156658 0.10958904 0.0017352153 -0.70655951   0.4798402603\n[55,] 0.123498695 0.20547945 0.0029031487 -1.52151868   0.1281297272\n[56,] 0.131920530 0.21917808 0.0031468082 -1.55549239   0.1198288467\n[57,] 0.035637728 0.01369863 0.0002508843  1.38510282   0.1660210309\n[58,] 0.034807642 0.01369863 0.0002419663  1.35703308   0.1747706992\n[59,] 0.366230366 0.26027397 0.0034505972  1.80376535   0.0712681006\n[60,] 0.292600313 0.24657534 0.0032811258  0.80349295   0.4216898674\n[61,] 0.354370214 0.26027397 0.0035214197  1.58567210   0.1128137120\n[62,] 0.295910393 0.27397260 0.0035100061  0.37028741   0.7111683500\n[63,] 0.299541655 0.21917808 0.0031753179  1.42615140   0.1538246447\n[64,] 0.222019781 0.23287671 0.0031438461 -0.19363169   0.8464642818\n[65,] 0.066967845 0.05479452 0.0009563271  0.39364611   0.6938423348\n[66,] 0.108660999 0.15068493 0.0023623728 -0.86461442   0.3872504579\n[67,] 0.124184712 0.19178082 0.0027494435 -1.28913769   0.1973502243\n[68,] 0.131770833 0.21917808 0.0030172252 -1.59126895   0.1115490605\n[69,] 0.041992697 0.12328767 0.0019156610 -1.85739540   0.0632549198\n[70,] 0.036378335 0.02739726 0.0004945225  0.40386411   0.6863126506\n[71,] 0.063607925 0.16438356 0.0024302999 -2.04421053   0.0409327537\n[72,] 0.096329081 0.19178082 0.0027302372 -1.82676914   0.0677344872\n[73,] 0.085438916 0.17808219 0.0025828347 -1.82291155   0.0683167878\n[74,] 0.065070276 0.16438356 0.0024173270 -2.01994548   0.0433890436\nattr(,\"cluster\")\n [1] Low  Low  Low  Low  Low  High Low  Low  Low  Low  High Low  High High High\n[16] Low  Low  High Low  Low  High High High Low  Low  High Low  Low  Low  Low \n[31] Low  Low  Low  High Low  High Low  Low  High Low  High High Low  High Low \n[46] High High High Low  High Low  Low  Low  Low  Low  High High Low  Low  Low \n[61] High Low  High Low  High High Low  Low  Low  High Low  Low  Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = Battles_2023$Incidents, listw = wm197_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nNext, we will join the Gi values to their corresponding sf data frame by using the code below.\n\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nBattles_2023.gi\n\nSimple feature collection with 74 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           DT   DT_PCODE    DT_MMR PCode_V year event_type Incidents Fatalities\n1    Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023    Battles         4          3\n2     Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023    Battles         1          1\n3     Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023    Battles         3          1\n4      Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023    Battles         3          2\n5        Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023    Battles        50        270\n6     Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023    Battles        87        493\n7        Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023    Battles        34         59\n8  Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023    Battles        50         93\n9       Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023    Battles        27         89\n10      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023    Battles        48        210\n   Z.Incidents lag_Incidents gstat_fixed                       geometry\n1  -0.80534765      18.20000   -2.162511 MULTIPOLYGON (((95.12637 18...\n2  -0.85573862       3.00000   -2.269457 MULTIPOLYGON (((95.04462 15...\n3  -0.82214464       2.50000   -2.366083 MULTIPOLYGON (((94.27572 15...\n4  -0.82214464       3.00000   -2.280608 MULTIPOLYGON (((95.20798 15...\n5  -0.03268604      40.66667   -1.404823 MULTIPOLYGON (((95.90674 18...\n6   0.58880265      29.00000   -1.734013 MULTIPOLYGON (((96.17964 19...\n7  -0.30143790      31.40000   -1.780915 MULTIPOLYGON (((95.70458 19...\n8  -0.03268604      35.60000   -2.095172 MULTIPOLYGON (((95.85173 18...\n9  -0.41901684      53.00000    1.584049 MULTIPOLYGON (((93.36931 24...\n10 -0.06628002      62.00000    3.747392 MULTIPOLYGON (((93.35213 23...\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe codes above performs three tasks.\n\nFirst, it converts the output vector (i.e. gi.fixed) into r matrix object by using as.matrix().\nNext, cbind() is used to join Battles_2023 and gi.fixed matrix to produce a new SpatialPolygonDataFrame called Battles_2023.gi.\nLastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\n\n4.12.3.1 Mapping Gi values with Fixed distance weights\nThe code below plots the Gi values derived using fixed distance weight matrix, for event type==Battles in 2023.\n\nGimap &lt;-tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(Battles_2023.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"Fixed Distance\\nlocal Gi\") +\n  tm_borders(alpha = 0.5)\n\nGimap"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-gi-statistics---adaptive-distance",
    "href": "Take-home-ex/Take-home-Ex4a/Take-home_Ex4a.html#computing-gi-statistics---adaptive-distance",
    "title": "Take-Home Exercise 4a - Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.12.4 Computing GI statistics - Adaptive distance",
    "text": "4.12.4 Computing GI statistics - Adaptive distance\nThe code below is used to compute the Gi values for Incidents of Battles in 2023 by using an adaptive distance weight matrix (i.e knb_lw).\n\ngi.adaptive &lt;- localG(Battles_2023$Incidents, knn_lw)\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\ndatatable(Battles_2023.gi)\n\n\n\n\n\n\n\n4.12.4.1 Mapping Gi values with Adaptive distance weights\nThe code below plots the Gi values derived using adaptive distance weight matrix for event type == Battles in 2023.\n\nGimap &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(Battles_2023.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"Adaptive Distance\\nlocal Gi\") + \n  tm_borders(alpha = 0.5)\n\nGimap\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user inputs:\n\nYear\nEvent type\nData Classification type, and\nNumber of clusters for the Adaptive weight matrix"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "",
    "text": "According to an official report as shown in the infographic below,\n\nThe intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer, and\nThe contrast between the wet months (November to January) and dry months (February and June to September) is likely to be more pronounced."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#installing-and-loading-r-packages",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#installing-and-loading-r-packages",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "2.1 Installing and Loading R packages",
    "text": "2.1 Installing and Loading R packages\n\npacman::p_load(ungeviz, plotly, crosstalk, patchwork,\n               DT, ggdist, ggridges, ggstatsplot,ggthemes,\n               colorspace, gganimate, tidyverse, dplyr, \n               readr)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#importing-data-and-data-preparation",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#importing-data-and-data-preparation",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "2.2 Importing Data and Data Preparation",
    "text": "2.2 Importing Data and Data Preparation\nI will download and import data for July (Dry) and December (Wet) for 1983, 1993, 2003, 2013, 2023.\n\nI have chosen Changi weather station, as it is one of the older weather stations with daily rainfall data recorded since 1981. For more information on the weather stations and the historical data available, please refer to this link.\n\n\nShow the code\n# List of file names\nDecember_files &lt;- c(\"data/DAILYDATA_S24_198312.csv\", \"data/DAILYDATA_S24_199312.csv\", \n                \"data/DAILYDATA_S24_200312.csv\", \"data/DAILYDATA_S24_201312.csv\", \n                \"data/DAILYDATA_S24_202312.csv\")\n\nJuly_files &lt;- c(\"data/DAILYDATA_S24_198307.csv\", \"data/DAILYDATA_S24_199307.csv\", \n                \"data/DAILYDATA_S24_200307.csv\", \"data/DAILYDATA_S24_201307.csv\", \n                \"data/DAILYDATA_S24_202307.csv\")\n\n\nUpon inspecting the csv files for the 5 years, the below columns were removed as they were only recorded since 2014:\n\nHighest  30-min Rainfall (mm)\nHighest  60-min Rainfall (mm)\nHighest 120-min Rainfall (mm)\n\nI will use the code below to import the csv files into our R environment\n\n# Reading and combining the CSV files for December\nDecember_data &lt;- lapply(December_files, read_csv, col_names = FALSE,\n                        col_select = c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13),\n                        skip = 1) %&gt;%\n  bind_rows(.id = \"file\")\n\n\n# Reading and combining the CSV files for July\nJuly_data &lt;- lapply(July_files, read_csv, col_names = FALSE,\n                        col_select = c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13),\n                        skip = 1) %&gt;%\n  bind_rows(.id = \"file\")\n\nI will use the code below to rename the column names for the data sets.\n\n\nShow the code\n# Renaming the columns\ncolnames(December_data) &lt;- c(\"ID\", \"Station\", \"Year\", \"Month\", \"Day\", \n                              \"Daily_Rainfall_Total_mm\", \"Mean_Temperature_C\", \n                              \"Maximum_Temperature_C\", \"Minimum_Temperature_C\", \n                              \"Mean_Wind_Speed_km_h\", \"Max_Wind_Speed_km_h\")\n\nDecember_data$Year &lt;- as.factor(December_data$Year)\n\ncolnames(July_data) &lt;- c(\"ID\", \"Station\", \"Year\", \"Month\", \"Day\", \n                              \"Daily_Rainfall_Total_mm\", \"Mean_Temperature_C\", \n                              \"Maximum_Temperature_C\", \"Minimum_Temperature_C\", \n                              \"Mean_Wind_Speed_km_h\", \"Max_Wind_Speed_km_h\")\n\nJuly_data$Year &lt;- as.factor(July_data$Year)\n\n\nI will use the code below to combine the July and December data sets.\n\nJuly_data$Month &lt;- 'July'\nDecember_data$Month &lt;- 'December'\n\ncombined_data &lt;- rbind(July_data, December_data)\n\nI will use the datatable() function to inspect the combined data set.\n\nData TableData StructureMissing Values?\n\n\n\nDT::datatable(combined_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\nstr(combined_data)\n\ntibble [310 × 11] (S3: tbl_df/tbl/data.frame)\n $ ID                     : chr [1:310] \"1\" \"1\" \"1\" \"1\" ...\n $ Station                : chr [1:310] \"Changi\" \"Changi\" \"Changi\" \"Changi\" ...\n $ Year                   : Factor w/ 5 levels \"1983\",\"1993\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Month                  : chr [1:310] \"July\" \"July\" \"July\" \"July\" ...\n $ Day                    : num [1:310] 1 2 3 4 5 6 7 8 9 10 ...\n $ Daily_Rainfall_Total_mm: num [1:310] 8.7 0 5.3 6.2 39.5 14.9 0 0 10.5 55.5 ...\n $ Mean_Temperature_C     : num [1:310] 27.5 28.7 27.9 28 25.4 27.1 26.2 28.2 27.8 25.4 ...\n $ Maximum_Temperature_C  : num [1:310] 33 32.6 32 31.9 27.4 31.1 28.1 31.9 31.1 27.2 ...\n $ Minimum_Temperature_C  : num [1:310] 24.4 25.5 24.4 25.7 21.4 24.1 22.6 25.6 26 23 ...\n $ Mean_Wind_Speed_km_h   : num [1:310] 3.7 10.5 5.8 7.6 3.6 8.4 5 11.8 9 5.3 ...\n $ Max_Wind_Speed_km_h    : num [1:310] 28.8 38.2 44.6 51.8 36 31.3 42.5 39.6 43.9 46.8 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   X1 = col_character(),\n  ..   X2 = col_double(),\n  ..   X3 = col_double(),\n  ..   X4 = col_double(),\n  ..   X5 = col_double(),\n  ..   X6 = col_skip(),\n  ..   X7 = col_skip(),\n  ..   X8 = col_skip(),\n  ..   X9 = col_double(),\n  ..   X10 = col_double(),\n  ..   X11 = col_double(),\n  ..   X12 = col_double(),\n  ..   X13 = col_double()\n  .. )\n\n\n\n\n\nsum(is.na(combined_data))\n\n[1] 0\n\n\nThere are no missing values in our data set."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#frequency-and-intensity-of-rainfall-events",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#frequency-and-intensity-of-rainfall-events",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "3.1 Frequency and Intensity of Rainfall events",
    "text": "3.1 Frequency and Intensity of Rainfall events\nFirst, I will use the code below to create a static line plot to visualize July’s and December’s rainfall over the 5 years.\n\n\nShow the code\nggplot(combined_data, aes(x = Day, \n        y = Daily_Rainfall_Total_mm, group = Month)) +\n  \n  # Use geom_area for one of the months, July in this case\n  geom_area(data = subset(combined_data, Month == \"July\"), \n            aes(fill = Year), alpha = 0.3) +\n  \n  # Use geom_line for both months to ensure the line is on top of the fill\n  geom_line(aes(color = Year, \n                linetype = Month)) +\n  facet_wrap(~Year, ncol = 1) +\n  labs(title = \"Daily Rainfall in July and December by Year (1983 - 2023)\",\n       x = \"Day of the Month\",\n       y = \"Rainfall (mm)\",\n       color = \"Year\",\n       fill = \"Year\",\n       linetype = \"Month\") +\n  theme_minimal() +\n  \n  guides(fill = guide_legend(\"Year\"), color = guide_legend(\"Year\"), \n         linetype = guide_legend(\"Month\"))\n\n\n\n\n\nNext, to enable readers to specifically examine the rainfall measures in more detail, I will use the code below to add interactivity.\n\n\nShow the code\np3 &lt;- ggplot(combined_data, aes(x = Day, y = Daily_Rainfall_Total_mm)) +\n  \n  # Use geom_line for both months, specifying linetype based on Month\n  geom_line(aes(color = Year, linetype = Month)) +\n  facet_wrap(~Year, ncol = 1) +\n  labs(title = \"Daily Rainfall in July and December by Year (1983-2023)\",\n       x = \"Day of the Month\",\n       y = \"Rainfall (mm)\",\n       color = \"Year\",\n       linetype = \"Month\") +\n  theme_minimal() +\n  \n  guides(fill = guide_legend(\"Year\"), color = guide_legend(\"Year\"), \n         linetype = guide_legend(\"Month\")) + scale_linetype_manual(values = c(\"July\" = \"dotted\", \"December\" = \"solid\"))\n\n\n\nggplotly(p3)\n\n\n\n\n\nI will also animate the combined plot of July and December’s Daily rainfall, using gganimate.\nThis animation will provide a visual comparison on how daily rainfall changes for each month, across the years.\n\n\nShow the code\n# Adjusted plot code without geom_area\np_combined_simple &lt;- ggplot(combined_data, \n                            aes(x = Day, \n                            y = Daily_Rainfall_Total_mm, \n                            group = interaction(Month, Year), \n                            color = Year)) +\n  \n  geom_line(aes(linetype = Month)) +\n  facet_wrap(~Year, ncol = 1) +\n  labs(title = \"Daily Rainfall in July and December by Year: Day {frame_along}\",\n       x = \"Day of the Month\",\n       y = \"Rainfall (mm)\",\n       color = \"Year\",\n       linetype = \"Month\") +\n  theme_minimal() +\n  guides(color = guide_legend(\"Year\"), linetype = guide_legend(\"Month\"))\n\n# Simplify the animation using transition_reveal for Day\nanimated_plot_simple &lt;- p_combined_simple +\n  transition_reveal(Day) +\n  ease_aes('linear')\n\n# Render the animation\nanimate(animated_plot_simple, nframes = 31, fps = 5, width = 800, height = 600)\n\n\n\n\n\n\n\n\n\n\n\nObservation 1 - Frequency and Intensity of Rainfall events show no substantial increase\n\n\n\nThe infographics had reported that the Intensity and frequency of heavy rain fall events is expected to increase as the world gets warmer.\nFrom the visualisations above, in general, there are visibly more rain days in December relative to July over the years.\nHowever, there seems to be no substantial increase in the intensity of heavy rain events. For example, we can see that the highest recorded daily rainfall event occured in December 1983 at 164.4 mm. However, this level of intensity has not been surpassed in subsequent years.\n\nAdditionally, the volume of rainfall for both July and December 2023 is the least within the 5 years.\nThis further challenges the claim that the intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#one-way-anova-test-on-daily-rainfall-for-the-same-month-by-year",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#one-way-anova-test-on-daily-rainfall-for-the-same-month-by-year",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "3.2 One-way Anova test on Daily Rainfall for the same month, by year",
    "text": "3.2 One-way Anova test on Daily Rainfall for the same month, by year\nThe infographics had reported that the Intensity and frequency of heavy rain fall events is expected to increase as the world gets warmer.\nOur visualisations in the previous section have showed otherwise.\nWe can validate this statistically and examine if there are any significant statistical differences between the daily rainfall for each July, and each December, across the five years.\nFirst, we will need to ascertain the nature of the distribution, and see if it is normal or non-normal distributed.\nWe can first visualise the distribution of daily rainfall using ridgeline plots, using the code below.\n\n\nShow the code\nggplot(July_data, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\"),\n    alpha = 0.6,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Daily Rainfall (mm)\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Distribution of the Daily Rainfall for July (1983-2023)\")\n\n\n\n\n\n\n\nShow the code\nggplot(December_data, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\"),\n    alpha = 0.6,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Daily Rainfall (mm)\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Distribution of the Daily Rainfall for December (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the ridgeline plots above, we can see that the distribution for both July and December rainfall do not resemble a normal distribution, and that there is a right skewness.\n\n\nSince the distribution of rainfall is non-normal, I will conduct non-parametric tests.\nThe Hypothesis will be as such:\nH0: There is no difference between the median daily rainfall for the same month across the 5 years.\nH1: There is a difference between the median daily rainfall for the same month across the 5 years.\nI will use ggstatsbetween() from the ggstatplot package.\n\n\nShow the code\nggbetweenstats(\n  data = July_data,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n) +\n  ggtitle(\"Daily Rainfall for July across the years (1983-2023)\")\n\n\n\n\n\n\n\nShow the code\nggbetweenstats(\n  data = December_data,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n) +\n  ggtitle(\"Daily Rainfall for December across the years (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\nValidation results\n\n\n\nIn both the tests above, the p-value is &gt; 0.05.\nHence, we have insufficient evidence to reject the Null Hypothesis and can conclude that there is no strong evidence to indicate that there is a difference in the Daily rainfall for the same month across the years.\nThis supports Observation 1, where we concluded that there seems to be no substantial increase in the frequency and intensity of heavy rain fall events."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#differences-between-july-and-december-across-the-years-dry-vs-wet-months",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#differences-between-july-and-december-across-the-years-dry-vs-wet-months",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "3.3 Differences between July and December across the years (Dry Vs Wet Months)",
    "text": "3.3 Differences between July and December across the years (Dry Vs Wet Months)\nTo visualise the differences for the June-December periods, I will use the codes below to examine:\n\nThe differences in the number of Rain days, and\nThe differences in the Daily Rainfall, between the two months, across the years\n\n\n\nShow the code\nrain_fall_summary &lt;- combined_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarize(\n    MeanRainfall = mean(Daily_Rainfall_Total_mm, na.rm = TRUE),\n    RainyDays = sum(Daily_Rainfall_Total_mm &gt; 0, na.rm = TRUE), # Count days with rain\n    .groups = 'drop'\n  )\n\n\n\n\nShow the code\np_1 &lt;- ggplot(rain_fall_summary, aes(x = Year, y = RainyDays, group = Month, color = Month)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"The largest difference in the number of Rain Days between\\nJuly and December was in 2023\",\n       x = \"Year\",\n       y = \"Number of Rainy Days\",\n       color = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np_2 &lt;- ggplot(rain_fall_summary, aes(x = Year, y = MeanRainfall, group = Month, color = Month)) +\n  geom_line() +\n  geom_point() +\n  \n  labs(title = \"Both July's and December's mean Daily Rainfall have\\ntrended lower over the Years\",\n       x = \"Year\",\n       y = \"Mean Daily Rainfall (mm)\",\n       color = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_3 &lt;- ggplot(combined_data, aes(x = Year, y = Daily_Rainfall_Total_mm, fill = Month)) +\n  geom_boxplot() +\n  labs(title = \"The gap between July and December has reduced in 2023 \",\n       subtitle = \"The intensity of December's heavy rainfall events seem to be diminishing\",\n       x = \"Year\",\n       y = \"Daily Rainfall (mm)\",\n       fill = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\nUsing Patchwork to combine the plots.\n\n\nShow the code\ncombined_plot &lt;- (p_1 + p_2) / p_3 + \n  plot_annotation(\n    title = \"The contrast between July and December has become more pronounced over the years\",\n    theme = theme(\n      plot.title = element_text(size = 20, face = \"bold\")\n    )\n  ) +\n  theme(plot.title.position = \"plot\")\n\ncombined_plot\n\n\n\n\n\nNext, to enable readers to specifically examine the differences in the rainfall between the two months in more detail, I will use the code below to add interactivity.\n\n\nShow the code\ncombined_mean &lt;- combined_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(MeanRainfall = round(mean(Daily_Rainfall_Total_mm, na.rm = TRUE), 2)) %&gt;%\n  ungroup()\n\n# Create the ggplot object\np_combined &lt;- ggplot(combined_data, aes(x = Year, y = Daily_Rainfall_Total_mm, color = Month)) +\n  geom_jitter(aes(text = paste('Day:', Day, 'Month:', Month)), width = 0.2, alpha = 0.5) + \n  geom_line(data = combined_mean, aes(x = Year, y = MeanRainfall, group = Month), \n            size = 0.5, linetype = \"dotted\") + \n  geom_point(data = combined_mean, aes(x = Year, y = MeanRainfall), \n             size = 3, show.legend = FALSE) + \n  scale_color_manual(values = c(\"July\" = \"blue\", \"December\" = \"red\")) +\n  labs(title = \"Daily Rainfall in July and December (1983-2023)\",\n       x = \"Year\",\n       y = \"Daily Rainfall Total (mm)\") +\n  theme_minimal() +\n  \n  \n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(title = \"Month\"))\n\n\n\n\nShow the code\np_plotly &lt;- ggplotly(p_combined) %&gt;%\n  layout(hovermode = 'closest') \n\np_plotly\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 2 - Contrast between Dry and Wet Months are likely to be more pronounced\n\n\n\nThe infographics had reported that the contrast between Dry and Wet months is likely to be more pronounced.\nOur plots above show that:-\n\nThe difference in the number of rain days between July and December, in 2023, was at its highest, relative to previous years. The number of rain days in July has decreased, and the number of rain days in December has increased, over the years.\nThe mean daily rain fall for both July and December 2023 was at its lowest relative to other years.\nIn 2023, the gap of heavy rain fall events have been reduced. In previous years, there were more days with heavy rain fall in December relative to July.\n\nHence, this lends credence to the claim that the contrast between Dry and Wet months is likely to be more pronounced."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#two-sample-mean-test-july-vs-december-by-year",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#two-sample-mean-test-july-vs-december-by-year",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "3.4 Two-sample mean test: July Vs December, by Year",
    "text": "3.4 Two-sample mean test: July Vs December, by Year\nThe infographics had reported that the contrast between wet months and dry months is likely to be more pronounced.\nOur visualisations in the previous section have corroborated this claim.\nWe can validate this statistically and examine if there is any evidence to suggest that there is indeed a difference in the daily rainfall between the two months (July and December).\nThe Hypothesis will be as such:\nH0: There is no difference in the median daily rainfall between July and December across the 5 years\nH1: There is a difference in the median daily rainfall between July and December across the 5 years\nSince we have rain fall data for 5 different years, I will use grouped_ggbetweenstats() from the ggstatplot package.\n\n\nShow the code\ngroup_plot &lt;- grouped_ggbetweenstats(\n  data = combined_data,\n  x = Month,\n  y = Daily_Rainfall_Total_mm,\n  grouping.var = Year,\n  type = \"np\", # for non-parametric\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\", \n  output = \"plot\" \n) \n\ngroup_plot + plot_annotation(\n    title = \"Differences between July and December over the years (1983-2023)\",\n    theme = theme(\n      plot.title = element_text(size = 20, face = \"bold\")\n    )\n  ) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n\nValidation Results\n\n\n\nIn the tests above, the p-value is &gt; 0.05 for the years 1983,1993,2003 and 2013.\n\nHowever the p-value is &lt; 0.05 for year 2023.\nHence we can reject the Null Hypothesis and can conclude that there is some evidence to suggest that there is some difference in the daily rainfall between July to December, across the 5 years.\nThis supports Observation 2, where we concluded that the contrast between Dry and Wet months is likely to be more pronounced."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#interactive-error-bars",
    "href": "Take-home-ex/Take-home-Ex3/Take-home_Ex3.html#interactive-error-bars",
    "title": "Take-Home Exercise 3 - Be Weatherwise or Otherwise",
    "section": "4.1 Interactive Error Bars",
    "text": "4.1 Interactive Error Bars\nWe can also visualise the uncertainty of point estimates by plotting interactive error bars for the 99% confidence interval of mean Daily Rainfall in July and December by year as shown in the figures below.\n\n\nShow the code\nshared_df = SharedData$new(July_rain)\n\nbscols(widths = c(6,6),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(Year, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=Year, \n                     y=mean, \n                     text = paste(\"Year:\", `Year`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Rainfall:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Year\") + \n                   ylab(\"Average Daily Rainfall (mm)\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of&lt;br&gt;Avg Rainfall in July by Year\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 14,\n                                    scrollX=T), \n                     colnames = c(\"No. of observations\", \n                                  \"Avg Daily Rainfall\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nshared_df2 = SharedData$new(Dec_rain)\n\nbscols(widths = c(6,6),\n       ggplotly((ggplot(shared_df2) +\n                   geom_errorbar(aes(\n                     x=reorder(Year, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=Year, \n                     y=mean, \n                     text = paste(\"Year:\", `Year`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Rainfall:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Year\") + \n                   ylab(\"Average Daily Rainfall (mm)\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of&lt;br&gt;Avg Rainfall in Dec by Year\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df2, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 14,\n                                    scrollX=T), \n                     colnames = c(\"No. of observations\", \n                                  \"Avg Daily Rainfall\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations from Interactive Error bars\n\n\n\nJuly Trends:\n\n2023’s Average rainfall was the lowest, at 5.10 mm.\nThe increasing standard deviations (sd) from 2013 to 2023, indicates a trend of more variability in daily rainfall relative to the past.\n\nDecember Trends:\n\n2023’s Average rainfall was the lowest, at 8.26 mm.\nThe standard deviation shows a decrease over the 40-year span, indicating less variability in daily rainfall in December over time.\n\nJuly vs. December Relationship:\nThe differences in standard deviation (sd) between December and July for each respective year are as follows:\n\n1983: 18.92 mm\n1993: 4.24 mm\n2003: 10.12 mm\n2013: 15.59 mm\n2023: 0.82 mm\n\nThe difference in variability between December and July was much higher in 1983, but has significantly decreased by 2023.\nThe year 2023 stands out, with the variability in December being almost similar to that in July, indicating a convergence in the variability of rainfall between the two months in that year."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1b/Take-home_Ex1b.html",
    "href": "Take-home-ex/Take-home-Ex1b/Take-home_Ex1b.html",
    "title": "Take-home Exercise 1b",
    "section": "",
    "text": "pacman::p_load(tidyverse, sf, lubridate, leaflet, tmap,\n               raster, spatstat, plotly, DT, classInt, viridis,\n               ggplot2, sfdep)\n\n\n# Load conflict data\nconflict_data &lt;- read_csv(\"data/MMR.csv\")\n\nmmr_shp &lt;-  st_read(dsn = \"data/geospatial\",  \n                  layer = \"gadm41_MMR_1\")\n\nReading layer `gadm41_MMR_1' from data source \n  `C:\\imranmi\\ISSS608-VAA\\Take-home-ex\\Take-home-Ex1b\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 15 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1725 ymin: 8.824445 xmax: 101.1768 ymax: 28.54326\nGeodetic CRS:  WGS 84\n\n# Convert conflict data to an sf object\nconflict_sf &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nUse the SF file, with geometry column\n\n# Filter for battles only\nconflict_sf_battles_year &lt;- conflict_sf %&gt;%\n  filter(event_type == \"Battles\", year == 2010)\n\n# Plot using leaflet\nleaflet(conflict_sf_battles_year) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Battles, 2010&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2))\n\n\n\n\n\nusing the original acled file with long and lat columns\n\n# Filter for battles only\nconflict_battles_year &lt;- conflict_data %&gt;%\n  filter(event_type == \"Battles\", year == 2010)\n\n\n# Generate the map\nleaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # Base map layer\n  addCircleMarkers(data = conflict_battles_year, \n                   popup = ~paste(\"Event: Battles, 2010&lt;br&gt;Admin1:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2),\n                   radius = 8, # Adjust the size of the markers as needed\n                   fillColor = \"red\", fillOpacity = 0.8, color = \"#FFFFFF\", weight = 1) \n\n\n\n\n\nadding an additional layer of base map, mmr_shp to highlight visually MMR alone\n\nleaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(data = conflict_sf_battles_year, \n                   popup = ~paste(\"Event: Battles, 2010&lt;br&gt;Region/State:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2),\n                   radius = 8,\n                   fillColor = \"red\", fillOpacity = 0.8, color = \"#FFFFFF\", weight = 1) %&gt;%\n  addPolygons(data = mmr_shp, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  setView(lng = 96.1603, lat = 19.745, zoom = 6) # Center and zoom the map on Myanmar\n\n\n\n\n\n\nst_geometry(conflict_sf_battles_year) |&gt; \n  plot()\n\n\n\n\n\nclass(conflict_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nclass(mmr_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\nclass(conflict_data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\nmmr_shp[3,]\n\nSimple feature collection with 1 feature and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.6012 ymin: 20.64611 xmax: 94.17053 ymax: 24.11297\nGeodetic CRS:  WGS 84\n    GID_1 GID_0 COUNTRY NAME_1  VARNAME_1 NL_NAME_1 TYPE_1 ENGTYPE_1 CC_1\n3 MMR.3_1   MMR Myanmar   Chin Chin Hills        NA  Pyine     State   NA\n  HASC_1 ISO_1                       geometry\n3  MM.CH    NA MULTIPOLYGON (((92.66836 21...\n\n\n\nconflict_sf\n\nSimple feature collection with 57198 features and 33 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 92.199 ymin: 9.9824 xmax: 100.3576 ymax: 27.731\nGeodetic CRS:  WGS 84\n# A tibble: 57,198 × 34\n   event_id_cnty event_date        year time_precision disorder_type  event_type\n * &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;     \n 1 MMR58558      16 February 2024  2024              1 Political vio… Battles   \n 2 MMR58559      16 February 2024  2024              1 Political vio… Battles   \n 3 MMR58443      16 February 2024  2024              2 Political vio… Violence …\n 4 MMR58502      16 February 2024  2024              1 Political vio… Violence …\n 5 MMR58507      16 February 2024  2024              1 Political vio… Explosion…\n 6 MMR58508      16 February 2024  2024              1 Political vio… Explosion…\n 7 MMR58547      16 February 2024  2024              1 Strategic dev… Strategic…\n 8 MMR58560      16 February 2024  2024              1 Political vio… Battles   \n 9 MMR58589      16 February 2024  2024              2 Political vio… Violence …\n10 MMR58648      16 February 2024  2024              1 Political vio… Explosion…\n# ℹ 57,188 more rows\n# ℹ 28 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, actor2 &lt;chr&gt;, assoc_actor_2 &lt;chr&gt;, inter2 &lt;dbl&gt;,\n#   interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;, region &lt;chr&gt;,\n#   country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;, location &lt;chr&gt;,\n#   geo_precision &lt;dbl&gt;, source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;,\n#   fatalities &lt;dbl&gt;, tags &lt;chr&gt;, timestamp &lt;dbl&gt;, population_1km &lt;dbl&gt;, …\n\n\n\nplot(st_geometry(conflict_sf))\n\n\n\n\n\nBattles &lt;- filter(conflict_sf, event_type == \"Battles\")\nViolence_CV &lt;- filter(conflict_sf, event_type == \"Violence against civilians\")\nProtests &lt;- filter(conflict_sf, event_type == \"Protests\")\nRiots &lt;- filter(conflict_sf, event_type == \"Riots\")\nExplosions &lt;- filter(conflict_sf, event_type == \"Explosions/Remote violence\")\nStrategic_dev &lt;- filter(conflict_sf, event_type == \"Strategic developments\")\n\n\nclass(Explosions)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\nIn shiny App , we will need to enable users to filter by\n\nspecific year,\nyear range, and\nevent_type\n\nUsing subsets of event types which have been converted to sf objects\n\nBattlesViolence against CiviliansProtestsRiotsExplosions/Remote Devices\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Battles) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Battles&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Violence_CV) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Violence on Civillians&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Protests) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Protests&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Riots) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Riots&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Explosions) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Explosions&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1b/Take-home_Ex1b.html#loading-r-package",
    "href": "Take-home-ex/Take-home-Ex1b/Take-home_Ex1b.html#loading-r-package",
    "title": "Take-home Exercise 1b",
    "section": "",
    "text": "pacman::p_load(tidyverse, sf, lubridate, leaflet, tmap,\n               raster, spatstat, plotly, DT, classInt, viridis,\n               ggplot2, sfdep)\n\n\n# Load conflict data\nconflict_data &lt;- read_csv(\"data/MMR.csv\")\n\nmmr_shp &lt;-  st_read(dsn = \"data/geospatial\",  \n                  layer = \"gadm41_MMR_1\")\n\nReading layer `gadm41_MMR_1' from data source \n  `C:\\imranmi\\ISSS608-VAA\\Take-home-ex\\Take-home-Ex1b\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 15 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1725 ymin: 8.824445 xmax: 101.1768 ymax: 28.54326\nGeodetic CRS:  WGS 84\n\n# Convert conflict data to an sf object\nconflict_sf &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nUse the SF file, with geometry column\n\n# Filter for battles only\nconflict_sf_battles_year &lt;- conflict_sf %&gt;%\n  filter(event_type == \"Battles\", year == 2010)\n\n# Plot using leaflet\nleaflet(conflict_sf_battles_year) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Battles, 2010&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2))\n\n\n\n\n\nusing the original acled file with long and lat columns\n\n# Filter for battles only\nconflict_battles_year &lt;- conflict_data %&gt;%\n  filter(event_type == \"Battles\", year == 2010)\n\n\n# Generate the map\nleaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # Base map layer\n  addCircleMarkers(data = conflict_battles_year, \n                   popup = ~paste(\"Event: Battles, 2010&lt;br&gt;Admin1:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2),\n                   radius = 8, # Adjust the size of the markers as needed\n                   fillColor = \"red\", fillOpacity = 0.8, color = \"#FFFFFF\", weight = 1) \n\n\n\n\n\nadding an additional layer of base map, mmr_shp to highlight visually MMR alone\n\nleaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(data = conflict_sf_battles_year, \n                   popup = ~paste(\"Event: Battles, 2010&lt;br&gt;Region/State:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2),\n                   radius = 8,\n                   fillColor = \"red\", fillOpacity = 0.8, color = \"#FFFFFF\", weight = 1) %&gt;%\n  addPolygons(data = mmr_shp, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  setView(lng = 96.1603, lat = 19.745, zoom = 6) # Center and zoom the map on Myanmar\n\n\n\n\n\n\nst_geometry(conflict_sf_battles_year) |&gt; \n  plot()\n\n\n\n\n\nclass(conflict_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nclass(mmr_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\nclass(conflict_data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\nmmr_shp[3,]\n\nSimple feature collection with 1 feature and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.6012 ymin: 20.64611 xmax: 94.17053 ymax: 24.11297\nGeodetic CRS:  WGS 84\n    GID_1 GID_0 COUNTRY NAME_1  VARNAME_1 NL_NAME_1 TYPE_1 ENGTYPE_1 CC_1\n3 MMR.3_1   MMR Myanmar   Chin Chin Hills        NA  Pyine     State   NA\n  HASC_1 ISO_1                       geometry\n3  MM.CH    NA MULTIPOLYGON (((92.66836 21...\n\n\n\nconflict_sf\n\nSimple feature collection with 57198 features and 33 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 92.199 ymin: 9.9824 xmax: 100.3576 ymax: 27.731\nGeodetic CRS:  WGS 84\n# A tibble: 57,198 × 34\n   event_id_cnty event_date        year time_precision disorder_type  event_type\n * &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;     \n 1 MMR58558      16 February 2024  2024              1 Political vio… Battles   \n 2 MMR58559      16 February 2024  2024              1 Political vio… Battles   \n 3 MMR58443      16 February 2024  2024              2 Political vio… Violence …\n 4 MMR58502      16 February 2024  2024              1 Political vio… Violence …\n 5 MMR58507      16 February 2024  2024              1 Political vio… Explosion…\n 6 MMR58508      16 February 2024  2024              1 Political vio… Explosion…\n 7 MMR58547      16 February 2024  2024              1 Strategic dev… Strategic…\n 8 MMR58560      16 February 2024  2024              1 Political vio… Battles   \n 9 MMR58589      16 February 2024  2024              2 Political vio… Violence …\n10 MMR58648      16 February 2024  2024              1 Political vio… Explosion…\n# ℹ 57,188 more rows\n# ℹ 28 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, actor2 &lt;chr&gt;, assoc_actor_2 &lt;chr&gt;, inter2 &lt;dbl&gt;,\n#   interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;, region &lt;chr&gt;,\n#   country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;, location &lt;chr&gt;,\n#   geo_precision &lt;dbl&gt;, source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;,\n#   fatalities &lt;dbl&gt;, tags &lt;chr&gt;, timestamp &lt;dbl&gt;, population_1km &lt;dbl&gt;, …\n\n\n\nplot(st_geometry(conflict_sf))\n\n\n\n\n\nBattles &lt;- filter(conflict_sf, event_type == \"Battles\")\nViolence_CV &lt;- filter(conflict_sf, event_type == \"Violence against civilians\")\nProtests &lt;- filter(conflict_sf, event_type == \"Protests\")\nRiots &lt;- filter(conflict_sf, event_type == \"Riots\")\nExplosions &lt;- filter(conflict_sf, event_type == \"Explosions/Remote violence\")\nStrategic_dev &lt;- filter(conflict_sf, event_type == \"Strategic developments\")\n\n\nclass(Explosions)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1b/Take-home_Ex1b.html#visualising-of-fatalities-by-event-type",
    "href": "Take-home-ex/Take-home-Ex1b/Take-home_Ex1b.html#visualising-of-fatalities-by-event-type",
    "title": "Take-home Exercise 1b",
    "section": "",
    "text": "In shiny App , we will need to enable users to filter by\n\nspecific year,\nyear range, and\nevent_type\n\nUsing subsets of event types which have been converted to sf objects\n\nBattlesViolence against CiviliansProtestsRiotsExplosions/Remote Devices\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Battles) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Battles&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Violence_CV) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Violence on Civillians&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Protests) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Protests&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Riots) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Riots&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Explosions) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(popup = ~paste(\"Event: Explosions&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#loading-r-packages",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#loading-r-packages",
    "title": "Take-Home Exercise 1",
    "section": "1.1 Loading R packages",
    "text": "1.1 Loading R packages\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#importing-pisa-data",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#importing-pisa-data",
    "title": "Take-Home Exercise 1",
    "section": "1.2 Importing PISA data",
    "text": "1.2 Importing PISA data\nThe code chunk below uses ‘read_sas()’ of haven to import PISA data into the R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nUpon first import, as the student questionaire data file contains data from other countries, we will use filter() to filter the data file to only Singapore data.\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\nWe use write_rds() to save the filtered datafile to a seperate file called stu_qqq_SG\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")\n\nFor our analysis we shall read in data from stu_qqq_SG.rds using read_rds()\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#installing-and-loading-r-instvy",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#installing-and-loading-r-instvy",
    "title": "Take-Home Exercise 1",
    "section": "1.3 Installing and Loading R instvy",
    "text": "1.3 Installing and Loading R instvy\nThe R package intsvy allows R users to analyse PISA data among other international large-scale assessments. The use of PISA data via R requires data preparation, and intsvy offers a data transfer function to import data available in other formats directly into R. Intsvy also provides a merge function to merge the student, school, parent, teacher and cognitive databases.\nTo understand more about the packages available and the methodology to analyse the PISA data files, please refer to this link.\nThe analytical commands within intsvy enables users to derive mean statistics, standard deviations, frequency tables, correlation coefficients and regression estimates.\nAdditionally, intsvy deals with the calculation of point estimates and standard errors that take into account the complex PISA sample design with replicate weights, as well as the rotated test forms with plausible values.\nTo understand more about the instvy package, please refer to this link.\n\ninstall.packages(\"intsvy\",repos = \"http://cran.us.r-project.org\")\n\nWe will load the package using library()\n\nlibrary(\"intsvy\")"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#working-with-plausible-values",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#working-with-plausible-values",
    "title": "Take-Home Exercise 1",
    "section": "1.2.1 Working with Plausible Values",
    "text": "1.2.1 Working with Plausible Values\nPISA reports student performance through plausible values (PVs), obtained from Item Response Theory models (for details, see Chapter 5 of the PISA Data Analysis Manual: SAS or SPSS, Second Edition or the associated guide “Scaling of Cognitive Data and Use of Students Performance Estimates”).\nAn accurate and efficient way of measuring proficiency estimates in PISA requires five steps:\n\nCompute estimates for each Plausible Values (PV)\nCompute final estimate by averaging all estimates obtained from (1)\nCompute sampling variance (unbiased estimate are providing by using only one PV)\nCompute imputation variance (measurement error variance, estimated for each PV and then average over the set of PVs)\nCompute final standard error by combining (3) and (4)\n\nFor more information, please refer to this link.\nFor example, in order to obtain single mean scores in Math, Reading and Science for the student cohort, we can use the pisa.mean.pv() function from 'instvy' package like below.\n\nMath_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", data=stu_qqq_SG)\n\nRead_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", data=stu_qqq_SG)\n\nSCIE_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", data=stu_qqq_SG)\n\nBelow is an example of a single mean score for Math for the student cohort.\n\nprint(Math_mean_SG)\n\n  CNT Freq   Mean s.e.    SD  s.e\n1 SGP 6606 574.66 1.23 102.8 0.91\n\nprint(Read_mean_SG)\n\n  CNT Freq   Mean s.e.     SD  s.e\n1 SGP 6606 542.55 1.87 105.89 1.15\n\nprint(SCIE_mean_SG)\n\n  CNT Freq   Mean s.e.    SD s.e\n1 SGP 6606 561.43 1.33 99.09 1.1\n\n\nThese mean scores values can be corroborated at the following link (pages 310-315).\nFor a visual of the past performance of the mean scores for Singaporean students, please refer to this link.\nWe will use these calculated mean values from the 10 plausible values as a statistic (Mean) for some of our visualizations."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#distribution-of-students-performance-in-math-reading-and-science",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#distribution-of-students-performance-in-math-reading-and-science",
    "title": "Take-Home Exercise 1",
    "section": "1.3.1 Distribution of student’s performance in Math, Reading and Science",
    "text": "1.3.1 Distribution of student’s performance in Math, Reading and Science\nThe objective of this visualization is to examine if subject scores are normally distributed in general within the student population sampled in the PISA test.\nSince there are 10 Plausible Values for the 3 subjects, we shall use the first plausible value, PV1 to visualize the distribution of scores for the subjects.\nFor an example of precedence of using only one Plausible Value, please refer to the article on “How to deal with Plausible Values from International Large-scale assessments.”\nWe will use the below code to plot our histograms to show the distribution of scores across subjects.\n\n\nShow the code\n# Create the histogram plot with an annotated mean line using Math_mean_SG\nplt1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightblue') +\n  labs(x = \"PV1 Math Score\",\n       y = \"Frequency\") +\n  geom_vline(xintercept = Math_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Math_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create the histogram plot with an annotated mean line using Read_mean_SG\nplt2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightgreen') +\n  labs(x = \"PV1 Reading Score\",\n       y = \"Frequency\") +\n  geom_vline(xintercept = Read_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Read_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n\n# Create the histogram plot with an annotated mean line using Science_mean_SG\nplt3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightpink') +\n  labs(x = \"PV1 Science Score\",\n       y = \"Frequency\") +\n  geom_vline(xintercept = SCIE_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = SCIE_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(SCIE_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create a single plot with density plots for Math, Reading, and Science scores\nplt4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +\n  geom_density(alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +\n  labs(x = \"Scores\",\n       y = \"Density\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  guides(fill = FALSE) +  # Remove the legend\n  theme_minimal()\n\n\n\nCombined Visual of the distribution of scores in general\nWe will use patchwork to create a composite plot.\n\n\nShow the code\nlibrary(patchwork)\n\npatch1 &lt;- (plt1+plt2) / (plt3+plt4)  + \n              plot_annotation(\n                title = \"Distribution of student performance in Math, Reading and Science\")\n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 1\n\n\n\nThe distribution of scores seem to resemble a normal distribution across all 3 subjects. Singaporean students seem to have a higher mean score In Mathematics relative to Reading and Science.\nFurther statistical tests like the Anderson-Darling or Shapiro-Wilk tests will need to be conducted to confirm the normality in distribution."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-schools",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-schools",
    "title": "Take-Home Exercise 1",
    "section": "1.3.2 Relationship between Scores and Schools",
    "text": "1.3.2 Relationship between Scores and Schools\nThe objective of this visualization is to examine the relationship between subject scores and the schools sampled in the PISA test.\nWe will use unique() and length() to obtain the number of unique schools in the data set.\n\nunique_values &lt;- unique(stu_qqq_SG$CNTSCHID)\n\nlength(unique_values)\n\n[1] 164\n\n\nThere are 164 unique schools in this data set.\nNext, we use the code below to plot our scatter plots.\n\n\nShow the code\np1 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1MATH)) +\n  geom_point(color = \"lightblue\", alpha = 0.5) +\n  geom_hline(yintercept = Math_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +  \n  annotate(\"text\", x = Inf, y = Math_mean_SG$Mean, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2)), \n           hjust = 1, vjust = -1) +  \n  labs(x = \"School ID\",\n       y = \"PV1 Math Score\") +\n  theme_minimal()\n\np2 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1READ)) +\n  geom_point(color = \"lightgreen\", alpha = 0.5) +\n  geom_hline(yintercept = Read_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +  \n  annotate(\"text\", x = Inf, y = Read_mean_SG$Mean, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2)), \n           hjust = 1, vjust = -1) +  \n  labs(x = \"School ID\",\n       y = \"PV1 Reading Score\") +\n  theme_minimal()\n\np3 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1SCIE)) +\n  geom_point(color = \"lightpink\", alpha = 0.5) +\n  geom_hline(yintercept = SCIE_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +  \n  annotate(\"text\", x = Inf, y = SCIE_mean_SG$Mean, label = paste(\"Mean =\", round(SCIE_mean_SG$Mean, 2)), \n           hjust = 1, vjust = -1) +  \n  labs(x = \"School ID\",\n       y = \"PV1 Science Score\") +\n  theme_minimal()\n\n\n\nCombined Visual of the distribution of scores across Schools\nWe will use patchwork to create a composite plot.\n\n\nShow the code\npatch2 &lt;- p1/p2/p3 + \n              plot_annotation(\n                title = \"Students seem to be performing equally across Schools\")\n\npatch2 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 2\n\n\n\nAcross the 164 schools, students seem to be performing equally across the 3 subjects. There are no significant clusters that are different from each other, for example, a large number of schools which only have good scores or only poor scores.\nAdditional analysis could be done to examine if the highest and lowest performing students (in terms of scores) belong to the same type of schools.\nSince this data set only contains students on students, there is no additional information on either the type of school or its resources.. Further analysis could incorporate other data sets to build a more complete analysis."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-gender",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-gender",
    "title": "Take-Home Exercise 1",
    "section": "1.3.3 Relationship between Scores and Gender",
    "text": "1.3.3 Relationship between Scores and Gender\nThe objective of this visualization is to examine the relationship between subject scores and gender within the students sampled in the PISA test.\nThe gender column of the data set is named as “ST004D01T” with values of 1=Female and 2=Male.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by gender.\nIn the code below, we will create separate tables for the mean scores for each subject by different genders.\n\n\nShow the code\nMath_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"ST004D01T\", data = stu_qqq_SG)\n\nMath_gender$ST004D01T &lt;- factor(Math_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nRead_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"ST004D01T\", data = stu_qqq_SG)\n\nRead_gender$ST004D01T &lt;- factor(Read_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nSCIE_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"ST004D01T\", data = stu_qqq_SG)\n\nSCIE_gender$ST004D01T &lt;- factor(SCIE_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\n\nBelow is how the new tables look like for mean Math score grouped by gender. We can use the mean statistic here as an additional statistic for our box plots.\n\nprint(Math_gender)\n\n  ST004D01T Freq   Mean s.e.     SD  s.e\n1    Female 3248 568.49 1.65  97.62 1.14\n2      Male 3358 580.59 1.75 107.20 1.33\n\n\nNext, we plot the PV1 scores by different genders to examine the performance of different genders across subjects.\n\n\nShow the code\n# Create a subset of the data with gender and PV1 score columns\nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)\n\n# Convert the \"ST004D01T\" column to a factor \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\n# Create the plot using the subset_data\nbxp1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +\n  geom_boxplot() +\n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\",\n       y = \"PV1 Math Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +  \n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove the legend\n\n\nbxp2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +\n  geom_boxplot() +\n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Read_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\",\n       y = \"PV1 Reading Score\") +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightgreen\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove the legend\n\nbxp3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +\n  geom_boxplot() +\n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = SCIE_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\",\n       y = \"PV1 Science Score\") +\n  scale_fill_manual(values = c(\"lightpink\", \"lightpink\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove the legend\n\n\n\nCombined Visual of Performance across Genders\nWe will use the code below to create a composite plot for our box plots.\n\n\nShow the code\npatch3 &lt;- bxp1 + bxp2 + bxp3 + \n              plot_annotation(\n                title = \"Male students outperform in Maths and Science\")\n\npatch3 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 3\n\n\n\nMale students seem to outperform Female students in both Maths and Science with mean scores of 580.59 and 564.81 respectively. Female students seem to outperform Male students in Reading with a mean score of 552.55."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-socioeconomic-status-of-students",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-socioeconomic-status-of-students",
    "title": "Take-Home Exercise 1",
    "section": "1.3.4 Relationship between Scores and Socioeconomic status of students",
    "text": "1.3.4 Relationship between Scores and Socioeconomic status of students\nThe socioeconomic status of students is represented by the “ESCS” score in the PISA data set. The ESCS score is a continuous variable and is calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS). A higher ESCS score translates to a “better-off” student.\nFurther break down on the 3 main components of the ESCS score is shown in the diagram below. For further information on the computation methodology, please refer to the PISA 2022 Technical report: Chapter 19.\n\n\n\nComputation of ESCS in PISA 2022\n\n\nThe objective of this visualization is to examine the relationship between subject scores and a student’s socioeconomic status.\nFirst we check for any missing values in the ESCS column using the code below.\n\n# Check for NAs in the 'ESCS' column\nhas_nas &lt;- any(is.na(stu_qqq_SG$ESCS))\n\nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the ESCS column, we shall delete the rows with missing ESCS values. We will create a new subset with ESCS and the PV1 scores for this visualization.\n\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)\n\n#omiting NA values\nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\nUsing our new table subset_ESCS_PV1, we will create scatter plots for ESCS versus each PV1 score for each subject using the code below.\n\n\nShow the code\nc_coeff_ESCS_Math &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1MATH)\n\nC_plt1 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1MATH)) +\n  geom_point(color = \"lightblue\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1MATH),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Math, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n       y = \"PV1 Math Score\") +\n  theme_minimal()\n\nc_coeff_ESCS_Read &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1READ)\n\nC_plt2 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1READ)) +\n  geom_point(color = \"lightgreen\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1READ),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Read, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n       y = \"PV1 Read Score\") +\n  theme_minimal()\n\nc_coeff_ESCS_Scie &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1SCIE)\n\nC_plt3 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1SCIE)) +\n  geom_point(color = \"lightpink\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1SCIE),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Scie, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n       y = \"PV1 Science Score\") +\n  theme_minimal()\n\n\n\nCombined Scatter plots of PV1 Scores Vs ESCS scores\nWe will use patchwork to create a composite plot for our scatter plots.\n\n\nShow the code\npatch4 &lt;- C_plt1 / C_plt2 / C_plt3 + \n              plot_annotation(\n                title = \"Weak positive relationship between Scores and ESCS\")\n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 4\n\n\n\nThere is a weak positive relationship between subject scores and Socioeconomic statuses. The ESCS score is a composite score calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS). It could be likely that the larger number of constituents has ‘diluted’ the score, where the effect is more prominent for developed countries like Singapore.\n\nFurther analysis could be conducted on the individual components of the ESCS score to check for their individual influence on student performance."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-the-years-of-education-for-parents",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#relationship-between-scores-and-the-years-of-education-for-parents",
    "title": "Take-Home Exercise 1",
    "section": "1.3.5 Relationship between Scores and the years of Education for Parents",
    "text": "1.3.5 Relationship between Scores and the years of Education for Parents\nAs highlighted in the previous analysis, there is a weak positive relationship between student scores and ESCS scores. The objective of this visualization is to examine the relationship between one of the constituents, PAREDINT and student scores.\nPAREDINT is the index of the highest education of parents in years, based on the median cumulative years of education completed. The variable values are discrete and ranges from a scale of 3 to 16 years. For more information on this variable, please refer to the PISA 2022 Technical report: Chapter 19.\n\nFirst we check for any missing values in the ESCS column using the code below.\n\n# Check for NAs in the 'PAREDINT' column\nhas_nas &lt;- any(is.na(stu_qqq_SG$PAREDINT))\n\nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the PAREDINT column, we shall delete the rows with missing PAREDINT values. We will then create a new table with PAREDINT and the Mean scores of the 10 Plausible Values for this visualization.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the years of Parents education (PARENDINT).\nIn the code below, we will create separate tables for the mean scores for each subject by different years of Education.\n\n\nShow the code\nParents_edu_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"PAREDINT\", data = stu_qqq_SG)\n\nParents_edu_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"PAREDINT\", data = stu_qqq_SG)\n\nParents_edu_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"PAREDINT\", data = stu_qqq_SG)\n\n\nWe examine the one of the new tables, Parents_edu_math.\n\nprint(Parents_edu_math)\n\n  PAREDINT Freq   Mean  s.e.     SD   s.e\n1        3    8 482.25 53.59 131.99 25.91\n2        6   62 500.76 12.97 102.52  9.03\n3        9  127 540.05  8.87  98.63  6.43\n4       12 1470 530.62  3.14 100.32  1.75\n5     14.5 1213 559.85  3.00  97.10  1.95\n6       16 3669 600.47  1.68  97.13  1.37\n7     &lt;NA&gt;   57 485.92 11.61  85.05  9.59\n\n\nSince there 57 rows with missing values, we will delete the rows with missing values.\n\nParents_edu_math &lt;- na.omit(Parents_edu_math)\nParents_edu_read &lt;- na.omit(Parents_edu_read)\nParents_edu_scie &lt;- na.omit(Parents_edu_scie)\n\nNext, we use the below code to plot dot plots for each subject.\n\n\n\nShow the code\n# Create a dot plot with annotations\nDp1 &lt;- ggplot(Parents_edu_math, aes(x = as.factor(PAREDINT), y = Mean)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +\n  geom_line(aes(group = 1), color = \"lightblue\", size = 1, alpha = 0.5) +\n  geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels\n  labs(title = \"Math\",\n       x = \"Education (Yrs)\")+ \n  theme_minimal()\n\n\nDp2 &lt;- ggplot(Parents_edu_read, aes(x = as.factor(PAREDINT), y = Mean)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +\n  geom_line(aes(group = 1), color = \"lightgreen\", size = 1, alpha = 0.5) +\n  geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels\n  labs(title = \"Reading\",\n       x = \"Education (Yrs)\")+ \n  theme_minimal()\n\nDp3 &lt;- ggplot(Parents_edu_scie, aes(x = as.factor(PAREDINT), y = Mean)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +\n  geom_line(aes(group = 1), color = \"lightpink\", size = 1, alpha = 0.5) +\n  geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels\n  labs(title = \"Science\",\n       x = \"Education (Yrs)\")+ \n  theme_minimal()\n\n\n\nCombined dot plots of Subject Mean Scores Vs Parents Education years\nWe will use the code below to create a composite plot.\n\n\nShow the code\npatch4 &lt;- (Dp1 + Dp2 + Dp3\n               ) + \n              plot_annotation(\n                title = \"Parents with more education years seem to have children with higher scores\")\n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 5\n\n\n\nIn general, students seem to have performed better across all subjects the more their parents have been educated. However this factor alone is likely insufficient to cause a better performance.\nAlso, there is a slight drop off in mean scores from 9-12 years of Parents Education\nAdditional analyses taking into account the state of the study environment, both at home and in school, as well as the emotional aspects and motivation of students could be further analysed to derive more complete insights on the factors that could influence performance."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#additional-eda",
    "href": "Take-home-ex/Take-home-Ex1/Take-home_Ex1.html#additional-eda",
    "title": "Take-Home Exercise 1",
    "section": "1.3.6 Additional EDA",
    "text": "1.3.6 Additional EDA\n\n1) Examining closer into Mean scores per School\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the School ID (CNTSCHID).\nIn the code below, we will create separate tables for the mean scores for each subject by different School Ids.\n\n\nShow the code\nSchoolid_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\n\nWe examine the new tables created. In these new tables we are able to extract the number of students and the mean scores per School.\n\nhead(Schoolid_math)\n\n  CNTSCHID Freq   Mean  s.e.    SD   s.e\n1 70200001   55 725.21  9.34 59.23  6.38\n2 70200002   38 534.22 16.75 89.96 13.84\n3 70200003   36 739.92 12.30 59.23  7.70\n4 70200004   56 509.61 12.84 86.63  7.71\n5 70200005   38 546.52 12.95 86.04  8.86\n6 70200006   36 485.30 13.90 76.47  8.86\n\n\nNext, we use the below code to plot bubble plots to examine the number of students and their mean scores for each school. We will also use the plotly package for added interactivity.\n\n#install.packages(\"plotly\")\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(plotly)\n\np_1 &lt;- ggplot(Schoolid_math, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"skyblue\", high = \"darkblue\") +\n  labs(title = \"Mean Math Scores by School ID\",\n    y = \"Mean Math Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Convert to an interactive plot\nggplotly(p_1, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(plotly)\n\np_2 &lt;- ggplot(Schoolid_read, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"yellow\", high = \"darkorchid\") +\n  labs(title = \"Mean Reading Scores by School ID\",\n    y = \"Mean Reading Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Convert to an interactive plot\nggplotly(p_2, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(plotly)\n\np_3 &lt;- ggplot(Schoolid_scie, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"lightpink\", high = \"darkred\") +\n  labs(title = \"Mean Science Scores by School ID\",\n    y = \"Mean Science Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Convert to an interactive plot\nggplotly(p_3, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 6\n\n\n\nThe ability to extract and assign Mean scores to individual schools enables us to further explore and examine the disparity in performance between schools. For example, looking at the two extremes of score results, we note that Schools (70200001 & 70200003) out perform other schools in all subjects. On the other hand, Schools (7020115 & 70200149) under perform other schools in all subjects.\n\nThis shows that there are still marked differences between the ‘’best” schools and the’‘worst’’ schools. Additional analysis should be done to identify the differences between these two sets of schools in terms of resources, teaching quality, and students attitudes or motivation to fully understand the differences between the scores.\n\n\n\n\n2) Examining the Breakdown of scores per Subject\nPreviously we had only examined the distribution of marks for the student population across the 3 subjects (whether resembles normal distribution).\nWe can further examine the percentage of students per score range for each subject. This might help us examine whether there are specific strengths or weaknesses in the student cohort.\nFirst, we use the pisa.ben.pv() function from the instvy package which calculates student scores from the 10 plausible values and calculates the percentage of students at each proficiency level (Score range) as defined by PISA.\nIn the code below, we will create separate tables for the percentage breakdown of scores for each subject.\n\n\nShow the code\nMath_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nRead_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nScie_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\n\nWe examine the new tables created. In these new tables we are able to obtain the percentage breakdown of students per score range for our next visualization.\n\nprint(Math_Breakdown)\n\n  CNT       Benchmarks Percentage Std. err.\n1 SGP        &lt;= 357.77       2.17      0.22\n2 SGP (357.77, 420.07]       5.85      0.38\n3 SGP (420.07, 482.38]      11.25      0.59\n4 SGP (482.38, 544.68]      17.59      0.61\n5 SGP (544.68, 606.99]      22.62      0.69\n6 SGP  (606.99, 669.3]      21.96      0.69\n7 SGP          &gt; 669.3      18.56      0.52\n\n\nWe will plot bar charts for the percentage of students per Score range for each subject.\n\nMath Scores breakdownReading Scores breakdownScience Scores breakdown\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Math_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Math\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Read_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Reading\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Scie_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightpink\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Science\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNext, we can combine these tables and plots into one plot to show the percentage of students per Score Range for all subjects.\n\n\nShow the code\n# Creating a new combined table\n\nMath_Breakdown$Subject &lt;- 'Math'\nRead_Breakdown$Subject &lt;- 'Reading'\nScie_Breakdown$Subject &lt;- 'Science'\n\nCombined_Breakdown &lt;- bind_rows(Math_Breakdown, Read_Breakdown, Scie_Breakdown)\n\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_Breakdown &lt;- Combined_Breakdown %&gt;%\n  mutate(Benchmarks = fct_inorder(Benchmarks))\n\n# Now plot using ggplot\np &lt;- ggplot(Combined_Breakdown, aes(x = Benchmarks, y = Percentage, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +  # Dodge position for the bars\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percentage)),  # This will format the label to have 1 decimal place and a percentage sign\n    position = position_dodge(width = 0.9),  # Match the position of the text with the dodged bars\n    vjust = -0.25,   \n    size = 2  \n  ) +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"For lower Score ranges, students seem to do better in Reading\",\n       x = \"Score Range\",\n       y = \"Percentage of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert to interactive plotly object\np_interactive &lt;- ggplotly(p, tooltip = c(\"x\", \"y\", \"fill\", \"text\"))\n\n# Print the interactive plot\np_interactive\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 7\n\n\n\n1) 63.14 % of students scored 544.68 and above for Math\n2) 52.45% of students scored 544.68 and above for Reading\n3) 59.65 % of students scored 544.68 and above for Science\n\nIf we were to select a baseline score as 544.68, we can see that students generally do better in Math relative to Science and Reading.\nWhile individual plots are useful to visualize the break down of students across scores, we can also combine them into a single bar plot to obtain insights on relative performance. From the combined plot, we can see that at lower score ranges, students seem to do better in Reading relative to Math and Science. However, at higher score ranges, students do worse in Reading relative to Math and Science.\n\n\n\n\n3) Violin plots for Gender performance across subjects\nPreviously we had used box plots to visualize the difference in performance between the genders across subjects. The code below ‘switches’ to violin plots instead.\n\n\nShow the code\n# Create the plot using the subset_data\nvx_plot1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = Math_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Math Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\nvx_plot2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = Read_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Reading Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightgreen\", \"Male\" = \"lightgreen\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\nvx_plot3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = SCIE_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Science Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightpink\", \"Male\" = \"lightpink\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\n\nWe will use the code below to do a composite plot for all 3 subjects\n\n\nShow the code\npatch5 &lt;- vx_plot1 + vx_plot2 + vx_plot3 + \n              plot_annotation(\n                title = \"Male students outperform in Maths and Science\")\n\npatch5 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nViolin plots may provide a fuller picture of the data distribution, revealing density and multimodality that box plots may obscure. This could be useful for detecting data patterns and clusters.\nViolin plots could also emphasize the concentration of data points and potential outliers, showcasing the entire range of the data including peaks, valleys, and tails that may not be evident from box plots.\n\n\n\n\n4) Relationship between Scores and the times spent on studying or homework before or after school\nThe objective of this visualization is to examine the relationship between student scores and the time spent on studying or on homework (STUDYHMW).\nThe variable “STUDYHMW” measures how many times during a typical school week students studied for school or homework before going to school and/or after leaving school. Values on this index range from 0 (no studying) to 10 (10 or more times of studying per week).\nFirst we check for any missing values in the STUDYHMW column using the code below.\n\n# Check for NAs in the 'STUDYHMW' column\nhas_nas &lt;- any(is.na(stu_qqq_SG$STUDYHMW))\n\nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the STUDYHMW column, we shall delete the rows with missing values. We will then create a new table with STUDYHMW and the Mean scores of the 10 Plausible Values for this visualization.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the time spent on studying (STUDYHMW).\nIn the code below, we will create separate tables for the mean scores for each subject.\n\n\nShow the code\nhomework_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\nhomework_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\nhomework_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\n\nWe examine the new tables created.\n\nprint(homework_math)\n\n   STUDYHMW Freq   Mean  s.e.     SD  s.e\n1         0  274 514.96  7.21 111.29 4.17\n2         1  154 537.23  9.42 112.48 6.16\n3         2  422 540.72  5.89 110.16 3.82\n4         3  500 566.21  4.89 105.07 3.27\n5         4  602 569.18  4.54 104.79 3.12\n6         5 1750 616.13  2.13  90.56 1.64\n7         6  568 551.80  4.29 103.99 3.02\n8         7  415 577.30  4.96  97.93 3.69\n9         8  416 561.92  4.83  94.31 3.86\n10        9  163 554.53  8.07  91.50 5.98\n11       10 1296 570.87  2.45  95.76 2.11\n12     &lt;NA&gt;   46 486.26 11.65  77.81 8.28\n\n\nSince there 46 rows with missing values, we will delete the rows with missing values.\n\nhomework_math &lt;- na.omit(homework_math)\nhomework_read &lt;- na.omit(homework_read)\nhomework_scie &lt;- na.omit(homework_scie)\n\nNext, we can combine these tables to be able to plot one graph to show the mean scores across the time spent on homework and studying, for all subjects.\n\nhomework_math$Subject &lt;- 'Math'\nhomework_read$Subject &lt;- 'Reading'\nhomework_scie$Subject &lt;- 'Science'\n\nCombined_homework &lt;- bind_rows(homework_math, homework_read, homework_scie)\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_homework &lt;- Combined_homework %&gt;%\n  mutate(STUDYHMW = fct_inorder(STUDYHMW))\n\n# Now plot using ggplot\np2 &lt;- ggplot(Combined_homework, aes(x = STUDYHMW, y = Mean, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"Students who study more may not necessarily score higher\",\n       x = \"Number of times in a week\",\n       y = \"Mean Scores\") +\n  theme_minimal() \n\n# Convert to interactive plotly object\np_interactive2 &lt;- ggplotly(p2, tooltip = c(\"x\", \"y\", \"fill\", \"text\"))\n\n# Print the interactive plot\np_interactive2\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 8\n\n\n\nFrom the combined plot, it appears that the optimal number of times to study in a week is 5 times for all subjects. More times spent on homework and/or studying does not seem to translate to higher scores for this PISA test. However this factor alone is insufficient to conclude causality, and we should do additional analysis on other factors that may impact a student’s ability to prepare for this test."
  },
  {
    "objectID": "In-class-ex/In-class-Ex4/In-class_Ex4.html",
    "href": "In-class-ex/In-class-Ex4/In-class_Ex4.html",
    "title": "In-Class Exercise 7 - Isohyet Map for Rainfall Data",
    "section": "",
    "text": "In this in-class exercise, beside tidyverse, viridis, sf and tmap libaries, two new R packages will be used, they are:\n\nterra is a replacement of the raster package. It has a very similar, but simpler, interface, and it is faster than raster. In this hands-on exercise, it will be used to create grid (also known as raster) objects as the input and output of spatial interpolation.\ngstat, an r packages for spatial and spatio-temporal geostatistical modelling, prediction and simulation. In this in-class exercise, it will be used to perform spatial interpolation.\nautomap, an r package for performing automatic variogram modelling and kriging interpolation.\n\n\n\n\npacman::p_load(sf, terra, gstat, automap,\n               tmap, viridis, tidyverse)\n\nThree data sets will be used in this exercise, they are:\n\nRainfallStation.csv provides location information of existing rainfall stations in Singapore. The data is downloaded from Meteological Service Singapore.\nDAILYDATA_202402.csv provides weather data are rainfall stations for the month February, 2024. The data is also downloaded from Meteological Service Singapore.\nMPSZ-2019 contains planning subzone boundary of URA Master Plan 2019. It is downloaded from data.gov.sg. The original data is in kml format.\n\n\n\nIn the code below, read_csv() of readr package is used to import RainfallStation.csv. rfstations, the output object is in tibble data.frame format.\n\nrfstation &lt;- read_csv(\"data/aspatial/RainfallStation.csv\")\n\n\n\n\nIn the code chunk below, read_csv() of readr package is used to import DAILYDATA_202402.csv. rfdata, the output object is in tibble data.frame format.\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1, 5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\nselect() of dplyr package is used to retain column 1 and 5 of the input data.\ngroup_by() and summarise() of dplyr package are used to compute the total monthly rainfall from Daily Rainfall Total (mm) field. The output is stored in a new field called MONTHSUM.\n\n\n\n\n\n\nNext, left_join() of dplyr is used to join rfstations to rfdata by using the code chunk below.\n\nrfdata &lt;-  rfdata%&gt;%\n  left_join(rfstation)\n\n\n\n\n\n\n\nNote\n\n\n\nBecause Station field is available in both rfstations and rfdata, by() argument of left_join() is not needed.\n\n\nIn the code below, st_as_sf() of sf package is used to convert rfdata into a simple feature data.frame object called rfdata_sf.\n\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\nNote\n\n\n\n\nFor coords argument, it is important to map the X (i.e. Longitude) first, then follow by the Y (i.e. Latitude).\ncrs = 4326 indicates that the source data is in wgs84 coordinates system.\nst_transform() of sf package is then used to transform the source data from wgs84 to svy21 projected coordinates system.\nsvy21 is the official projected coordinates of Singapore. 3414 is the EPSG code of svy21.\n\n\n\n\n\n\nIn the code below, st_read() of sf package is used to import MPSZ-2019 shapefile into R. The output is called mpsz2019. It is in polygon feature tibble data.frame format.\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\imranmi\\ISSS608-VAA\\In-class-ex\\In-class-Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe source data is in wgs84 coordinates system, hence st_tranform() of sf package is used to theo output sf data.frame into svy21 project coordinates system.\n\n\n\n\n\n\nIt is always a good practice to visualise the data prepared. In the code chunk below, tmap functions are used to create a dot map showing locations of rainfall station in Singapore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(rfdata_sf) +\n  tm_dots(col = \"red\")\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nIn the code below, tmap functions are used to create a quantitative dot map of rainfall distribution by rainfall station in Singaspore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019) +\n  tm_borders() +\ntm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\n\nIn this section, we will gain hands-on experience on performing spatial interpolation by using gstat package. In order to perform spatial interpolation by using gstat, we first need to create an object of class called gstat, using a function of the same name: gstat. A gstat object contains all necessary information to conduct spatial interpolation, namely:\n\nThe model definition\nThe calibration data\n\nBased on its arguments, the gstat function “understands” what type of interpolation model we want to use:\n\nNo variogram model → IDW\nVariogram model, no covariates → Ordinary Kriging\nVariogram model, with covariates → Universal Kriging\n\nThe complete decision tree of gstat, including several additional methods which we are not going to use, is shown in the figure below.\n\n\n\nTo getting start, we need create a grid data object by using rast() of terra package as shown in the cod chunk below.\n\ngrid &lt;- terra::rast(mpsz2019, \n                    nrows = 690, \n                    ncols = 1075)\ngrid\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \n\n\nNext, a list called xy will be created by using xyFromCell() of terra package.\n\nxy &lt;- terra::xyFromCell(grid, \n                        1:ncell(grid))\nhead(xy)\n\n            x        y\n[1,] 2692.528 50231.33\n[2,] 2742.509 50231.33\n[3,] 2792.489 50231.33\n[4,] 2842.469 50231.33\n[5,] 2892.450 50231.33\n[6,] 2942.430 50231.33\n\n\n\n\n\n\n\n\nNote\n\n\n\nxyFromCell() gets coordinates of the center of raster cells for a row, column, or cell number of a SpatRaster. Or get row, column, or cell numbers from coordinates or from each other.\n\n\nLastly, we will create a data frame called coop with prediction/simulation locations by using the code chunk below.\n\ncoop &lt;- st_as_sf(as.data.frame(xy), \n                 coords = c(\"x\", \"y\"),\n                 crs = st_crs(mpsz2019))\ncoop &lt;- st_filter(coop, mpsz2019)\nhead(coop)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25883.42 ymin: 50231.33 xmax: 26133.32 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\n                   geometry\n1 POINT (25883.42 50231.33)\n2  POINT (25933.4 50231.33)\n3 POINT (25983.38 50231.33)\n4 POINT (26033.36 50231.33)\n5 POINT (26083.34 50231.33)\n6 POINT (26133.32 50231.33)\n\n\n\n\n\n\n\n\nIn the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create.\n\nWeighting is assigned to sample points through the use of a weighting coefficient that controls how the weighting influence will drop off as the distance from new point increases. The greater the weighting coefficient, the less the effect points will have if they are far from the unknown point during the interpolation process. As the coefficient increases, the value of the unknown point approaches the value of the nearest observational point.\nIt is important to notice that the IDW interpolation method also has some disadvantages: the quality of the interpolation result can decrease, if the distribution of sample data points is uneven. Furthermore, maximum and minimum values in the interpolated surface can only occur at sample data points. This often results in small peaks and pits around the sample data points.\n\n\n\nWe are going to use three parameters of the gstat function:\n\nformula: The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata: The calibration data\nmodel: The variogram model\n\nKeep in mind that we need to specify parameter names, because these three parameters are not the first three in the gstat function definition.\nFor example, to interpolate using the IDW method we create the following gstat object, specifying just the formula and data:\ng = gstat(formula = annual ~ 1, data = rainfall)\n\n\n\n\n\n\nTip\n\n\n\nIn R, formula objects are used to specify relation between objects, in particular—the role of different data columns in statistical models. A formula object is created using the ~ operator, which separates names of dependent variables (to the left of the ~ symbol) and independent variables (to the right of the ~ symbol). Writing 1 to the right of the ~ symbol, as in ~ 1, means that there are no independent variables38.\n\n\nIn the code chunk below,\n\nres &lt;- gstat(formula = MONTHSUM ~ 1, \n             locations = rfdata_sf, \n             nmax = 5,\n             set = list(idp = 0))\n\nNow that our model is defined, we can use predict() to actually interpolate, i.e., to calculate predicted values. The predict function accepts:\n\nA raster—stars object, such as dem\nA model—gstat object, such as g\n\nThe raster serves for two purposes:\n\nSpecifying the locations where we want to make predictions (in all methods), and\nSpecifying covariate values (in Universal Kriging only).\n\n\nresp &lt;- predict(res, coop)\n\n[inverse distance weighted interpolation]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\npred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\", \n                         fun = \"mean\")\n\nNow, we will map the interpolated surface by using tmap functions as shown in the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\")\n\n\n\n\n\n\n\n\n\n\nKriging is one of several methods that use a limited set of sampled data points to estimate the value of a variable over a continuous spatial field. An example of a value that varies across a random spatial field might be total monthly rainfall over Singapore. It differs from Inverse Distance Weighted Interpolation discussed earlier in that it uses the spatial correlation between sampled points to interpolate the values in the spatial field: the interpolation is based on the spatial arrangement of the empirical observations, rather than on a presumed model of spatial distribution. Kriging also generates estimates of the uncertainty surrounding each interpolated value.\nIn a general sense, the kriging weights are calculated such that points nearby to the location of interest are given more weight than those farther away. Clustering of points is also taken into account, so that clusters of points are weighted less heavily (in effect, they contain less information than single points). This helps to reduce bias in the predictions.\nThe kriging predictor is an “optimal linear predictor” and an exact interpolator, meaning that each interpolated value is calculated to minimize the prediction error for that point. The value that is generated from the kriging process for any actually sampled location will be equal to the observed value at this point, and all the interpolated values will be the Best Linear Unbiased Predictors (BLUPs).\nKriging will in general not be more effective than simpler methods of interpolation if there is little spatial autocorrelation among the sampled data points (that is, if the values do not co-vary in space). If there is at least moderate spatial autocorrelation, however, kriging can be a helpful method to preserve spatial variability that would be lost using a simpler method (for an example, see Auchincloss 2007, below).\nKriging can be understood as a two-step process:\n\nfirst, the spatial covariance structure of the sampled points is determined by fitting a variogram; and\nsecond, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.\n\nKriging methods require a variogram model. A variogram (sometimes called a “semivariogram”) is a visual depiction of the covariance exhibited between each pair of points in the sampled data. For each pair of points in the sampled data, the gamma-value or “semi-variance” (a measure of the half mean-squared difference between their values) is plotted against the distance, or “lag”, between them. The “experimental” variogram is the plot of observed values, while the “theoretical” or “model” variogram is the distributional model that best fits the data.\n\n\n\n\nFirstly, we will calculate and examine the empirical variogram by using variogram() of gstat package. The function requires two arguments:\n\nformula, the dependent variable and the covariates (same as in gstat, see Section 12.2.1)\ndata, a point layer with the dependent variable and covariates as attributes\n\nas shown in the code chunk below.\n\nv &lt;- variogram(MONTHSUM ~ 1, \n               data = rfdata_sf)\nplot(v)\n\n\n\n\nWe can then compare the plot with the theoretical models below.\n\nWith reference to the comparison above, am empirical variogram model will be fitted by using fit.variogram() of gstat package as shown in the code chunk below.\n\nfv &lt;- fit.variogram(object = v,\n                    model = vgm(\n                      psill = 0.5, \n                      model = \"Sph\",\n                      range = 5000, \n                      nugget = 0.1))\nfv\n\n  model     psill    range\n1   Nug 0.1129190    0.000\n2   Sph 0.5292397 5213.396\n\n\nWe can visualise how well the observed data fit the model by plotting fv using the code chunk below.\n\nplot(v, fv)\n\n\n\n\nThe plot above reveals that the empirical model fits rather well. In view of this, we will go ahead to perform spatial interpolation by using the newly derived model as shown in the code chunk below.\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           data = rfdata_sf, \n           model = fv)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model     psill    range\nvar1[1]   Nug 0.1129190    0.000\nvar1[2]   Sph 0.5292397 5213.396\n\n\nOnce we are happy with the results, predict() of gstat package will be used to estimate the unknown grids by using the code chunk below.\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\nresp\n\nSimple feature collection with 314019 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2692.528 ymin: 15773.73 xmax: 56371.45 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   var1.pred  var1.var                  geometry        x        y     pred\n1   131.0667 0.6608399 POINT (25883.42 50231.33) 25883.42 50231.33 131.0667\n2   130.9986 0.6610337  POINT (25933.4 50231.33) 25933.40 50231.33 130.9986\n3   130.9330 0.6612129 POINT (25983.38 50231.33) 25983.38 50231.33 130.9330\n4   130.8698 0.6613782 POINT (26033.36 50231.33) 26033.36 50231.33 130.8698\n5   130.8092 0.6615303 POINT (26083.34 50231.33) 26083.34 50231.33 130.8092\n6   130.7514 0.6616697 POINT (26133.32 50231.33) 26133.32 50231.33 130.7514\n7   130.6965 0.6617971  POINT (26183.3 50231.33) 26183.30 50231.33 130.6965\n8   130.6446 0.6619131 POINT (26233.28 50231.33) 26233.28 50231.33 130.6446\n9   130.5958 0.6620184 POINT (26283.26 50231.33) 26283.26 50231.33 130.5958\n10  132.5484 0.6542154 POINT (25033.76 50181.32) 25033.76 50181.32 132.5484\n\n\n\n\n\n\n\n\nNote\n\n\n\nresp is a sf tibble data.frame with point features.\n\n\nIn order to create a raster surface data object, rasterize() of terra is used as shown in the code chunk below.\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\nkpred\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :      last \nmin value   :  72.77826 \nmax value   : 195.53284 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output object kpred is in SpatRaster object class with a spatial resolution of 50m x 50m. It consists of 1075 columns and 690 rows and in SVY21 projected coordinates system.\n\n\n\n\n\nFinally, tmap functions are used to map the interpolated rainfall raster (i.e. kpred) by using the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\nBeside using gstat to perform variogram modelling manually, autofirVariogram() of automap package can be used to perform varigram modelling as shown in the code chunk below.\n\nv_auto &lt;- autofitVariogram(MONTHSUM ~ 1, \n                           rfdata_sf)\nplot(v_auto)\n\n\n\n\n\nv_auto\n\n$exp_var\n   np      dist     gamma dir.hor dir.ver   id\n1  15  1957.436  311.9613       0       0 var1\n2  33  3307.349  707.7685       0       0 var1\n3  54  4861.368  848.1314       0       0 var1\n4 116  6716.531  730.3969       0       0 var1\n5 111  9235.708 1006.5381       0       0 var1\n6 120 11730.199 1167.5988       0       0 var1\n7 135 14384.636 1533.5903       0       0 var1\n\n$var_model\n  model    psill   range kappa\n1   Nug     0.00       0   0.0\n2   Ste 24100.71 1647955   0.3\n\n$sserr\n[1] 0.2178294\n\nattr(,\"class\")\n[1] \"autofitVariogram\" \"list\"            \n\n\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           model = v_auto$var_model,\n           data = rfdata_sf)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model    psill   range kappa\nvar1[1]   Nug     0.00       0   0.0\nvar1[2]   Ste 24100.71 1647955   0.3\n\n\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\nOlea, Ricardo A. (2006-07) “A six-step practical approach to semivariogram modeling”, Stochastic Environmental Research and Risk Assessment, 2006-07, Vol.20 (5), p.307-318. SMU e-journal."
  },
  {
    "objectID": "In-class-ex/In-class-Ex4/In-class_Ex4.html#loading-r-packages-and-importing-data",
    "href": "In-class-ex/In-class-Ex4/In-class_Ex4.html#loading-r-packages-and-importing-data",
    "title": "In-Class Exercise 7 - Isohyet Map for Rainfall Data",
    "section": "",
    "text": "pacman::p_load(sf, terra, gstat, automap,\n               tmap, viridis, tidyverse)\n\nThree data sets will be used in this exercise, they are:\n\nRainfallStation.csv provides location information of existing rainfall stations in Singapore. The data is downloaded from Meteological Service Singapore.\nDAILYDATA_202402.csv provides weather data are rainfall stations for the month February, 2024. The data is also downloaded from Meteological Service Singapore.\nMPSZ-2019 contains planning subzone boundary of URA Master Plan 2019. It is downloaded from data.gov.sg. The original data is in kml format.\n\n\n\nIn the code below, read_csv() of readr package is used to import RainfallStation.csv. rfstations, the output object is in tibble data.frame format.\n\nrfstation &lt;- read_csv(\"data/aspatial/RainfallStation.csv\")\n\n\n\n\nIn the code chunk below, read_csv() of readr package is used to import DAILYDATA_202402.csv. rfdata, the output object is in tibble data.frame format.\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1, 5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\nselect() of dplyr package is used to retain column 1 and 5 of the input data.\ngroup_by() and summarise() of dplyr package are used to compute the total monthly rainfall from Daily Rainfall Total (mm) field. The output is stored in a new field called MONTHSUM.\n\n\n\n\n\n\nNext, left_join() of dplyr is used to join rfstations to rfdata by using the code chunk below.\n\nrfdata &lt;-  rfdata%&gt;%\n  left_join(rfstation)\n\n\n\n\n\n\n\nNote\n\n\n\nBecause Station field is available in both rfstations and rfdata, by() argument of left_join() is not needed.\n\n\nIn the code below, st_as_sf() of sf package is used to convert rfdata into a simple feature data.frame object called rfdata_sf.\n\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\nNote\n\n\n\n\nFor coords argument, it is important to map the X (i.e. Longitude) first, then follow by the Y (i.e. Latitude).\ncrs = 4326 indicates that the source data is in wgs84 coordinates system.\nst_transform() of sf package is then used to transform the source data from wgs84 to svy21 projected coordinates system.\nsvy21 is the official projected coordinates of Singapore. 3414 is the EPSG code of svy21.\n\n\n\n\n\n\nIn the code below, st_read() of sf package is used to import MPSZ-2019 shapefile into R. The output is called mpsz2019. It is in polygon feature tibble data.frame format.\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\imranmi\\ISSS608-VAA\\In-class-ex\\In-class-Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe source data is in wgs84 coordinates system, hence st_tranform() of sf package is used to theo output sf data.frame into svy21 project coordinates system.\n\n\n\n\n\n\nIt is always a good practice to visualise the data prepared. In the code chunk below, tmap functions are used to create a dot map showing locations of rainfall station in Singapore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(rfdata_sf) +\n  tm_dots(col = \"red\")\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nIn the code below, tmap functions are used to create a quantitative dot map of rainfall distribution by rainfall station in Singaspore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019) +\n  tm_borders() +\ntm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class-ex/In-class-Ex4/In-class_Ex4.html#spatial-interpolation-gstat-method",
    "href": "In-class-ex/In-class-Ex4/In-class_Ex4.html#spatial-interpolation-gstat-method",
    "title": "In-Class Exercise 7 - Isohyet Map for Rainfall Data",
    "section": "",
    "text": "In this section, we will gain hands-on experience on performing spatial interpolation by using gstat package. In order to perform spatial interpolation by using gstat, we first need to create an object of class called gstat, using a function of the same name: gstat. A gstat object contains all necessary information to conduct spatial interpolation, namely:\n\nThe model definition\nThe calibration data\n\nBased on its arguments, the gstat function “understands” what type of interpolation model we want to use:\n\nNo variogram model → IDW\nVariogram model, no covariates → Ordinary Kriging\nVariogram model, with covariates → Universal Kriging\n\nThe complete decision tree of gstat, including several additional methods which we are not going to use, is shown in the figure below.\n\n\n\nTo getting start, we need create a grid data object by using rast() of terra package as shown in the cod chunk below.\n\ngrid &lt;- terra::rast(mpsz2019, \n                    nrows = 690, \n                    ncols = 1075)\ngrid\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \n\n\nNext, a list called xy will be created by using xyFromCell() of terra package.\n\nxy &lt;- terra::xyFromCell(grid, \n                        1:ncell(grid))\nhead(xy)\n\n            x        y\n[1,] 2692.528 50231.33\n[2,] 2742.509 50231.33\n[3,] 2792.489 50231.33\n[4,] 2842.469 50231.33\n[5,] 2892.450 50231.33\n[6,] 2942.430 50231.33\n\n\n\n\n\n\n\n\nNote\n\n\n\nxyFromCell() gets coordinates of the center of raster cells for a row, column, or cell number of a SpatRaster. Or get row, column, or cell numbers from coordinates or from each other.\n\n\nLastly, we will create a data frame called coop with prediction/simulation locations by using the code chunk below.\n\ncoop &lt;- st_as_sf(as.data.frame(xy), \n                 coords = c(\"x\", \"y\"),\n                 crs = st_crs(mpsz2019))\ncoop &lt;- st_filter(coop, mpsz2019)\nhead(coop)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25883.42 ymin: 50231.33 xmax: 26133.32 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\n                   geometry\n1 POINT (25883.42 50231.33)\n2  POINT (25933.4 50231.33)\n3 POINT (25983.38 50231.33)\n4 POINT (26033.36 50231.33)\n5 POINT (26083.34 50231.33)\n6 POINT (26133.32 50231.33)"
  },
  {
    "objectID": "In-class-ex/In-class-Ex4/In-class_Ex4.html#inverse-distance-weighted-idw",
    "href": "In-class-ex/In-class-Ex4/In-class_Ex4.html#inverse-distance-weighted-idw",
    "title": "In-Class Exercise 7 - Isohyet Map for Rainfall Data",
    "section": "",
    "text": "In the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create.\n\nWeighting is assigned to sample points through the use of a weighting coefficient that controls how the weighting influence will drop off as the distance from new point increases. The greater the weighting coefficient, the less the effect points will have if they are far from the unknown point during the interpolation process. As the coefficient increases, the value of the unknown point approaches the value of the nearest observational point.\nIt is important to notice that the IDW interpolation method also has some disadvantages: the quality of the interpolation result can decrease, if the distribution of sample data points is uneven. Furthermore, maximum and minimum values in the interpolated surface can only occur at sample data points. This often results in small peaks and pits around the sample data points.\n\n\n\nWe are going to use three parameters of the gstat function:\n\nformula: The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata: The calibration data\nmodel: The variogram model\n\nKeep in mind that we need to specify parameter names, because these three parameters are not the first three in the gstat function definition.\nFor example, to interpolate using the IDW method we create the following gstat object, specifying just the formula and data:\ng = gstat(formula = annual ~ 1, data = rainfall)\n\n\n\n\n\n\nTip\n\n\n\nIn R, formula objects are used to specify relation between objects, in particular—the role of different data columns in statistical models. A formula object is created using the ~ operator, which separates names of dependent variables (to the left of the ~ symbol) and independent variables (to the right of the ~ symbol). Writing 1 to the right of the ~ symbol, as in ~ 1, means that there are no independent variables38.\n\n\nIn the code chunk below,\n\nres &lt;- gstat(formula = MONTHSUM ~ 1, \n             locations = rfdata_sf, \n             nmax = 5,\n             set = list(idp = 0))\n\nNow that our model is defined, we can use predict() to actually interpolate, i.e., to calculate predicted values. The predict function accepts:\n\nA raster—stars object, such as dem\nA model—gstat object, such as g\n\nThe raster serves for two purposes:\n\nSpecifying the locations where we want to make predictions (in all methods), and\nSpecifying covariate values (in Universal Kriging only).\n\n\nresp &lt;- predict(res, coop)\n\n[inverse distance weighted interpolation]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\npred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\", \n                         fun = \"mean\")\n\nNow, we will map the interpolated surface by using tmap functions as shown in the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\")"
  },
  {
    "objectID": "In-class-ex/In-class-Ex4/In-class_Ex4.html#kriging",
    "href": "In-class-ex/In-class-Ex4/In-class_Ex4.html#kriging",
    "title": "In-Class Exercise 7 - Isohyet Map for Rainfall Data",
    "section": "",
    "text": "Kriging is one of several methods that use a limited set of sampled data points to estimate the value of a variable over a continuous spatial field. An example of a value that varies across a random spatial field might be total monthly rainfall over Singapore. It differs from Inverse Distance Weighted Interpolation discussed earlier in that it uses the spatial correlation between sampled points to interpolate the values in the spatial field: the interpolation is based on the spatial arrangement of the empirical observations, rather than on a presumed model of spatial distribution. Kriging also generates estimates of the uncertainty surrounding each interpolated value.\nIn a general sense, the kriging weights are calculated such that points nearby to the location of interest are given more weight than those farther away. Clustering of points is also taken into account, so that clusters of points are weighted less heavily (in effect, they contain less information than single points). This helps to reduce bias in the predictions.\nThe kriging predictor is an “optimal linear predictor” and an exact interpolator, meaning that each interpolated value is calculated to minimize the prediction error for that point. The value that is generated from the kriging process for any actually sampled location will be equal to the observed value at this point, and all the interpolated values will be the Best Linear Unbiased Predictors (BLUPs).\nKriging will in general not be more effective than simpler methods of interpolation if there is little spatial autocorrelation among the sampled data points (that is, if the values do not co-vary in space). If there is at least moderate spatial autocorrelation, however, kriging can be a helpful method to preserve spatial variability that would be lost using a simpler method (for an example, see Auchincloss 2007, below).\nKriging can be understood as a two-step process:\n\nfirst, the spatial covariance structure of the sampled points is determined by fitting a variogram; and\nsecond, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.\n\nKriging methods require a variogram model. A variogram (sometimes called a “semivariogram”) is a visual depiction of the covariance exhibited between each pair of points in the sampled data. For each pair of points in the sampled data, the gamma-value or “semi-variance” (a measure of the half mean-squared difference between their values) is plotted against the distance, or “lag”, between them. The “experimental” variogram is the plot of observed values, while the “theoretical” or “model” variogram is the distributional model that best fits the data.\n\n\n\n\nFirstly, we will calculate and examine the empirical variogram by using variogram() of gstat package. The function requires two arguments:\n\nformula, the dependent variable and the covariates (same as in gstat, see Section 12.2.1)\ndata, a point layer with the dependent variable and covariates as attributes\n\nas shown in the code chunk below.\n\nv &lt;- variogram(MONTHSUM ~ 1, \n               data = rfdata_sf)\nplot(v)\n\n\n\n\nWe can then compare the plot with the theoretical models below.\n\nWith reference to the comparison above, am empirical variogram model will be fitted by using fit.variogram() of gstat package as shown in the code chunk below.\n\nfv &lt;- fit.variogram(object = v,\n                    model = vgm(\n                      psill = 0.5, \n                      model = \"Sph\",\n                      range = 5000, \n                      nugget = 0.1))\nfv\n\n  model     psill    range\n1   Nug 0.1129190    0.000\n2   Sph 0.5292397 5213.396\n\n\nWe can visualise how well the observed data fit the model by plotting fv using the code chunk below.\n\nplot(v, fv)\n\n\n\n\nThe plot above reveals that the empirical model fits rather well. In view of this, we will go ahead to perform spatial interpolation by using the newly derived model as shown in the code chunk below.\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           data = rfdata_sf, \n           model = fv)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model     psill    range\nvar1[1]   Nug 0.1129190    0.000\nvar1[2]   Sph 0.5292397 5213.396\n\n\nOnce we are happy with the results, predict() of gstat package will be used to estimate the unknown grids by using the code chunk below.\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\nresp\n\nSimple feature collection with 314019 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2692.528 ymin: 15773.73 xmax: 56371.45 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   var1.pred  var1.var                  geometry        x        y     pred\n1   131.0667 0.6608399 POINT (25883.42 50231.33) 25883.42 50231.33 131.0667\n2   130.9986 0.6610337  POINT (25933.4 50231.33) 25933.40 50231.33 130.9986\n3   130.9330 0.6612129 POINT (25983.38 50231.33) 25983.38 50231.33 130.9330\n4   130.8698 0.6613782 POINT (26033.36 50231.33) 26033.36 50231.33 130.8698\n5   130.8092 0.6615303 POINT (26083.34 50231.33) 26083.34 50231.33 130.8092\n6   130.7514 0.6616697 POINT (26133.32 50231.33) 26133.32 50231.33 130.7514\n7   130.6965 0.6617971  POINT (26183.3 50231.33) 26183.30 50231.33 130.6965\n8   130.6446 0.6619131 POINT (26233.28 50231.33) 26233.28 50231.33 130.6446\n9   130.5958 0.6620184 POINT (26283.26 50231.33) 26283.26 50231.33 130.5958\n10  132.5484 0.6542154 POINT (25033.76 50181.32) 25033.76 50181.32 132.5484\n\n\n\n\n\n\n\n\nNote\n\n\n\nresp is a sf tibble data.frame with point features.\n\n\nIn order to create a raster surface data object, rasterize() of terra is used as shown in the code chunk below.\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\nkpred\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :      last \nmin value   :  72.77826 \nmax value   : 195.53284 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output object kpred is in SpatRaster object class with a spatial resolution of 50m x 50m. It consists of 1075 columns and 690 rows and in SVY21 projected coordinates system.\n\n\n\n\n\nFinally, tmap functions are used to map the interpolated rainfall raster (i.e. kpred) by using the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\nBeside using gstat to perform variogram modelling manually, autofirVariogram() of automap package can be used to perform varigram modelling as shown in the code chunk below.\n\nv_auto &lt;- autofitVariogram(MONTHSUM ~ 1, \n                           rfdata_sf)\nplot(v_auto)\n\n\n\n\n\nv_auto\n\n$exp_var\n   np      dist     gamma dir.hor dir.ver   id\n1  15  1957.436  311.9613       0       0 var1\n2  33  3307.349  707.7685       0       0 var1\n3  54  4861.368  848.1314       0       0 var1\n4 116  6716.531  730.3969       0       0 var1\n5 111  9235.708 1006.5381       0       0 var1\n6 120 11730.199 1167.5988       0       0 var1\n7 135 14384.636 1533.5903       0       0 var1\n\n$var_model\n  model    psill   range kappa\n1   Nug     0.00       0   0.0\n2   Ste 24100.71 1647955   0.3\n\n$sserr\n[1] 0.2178294\n\nattr(,\"class\")\n[1] \"autofitVariogram\" \"list\"            \n\n\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           model = v_auto$var_model,\n           data = rfdata_sf)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model    psill   range kappa\nvar1[1]   Nug     0.00       0   0.0\nvar1[2]   Ste 24100.71 1647955   0.3\n\n\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class-ex/In-class-Ex4/In-class_Ex4.html#reference",
    "href": "In-class-ex/In-class-Ex4/In-class_Ex4.html#reference",
    "title": "In-Class Exercise 7 - Isohyet Map for Rainfall Data",
    "section": "",
    "text": "Olea, Ricardo A. (2006-07) “A six-step practical approach to semivariogram modeling”, Stochastic Environmental Research and Risk Assessment, 2006-07, Vol.20 (5), p.307-318. SMU e-journal."
  },
  {
    "objectID": "In-class-ex/In-class-Ex3/In-class_Ex3.html",
    "href": "In-class-ex/In-class-Ex3/In-class_Ex3.html",
    "title": "In-Class Exercise 3 - Creating Horizon plots",
    "section": "",
    "text": "pacman::p_load(tidyverse, ggHoriPlot, ggthemes, dplyr)\n\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))\n\n\n\n\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "In-class-ex/In-class-Ex3/In-class_Ex3.html#loading-r-package-and-importing-data",
    "href": "In-class-ex/In-class-Ex3/In-class_Ex3.html#loading-r-package-and-importing-data",
    "title": "In-Class Exercise 3 - Creating Horizon plots",
    "section": "",
    "text": "pacman::p_load(tidyverse, ggHoriPlot, ggthemes, dplyr)\n\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))"
  },
  {
    "objectID": "In-class-ex/In-class-Ex3/In-class_Ex3.html#plotting-the-horizon-graph",
    "href": "In-class-ex/In-class-Ex3/In-class_Ex3.html#plotting-the-horizon-graph",
    "title": "In-Class Exercise 3 - Creating Horizon plots",
    "section": "",
    "text": "averp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "In-class-ex/In-class-Ex1/In-class_Ex1.html",
    "href": "In-class-ex/In-class-Ex1/In-class_Ex1.html",
    "title": "In-Class Exercise 1 - Now you see it!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven, DT)"
  },
  {
    "objectID": "In-class-ex/In-class-Ex1/In-class_Ex1.html#loading-r-packages",
    "href": "In-class-ex/In-class-Ex1/In-class_Ex1.html#loading-r-packages",
    "title": "In-Class Exercise 1 - Now you see it!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven, DT)"
  },
  {
    "objectID": "In-class-ex/In-class-Ex1/In-class_Ex1.html#importing-pisa-data",
    "href": "In-class-ex/In-class-Ex1/In-class_Ex1.html#importing-pisa-data",
    "title": "In-Class Exercise 1 - Now you see it!",
    "section": "Importing PISA data",
    "text": "Importing PISA data\nThe code chunk below uses ‘read_sas()’ of haven to import PISA data into R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nUpon first import, as the student questionaire data file contains data from other countries, we use filter() to filter the data file to only Singapore data.\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\nWe use write_rds() to save the filtered datafile to a seperate file called stu_qqq_SG\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")\n\nFor our analysis we shall read in data from stu_qqq_SG.rds using read_rds()\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\n\nhead(stu_qqq_SG)\n\n# A tibble: 6 × 1,279\n  CNT   CNTRYID CNTSCHID CNTSTUID CYC   NatCen STRATUM SUBNATIO REGION  OECD\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 SGP       702 70200052 70200001 08MS  070200 SGP01   7020000   70200     0\n2 SGP       702 70200134 70200002 08MS  070200 SGP01   7020000   70200     0\n3 SGP       702 70200112 70200003 08MS  070200 SGP01   7020000   70200     0\n4 SGP       702 70200004 70200004 08MS  070200 SGP01   7020000   70200     0\n5 SGP       702 70200152 70200005 08MS  070200 SGP01   7020000   70200     0\n6 SGP       702 70200043 70200006 08MS  070200 SGP01   7020000   70200     0\n# ℹ 1,269 more variables: ADMINMODE &lt;dbl&gt;, LANGTEST_QQQ &lt;dbl&gt;,\n#   LANGTEST_COG &lt;dbl&gt;, LANGTEST_PAQ &lt;dbl&gt;, Option_CT &lt;dbl&gt;, Option_FL &lt;dbl&gt;,\n#   Option_ICTQ &lt;dbl&gt;, Option_WBQ &lt;dbl&gt;, Option_PQ &lt;dbl&gt;, Option_TQ &lt;dbl&gt;,\n#   Option_UH &lt;dbl&gt;, BOOKID &lt;dbl&gt;, ST001D01T &lt;dbl&gt;, ST003D02T &lt;dbl&gt;,\n#   ST003D03T &lt;dbl&gt;, ST004D01T &lt;dbl&gt;, ST250Q01JA &lt;dbl&gt;, ST250Q02JA &lt;dbl&gt;,\n#   ST250Q03JA &lt;dbl&gt;, ST250Q04JA &lt;dbl&gt;, ST250Q05JA &lt;dbl&gt;, ST250D06JA &lt;chr&gt;,\n#   ST250D07JA &lt;chr&gt;, ST251Q01JA &lt;dbl&gt;, ST251Q02JA &lt;dbl&gt;, ST251Q03JA &lt;dbl&gt;, …\n\n\n\nsch_qqq &lt;- read_sas(\"data/cy08msp_sch_qqq.sas7bdat\")\n\n\nsch_qqq_SG &lt;- sch_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\n\nwrite_rds(sch_qqq_SG,\n          \"data/sch_qqq_SG.rds\")\n\n\nsch_qqq_SG &lt;- read_rds(\"data/sch_qqq_SG.rds\")\n\n\nDT::datatable(sch_qqq_SG, class= \"compact\")\n\n\n\n\n\n\n\nwrite.csv(sch_qqq_SG, \"data/sch_qqq_SG.csv\", row.names = FALSE)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package.\n\n\n\n\n\n\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nlist(GAStech_edges)\n\n[[1]]\n# A tibble: 9,063 × 10\n   source target SentDate SentTime Subject   MainSubject sourceLabel targetLabel\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;time&gt;   &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n 1     43     41 6/1/2014 08:39    GT-Seism… Work relat… Sven.Flecha Isak.Baza  \n 2     43     40 6/1/2014 08:39    GT-Seism… Work relat… Sven.Flecha Lucas.Alca…\n 3     44     51 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Felix.Resu…\n 4     44     52 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Hideki.Coc…\n 5     44     53 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Inga.Ferro \n 6     44     45 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Varja.Lagos\n 7     44     44 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Kanon.Herr…\n 8     44     46 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Stenig.Fus…\n 9     44     48 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Hennie.Osv…\n10     44     49 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Isia.Vann  \n# ℹ 9,053 more rows\n# ℹ 2 more variables: SendDate &lt;date&gt;, Weekday &lt;ord&gt;\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nlist(GAStech_edges_aggregated)\n\n[[1]]\n# A tibble: 1,372 × 4\n   source target Weekday   Weight\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;ord&gt;      &lt;int&gt;\n 1      1      2 Sunday         5\n 2      1      2 Monday         2\n 3      1      2 Tuesday        3\n 4      1      2 Wednesday      4\n 5      1      2 Friday         6\n 6      1      3 Sunday         5\n 7      1      3 Monday         2\n 8      1      3 Tuesday        3\n 9      1      3 Wednesday      4\n10      1      3 Friday         6\n# ℹ 1,362 more rows\n\n\n\n\n\n\nIn this section, we will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nMore information can be obtained from these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nFor more information we can refer to the reference guide of tbl_graph()\n\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function.\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, we will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#overview",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#overview",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#getting-started",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#getting-started",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#the-data",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#the-data",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "The data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nlist(GAStech_edges)\n\n[[1]]\n# A tibble: 9,063 × 10\n   source target SentDate SentTime Subject   MainSubject sourceLabel targetLabel\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;time&gt;   &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n 1     43     41 6/1/2014 08:39    GT-Seism… Work relat… Sven.Flecha Isak.Baza  \n 2     43     40 6/1/2014 08:39    GT-Seism… Work relat… Sven.Flecha Lucas.Alca…\n 3     44     51 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Felix.Resu…\n 4     44     52 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Hideki.Coc…\n 5     44     53 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Inga.Ferro \n 6     44     45 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Varja.Lagos\n 7     44     44 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Kanon.Herr…\n 8     44     46 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Stenig.Fus…\n 9     44     48 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Hennie.Osv…\n10     44     49 6/1/2014 08:58    Inspecti… Work relat… Kanon.Herr… Isia.Vann  \n# ℹ 9,053 more rows\n# ℹ 2 more variables: SendDate &lt;date&gt;, Weekday &lt;ord&gt;\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nlist(GAStech_edges_aggregated)\n\n[[1]]\n# A tibble: 1,372 × 4\n   source target Weekday   Weight\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;ord&gt;      &lt;int&gt;\n 1      1      2 Sunday         5\n 2      1      2 Monday         2\n 3      1      2 Tuesday        3\n 4      1      2 Wednesday      4\n 5      1      2 Friday         6\n 6      1      3 Sunday         5\n 7      1      3 Monday         2\n 8      1      3 Tuesday        3\n 9      1      3 Wednesday      4\n10      1      3 Friday         6\n# ℹ 1,362 more rows"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "both dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above-1",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above-1",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "four functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this section, we will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nMore information can be obtained from these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nFor more information we can refer to the reference guide of tbl_graph()\n\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "ggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above-2",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above-2",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "The basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above-3",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#things-to-learn-from-the-code-chunk-above-3",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "ggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#creating-facet-graphs",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#creating-facet-graphs",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "Another very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, we will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#network-metrics-analysis",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#network-metrics-analysis",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "Centrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on-ex/Hands-on-Ex8/Hands-on_Ex8.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 8 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "visNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "",
    "text": "Proportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, we will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap.\n\n\nBy the end of this hands-on exercise, we will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#learning-outcome",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#learning-outcome",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "",
    "text": "By the end of this hands-on exercise, we will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#the-data",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#the-data",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.2.1 The data",
    "text": "7.2.1 The data\nThe data set used is called SGPools_svy21.csv\nIt consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#data-import-and-preparation",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#data-import-and-preparation",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.2.2 Data import and preparation",
    "text": "7.2.2 Data import and preparation\nFirst, we will import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/SGPools_svy21.csv\")\n\nNext, we examine if the data file has been imported correctly.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.2.3 Creating a sf data frame from an aspatial data frame",
    "text": "7.2.3 Creating a sf data frame from an aspatial data frame\nThe code below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires us to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required us to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nWe can display the basic information of the newly created sgpools_sf below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#starting-with-an-interactive-point-symbol-map",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#starting-with-an-interactive-point-symbol-map",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.3.1 Starting with an interactive point symbol map",
    "text": "7.3.1 Starting with an interactive point symbol map\nThe code below is used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#making-it-proportional",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#making-it-proportional",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.3.2 Making it proportional",
    "text": "7.3.2 Making it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#using-a-different-colour",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#using-a-different-colour",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.3.3 Using a different colour",
    "text": "7.3.3 Using a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#faceted-plots-using-tm_facets-method",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#faceted-plots-using-tm_facets-method",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.3.4 Faceted plots using tm_facets() method",
    "text": "7.3.4 Faceted plots using tm_facets() method\nThe argument sync in tm_facets() can be used to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we will switch tmap’s Viewer back to plot mode by using the code below.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#plotting-practise",
    "href": "Hands-on-ex/Hands-on-Ex7b/Hands-on_Ex7b.html#plotting-practise",
    "title": "Hands-on Exercise 7b - Visualising Geospatial Point Data",
    "section": "7.3.5 Plotting Practise",
    "text": "7.3.5 Plotting Practise\nBelow are some additional plots for practise."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "In this hands-on exercise, we will be creating the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\n\n\n\npacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table, \n               tidyverse, CGPfunctions)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#loading-r-packages",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#loading-r-packages",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "pacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table, \n               tidyverse, CGPfunctions)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#examining-the-data-structure",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#examining-the-data-structure",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.2.1 Examining the data structure",
    "text": "6.2.1 Examining the data structure\nWe will use kable() to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#data-preparation",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#data-preparation",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.2.2 Data Preparation",
    "text": "6.2.2 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour will need to be derived.\nWe will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#multiple-calendar-heatmaps",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#multiple-calendar-heatmaps",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.3.1 Multiple Calendar Heatmaps",
    "text": "6.3.1 Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, we will need to do the following:\n\ncount the number of attacks by country,\ncalculate the percent of attacks by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, we will extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#importing-data",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#importing-data",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.4.1 Importing Data",
    "text": "6.4.1 Importing Data\nFor the purpose of this exercise, arrivals_by_air.xlsx will be used.\nThe code below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#deriving-month-and-year-fields",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#deriving-month-and-year-fields",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.4.2 Deriving month and year fields",
    "text": "6.4.2 Deriving month and year fields\nTwo new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#extracting-the-target-country",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#extracting-the-target-country",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.4.3 Extracting the target country",
    "text": "6.4.3 Extracting the target country\nThe code below is used to extract data for the target country (i.e. Vietnam).\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#computing-year-average-arrivals-by-month",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#computing-year-average-arrivals-by-month",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.4.4 Computing year average arrivals by month",
    "text": "6.4.4 Computing year average arrivals by month\nThe code below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#plotting-the-cycle-plot",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#plotting-the-cycle-plot",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.4.5 Plotting the cycle plot",
    "text": "6.4.5 Plotting the cycle plot\nThe code below is used to plot the cycle plot.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#importing-data-1",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#importing-data-1",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.5.1 Importing Data",
    "text": "6.5.1 Importing Data\nWe will use the code below to import the rice data set into R environment.\n\nrice &lt;- read_csv(\"data/rice.csv\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#plotting-the-slopegraph",
    "href": "Hands-on-ex/Hands-on-Ex6/Hands-on_Ex6.html#plotting-the-slopegraph",
    "title": "Hands-on Exercise 6 - Visualising and Analysing Time-oriented Data",
    "section": "6.5.2 Plotting the Slopegraph",
    "text": "6.5.2 Plotting the Slopegraph\nThe code below will be used to plot a basic slopegraph.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = NULL)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, we will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#plotting-a-simple-parallel-coordinates",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#plotting-a-simple-parallel-coordinates",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.4.1 Plotting a simple parallel coordinates",
    "text": "5.4.1 Plotting a simple parallel coordinates\nCode below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#plotting-a-parallel-coordinates-with-boxplot",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#plotting-a-parallel-coordinates-with-boxplot",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.4.2 Plotting a parallel coordinates with boxplot",
    "text": "5.4.2 Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, we will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn =2,\n           scale = \"uniminmax\",\n           alphaLines = 0.3,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\nThings to learn from the code above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#parallel-coordinates-with-facet",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#parallel-coordinates-with-facet",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.4.3 Parallel coordinates with facet",
    "text": "5.4.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combine with some of the ggplot2 functions when plotting a parallel coordinates plot.\nIn the code below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#rotating-x-axis-text-label",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#rotating-x-axis-text-label",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.4.4 Rotating x-axis text label",
    "text": "5.4.4 Rotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#adjusting-the-rotated-x-axis-text-label",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#adjusting-the-rotated-x-axis-text-label",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.4.5 Adjusting the rotated x-axis text label",
    "text": "5.4.5 Adjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#the-basic-plot",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#the-basic-plot",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.5.1 The basic plot",
    "text": "5.5.1 The basic plot\nThe code below plots an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. We will learn how to overcome this problem in the next step."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#rotate-axis-label",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#rotate-axis-label",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.5.2 Rotate axis label",
    "text": "5.5.2 Rotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#changing-the-colour-scheme",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#changing-the-colour-scheme",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.5.3 Changing the colour scheme",
    "text": "5.5.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#parallel-coordinates-plot-with-histogram",
    "href": "Hands-on-ex/Hands-on-Ex5d/Hands-on_Ex5d.html#parallel-coordinates-plot-with-histogram",
    "title": "Hands-on Exercise 5d- Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "5.5.4 Parallel coordinates plot with histogram",
    "text": "5.5.4 Parallel coordinates plot with histogram\nIn the code below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "",
    "text": "There are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, we will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, we create correlation matrix using pairs() of R Graphics. Next, we plot corrgram using corrplot package of R. Lastly, we create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#basic-correlation-matrix",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#basic-correlation-matrix",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.4.1 Basic correlation matrix",
    "text": "5.4.1 Basic correlation matrix\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function.\n\npairs(wine[,2:12])"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#drawing-the-lower-or-upper-corners",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#drawing-the-lower-or-upper-corners",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.4.2 Drawing the lower or upper corners",
    "text": "5.4.2 Drawing the lower or upper corners\npairs function of R Graphics provides many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\nWe can also display the upper half of the correlation matrix by using the code below.\n\npairs(wine[,2:12], lower.panel = NULL)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#including-with-correlation-coefficients",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#including-with-correlation-coefficients",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.4.3 Including with correlation coefficients",
    "text": "5.4.3 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#the-basic-plot",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#the-basic-plot",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.5.1 The Basic Plot",
    "text": "5.5.1 The Basic Plot\nOne of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nlibrary(ggstatsplot)\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\nThings to learn from the code above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\n#ggplot.component = list(\n    #theme(text=element_text(size=5),\n      #axis.text.x = element_text(size = 8),\n      #axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#getting-started-with-corrplot",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#getting-started-with-corrplot",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.7.1 Getting started with corrplot",
    "text": "5.7.1 Getting started with corrplot\nFirst, we need to compute the correlation matrix of wine data frame.\nIn the code below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code below.\n\ncorrplot(wine.cor)\n\n\n\n\nThe default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#working-with-visual-geometrics",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#working-with-visual-geometrics",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.7.2 Working with visual geometrics",
    "text": "5.7.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"square\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"number\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"color\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"pie\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#working-with-layout",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#working-with-layout",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.7.3 Working with layout",
    "text": "5.7.3 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         type=\"upper\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code and figure below.\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\nWe can also experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few.\nParameters group tl.* is for text-legend. The common-using are:\n\ntl.pos is for the position of text labels. It is character or logical. If character, it must be one of 'lt', 'ld', 'td', 'd', 'l' or 'n'. 'lt'(default if type='full') means left and top, 'ld'(default if type='lower') means left and diagonal, 'td'(default if type='upper') means top and diagonal(near), 'd' means diagonal, 'l' means left, 'n' means don’t add text-label.\ntl.cex is for the size of text label (variable names).\ntl.srt is for text label string rotation in degrees.\n\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.pos = 'lt' )\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"color\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.pos = 'td' )\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.pos = 'n' )\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"pie\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.cex = 0.5 )\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"square\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.cex = 1 )\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.srt = 45 )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#working-with-mixed-layout",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#working-with-mixed-layout",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.7.4 Working with mixed layout",
    "text": "5.7.4 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"shade\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"blue\",\n               tl.srt = 45)\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"circle\", \n               upper = \"square\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"red\")\n\n\n\n\nThe argument lower and upper are used to define the visualisation method used. In this case circle is used to map the lower half of the corrgram and square is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#combining-corrgram-with-significant-test",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#combining-corrgram-with-significant-test",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.7.5 Combining corrgram with significant test",
    "text": "5.7.5 Combining corrgram with significant test\nWe are also interested to know which pair of variables has correlation coefficients that are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\n\n\n\n\nWe can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#reorder-a-corrgram",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#reorder-a-corrgram",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.7.6 Reorder a corrgram",
    "text": "5.7.6 Reorder a corrgram\nMatrix reorder is very important for mining the hidden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-written by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"FPC\",\n               tl.col = \"black\")\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"alphabet\",\n               tl.col = \"black\")\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"red\",\n         order=\"hclust\",\n         hclust.method = \"centroid\",\n         addrect = 5)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#reordering-a-correlation-matrix-using-hclust",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#reordering-a-correlation-matrix-using-hclust",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.7.7 Reordering a correlation matrix using hclust",
    "text": "5.7.7 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 6)\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"mcquitty\",\n         addrect = 6)\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"average\",\n         addrect = 8)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#r-packages",
    "href": "Hands-on-ex/Hands-on-Ex5b/Hands-on_Ex5b.html#r-packages",
    "title": "Hands-on Exercise 5b- Visual Correlation Analysis",
    "section": "5.9.1 R packages",
    "text": "5.9.1 R packages\n\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, we will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#funnelplotr-methods-the-basic-plot",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#funnelplotr-methods-the-basic-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.4.1 FunnelPlotR methods: The basic plot",
    "text": "4.4.1 FunnelPlotR methods: The basic plot\nThe code below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#funnelplotr-methods-makeover-1",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#funnelplotr-methods-makeover-1",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.4.2 FunnelPlotR methods: Makeover 1",
    "text": "4.4.2 FunnelPlotR methods: Makeover 1\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions).\nxrange and yrange are used to set the range of x-axis and y-axis."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#funnelplotr-methods-makeover-2",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#funnelplotr-methods-makeover-2",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.4.3 FunnelPlotR methods: Makeover 2",
    "text": "4.4.3 FunnelPlotR methods: Makeover 2\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of \\nCOVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\nlabel = NA argument is removes the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#computing-the-basic-derived-fields",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#computing-the-basic-derived-fields",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.5.1 Computing the basic derived fields",
    "text": "4.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#calculate-lower-and-upper-limits-for-95-and-99.9-ci",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#calculate-lower-and-upper-limits-for-95-and-99.9-ci",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.5.2 Calculate lower and upper limits for 95% and 99.9% CI",
    "text": "4.5.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#plotting-a-static-funnel-plot",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#plotting-a-static-funnel-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.5.3 Plotting a static funnel plot",
    "text": "4.5.3 Plotting a static funnel plot\nIn the code below, ggplot2 functions are used to plot a static funnel plot.\n\n\nShow the code\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#interactive-funnel-plot-plotly-ggplot2",
    "href": "Hands-on-ex/Hands-on-Ex4d/Hands-on_Ex4d.html#interactive-funnel-plot-plotly-ggplot2",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.5.4 Interactive Funnel Plot: plotly + ggplot2",
    "text": "4.5.4 Interactive Funnel Plot: plotly + ggplot2\nWe will make the previous plot interactive, using ggplotly() of plotly package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "",
    "text": "In this hands-on exercise, we will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#installing-and-launching-r-packages",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.1 Installing and launching R packages",
    "text": "4.3.1 Installing and launching R packages\n\npacman::p_load(ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#importing-data",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#importing-data",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.2 Importing Data",
    "text": "4.3.2 Importing Data\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nstat(exam)\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#one-sample-test-gghistostats-method",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#one-sample-test-gghistostats-method",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.3 One-sample test: gghistostats() method",
    "text": "4.3.3 One-sample test: gghistostats() method\nIn the code below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#unpacking-the-bayes-factor",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#unpacking-the-bayes-factor",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.4 Unpacking the Bayes Factor",
    "text": "4.3.4 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10.\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#how-to-interpret-bayes-factor",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#how-to-interpret-bayes-factor",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.5 How to interpret Bayes Factor",
    "text": "4.3.5 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#two-sample-mean-test-ggbetweenstats",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#two-sample-mean-test-ggbetweenstats",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.6 Two-sample mean test: ggbetweenstats()",
    "text": "4.3.6 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#one-way-anova-test-ggbetweenstats-method",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#one-way-anova-test-ggbetweenstats-method",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.7 One way ANOVA Test: ggbetweenstats() method",
    "text": "4.3.7 One way ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#significant-test-of-correlation-ggscatterstats",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#significant-test-of-correlation-ggscatterstats",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.8 Significant test of correlation: ggscatterstats()",
    "text": "4.3.8 Significant test of correlation: ggscatterstats()\nIn the code below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#significant-test-of-association-dependence-ggbarstats-methods",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#significant-test-of-association-dependence-ggbarstats-methods",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.3.9 Significant test of Association (Dependence): ggbarstats() methods",
    "text": "4.3.9 Significant test of Association (Dependence): ggbarstats() methods\nFirst, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nNext, ggbarstats() is used to build a visual for Significant Test of Association.\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#importing-excel-file-readxl-method",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#importing-excel-file-readxl-method",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.1 Importing Excel file: readxl method",
    "text": "4.5.1 Importing Excel file: readxl method\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nThe output object car_resale is a tibble data frame."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#multiple-regression-model-using-lm",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#multiple-regression-model-using-lm",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.2 Multiple Regression Model using lm()",
    "text": "4.5.2 Multiple Regression Model using lm()\nThe code below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-checking-for-multicolinearity",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-checking-for-multicolinearity",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.3 Model Diagnostic: Checking for multicolinearity:",
    "text": "4.5.3 Model Diagnostic: Checking for multicolinearity:\nIn the code below, we use check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-checking-normality-assumption",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-checking-normality-assumption",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.4 Model Diagnostic: checking normality assumption",
    "text": "4.5.4 Model Diagnostic: checking normality assumption\nIn the code below, we use check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-check-model-for-homogeneity-of-variances",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-check-model-for-homogeneity-of-variances",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.5 Model Diagnostic: Check model for homogeneity of variances",
    "text": "4.5.5 Model Diagnostic: Check model for homogeneity of variances\nIn the code below, we use check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-complete-check",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#model-diagnostic-complete-check",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.6 Model Diagnostic: Complete check",
    "text": "4.5.6 Model Diagnostic: Complete check\nWe can also perform a complete check by using check_model().\n\ncheck_model(model1)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#visualising-regression-parameters-see-methods",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#visualising-regression-parameters-see-methods",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.7 Visualising Regression Parameters: see methods",
    "text": "4.5.7 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#visualising-regression-parameters-ggcoefstats-method",
    "href": "Hands-on-ex/Hands-on-Ex4b/Hands-on_Ex4b.html#visualising-regression-parameters-ggcoefstats-method",
    "title": "Hands-on Exercise 4b - Visual Stastistical Analysis",
    "section": "4.5.8 Visualising Regression Parameters: ggcoefstats() method",
    "text": "4.5.8 Visualising Regression Parameters: ggcoefstats() method\nIn the code below, ggcoefstats() of ggstatsplot package is used to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html",
    "href": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html",
    "title": "Hands-on Exercise 3b - Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "In this exercise, we will learn how to create animated data visualization by using gganimate and plotly r packages. We will also learn how to reshape data using tidyr package and process, wrangle and transform data using dplyr package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#loading-the-r-packages",
    "href": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#loading-the-r-packages",
    "title": "Hands-on Exercise 3b - Programming Animated Statistical Graphics with R",
    "section": "3.2.1 Loading the R packages",
    "text": "3.2.1 Loading the R packages\nFirst we install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#importing-data",
    "href": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#importing-data",
    "title": "Hands-on Exercise 3b - Programming Animated Statistical Graphics with R",
    "section": "3.2.2 Importing Data",
    "text": "3.2.2 Importing Data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nImporting Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nNote\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\n\n\nmutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0.\nWe will re-write the code by using mutate_at() as shown in the code below.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nInstead of using mutate_at(), across() can be used to derive the same outputs.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-a-static-population-bubble-plot",
    "href": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-a-static-population-bubble-plot",
    "title": "Hands-on Exercise 3b - Programming Animated Statistical Graphics with R",
    "section": "3.3.1 Building a static population bubble plot",
    "text": "3.3.1 Building a static population bubble plot\nIn the code below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young')"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-the-animated-bubble-plot",
    "href": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-the-animated-bubble-plot",
    "title": "Hands-on Exercise 3b - Programming Animated Statistical Graphics with R",
    "section": "3.3.2 Building the animated bubble plot",
    "text": "3.3.2 Building the animated bubble plot\nIn the code below:-\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-an-animated-bubble-plot-ggplot-method",
    "href": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-an-animated-bubble-plot-ggplot-method",
    "title": "Hands-on Exercise 3b - Programming Animated Statistical Graphics with R",
    "section": "3.4.1 Building an animated bubble plot: ggplot() method",
    "text": "3.4.1 Building an animated bubble plot: ggplot() method\nHere we will learn how to create an animated bubble plot by using ggplotly() method.\n\nThe plotThe code\n\n\n\n\n\n\n\n\nThe animated bubble plot above includes a play/pause button and a slider component to control the animation.\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\nNote\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\n\n\nAlthough show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used as shown in the plot and code chunk below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-an-animated-bubble-plot-plot_ly-method",
    "href": "Hands-on-ex/Hands-on-Ex3b/Hands-on_Ex3b.html#building-an-animated-bubble-plot-plot_ly-method",
    "title": "Hands-on Exercise 3b - Programming Animated Statistical Graphics with R",
    "section": "3.4.2 Building an animated bubble plot: plot_ly() method",
    "text": "3.4.2 Building an animated bubble plot: plot_ly() method\nIn this section, we will learn how to create an animated bubble plot using the plot_ly() method.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "",
    "text": "In this exercise, we learn several ggplot2 extensions to create more elegant and effective statistical graphics.\nOur objectives will be to:-\n\ncontrol the placement of annotations on graphs by using functions in the ggrepel package,\ncreate professional publication quality figures by using functions in the ggthemes and hbrthemes packages,\nplot composite figures by combining ggplot2 graphs by using the patchwork package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.2.1 Installing and loading the required libraries",
    "text": "2.2.1 Installing and loading the required libraries\nBesides tideverse, four R packages will be used.\n\nggrepel: an R package that provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package that provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package that provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\nThe code below will be used to check if the packages have been installed and to load them into our R environment.\n\npacman::p_load(ggrepel, patchwork,\n               ggthemes, hrbrthemes,\n               tidyverse)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#importing-data",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#importing-data",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.2.2 Importing Data",
    "text": "2.2.2 Importing Data\nWe will use a data file called Exam_data.csv. It consists of year end examination grades of a cohort of primary 3 students from a local school.\nThe code below imports exam_data.csv into our R environment by using read_csv() fuction of readr package.\n\nexam_data &lt;- read_csv('data/Exam_data.csv')\n\n\nspec(exam_data)\n\ncols(\n  ID = col_character(),\n  CLASS = col_character(),\n  GENDER = col_character(),\n  RACE = col_character(),\n  ENGLISH = col_double(),\n  MATHS = col_double(),\n  SCIENCE = col_double()\n)\n\n\n\n\n\n\n\n\nNote\n\n\n\nwe can use spec() to quickly inspect the column specifications for this data set.\n\n\nThere are seven attributes in the exam_data tibble data frame. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE (datatype = character).\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE (datatype = double).\n\nWe can also use glimpse() to inspect the data frame.\n\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#working-with-ggrepel",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#working-with-ggrepel",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.3.1 Working with ggrepel",
    "text": "2.3.1 Working with ggrepel\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#working-with-ggtheme-package",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#working-with-ggtheme-package",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.4.1 Working with ggtheme package",
    "text": "2.4.1 Working with ggtheme package\nggthemes provides ggplot2 themes that replicate the look of plots by the likes of Edward Tufte, Stephew few, The economist and The wall street journal among others. It also provides some extra geoms and scales for ‘ggplot2’.\nBelow are some examples of the different themes available.\n\nWall street Journal themefivethirtyeight themeThe economist theme\n\n\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_wsj()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#working-with-hrbthemes-package",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#working-with-hrbthemes-package",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.4.2 Working with hrbthemes package",
    "text": "2.4.2 Working with hrbthemes package\nhrbthemes package provides typography centric themes and theme components for ggplot2. This includes where labels are placed and the fonts used.\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used. Consult this vignette to learn more.\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat we can learn from the code chunk above?\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-composite-graphics-patchwork-method",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-composite-graphics-patchwork-method",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.5.1 Creating Composite Graphics: patchwork method",
    "text": "2.5.1 Creating Composite Graphics: patchwork method\nThere are several ggplot2 extension’s functions that support the preparation of composite figures such as grid.arrange() of gridExtra package and plot_grid() of cowplot package.\nIn this section, we will use a ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a simple syntax where we can create layouts easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#combining-two-ggplot2-graphs",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#combining-two-ggplot2-graphs",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.5.2 Combining two ggplot2 graphs",
    "text": "2.5.2 Combining two ggplot2 graphs\nThe tabset below shows a composite of two histograms created using patchwork along with the corresponding codes.\n\nTwo-column layoutTwo-row layoutChanging the relative size\n\n\n\nplot1+plot2\n\n\n\n\n\n\n\nplot1/plot2\n\n\n\n\n\n\n\nplot1 + plot2 + plot_layout(ncol=2,widths=c(2,1))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#combining-3-ggplot2-graphs",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#combining-3-ggplot2-graphs",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.5.3 Combining 3 ggplot2 graphs",
    "text": "2.5.3 Combining 3 ggplot2 graphs\nWe can plot more complex composite figures by using appropriate operators.\n\nThe composite figure below was plotted using\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\n(plot1 / plot2) | plot3\n\n\n\n\nTo learn more about patchwork, please refer to this link."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-a-composite-figure-with-tags",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-a-composite-figure-with-tags",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.5.4 Creating a composite figure with tags",
    "text": "2.5.4 Creating a composite figure with tags\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\nNumeralsDigitsLetters\n\n\n\n((plot1 / plot2) | plot3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n((plot1 / plot2) | plot3) + \n  plot_annotation(tag_levels = '1')\n\n\n\n\n\n\n\n((plot1 / plot2) | plot3) + \n  plot_annotation(tag_levels = 'A')"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-figures-with-insert",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-figures-with-insert",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.5.5 Creating figures with insert",
    "text": "2.5.5 Creating figures with insert\nBesides providing functions to place plots next to each other, we can also use the function inset_element() to place one or several plots or graphic elements within another plot.\n\nplot3 + inset_element(plot2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-a-composite-figure-by-using-patchwork-and-ggtheme",
    "href": "Hands-on-ex/Hands-on-Ex2/Hands-on_Ex2.html#creating-a-composite-figure-by-using-patchwork-and-ggtheme",
    "title": "Hands-on Exercise 2 - Beyond ggplot2 fundamentals",
    "section": "2.5.6 Creating a composite figure by using patchwork and ggtheme",
    "text": "2.5.6 Creating a composite figure by using patchwork and ggtheme\nThe figure below is created by combining patchwork and theme_economist() of the ggthemes package.\n\n\nShow the code\npatchwork &lt;- (plot1 / plot2) | plot3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "In this exercise, we learn the basic principles and essential components of ggplot2 to plot statistical graphics based on the principle of Layered Grammar of Graphics. The objective is to be able to apply essential graphical elements provided by ggplot2 to create elegant and yet functional statistical graphics."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.2.1 Installing and Loading the required libraries",
    "text": "1.2.1 Installing and Loading the required libraries\nThe code chunk below uses p_load( ) of pacman package to check if the tidyverse packages are installed in the computer. If they are, then they will be launched into the R environment.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#importing-data",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#importing-data",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.2.2 Importing Data",
    "text": "1.2.2 Importing Data\nWe import exam_data.csv into R environment by using the read.csv( ) function and assign it to exam_data\n\nexam_data &lt;- read.csv('data/Exam_data.csv')\n\nIn R, when we create objects, assignment statements etc, we use the form:\nobject_name &lt;- value\nWe can use glimpse( ) and summary( ) to quickly inspect exam_data.\n\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;int&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;int&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;int&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#r-graphics-vs-ggplot",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.3.1 R Graphics VS ggplot",
    "text": "1.3.1 R Graphics VS ggplot\nLets compare how R Graphics, the core graphical functions of Base R and ggplot plot a simple histogram.\n\nBase Rggplot\n\n\n\nhist(exam_data$MATHS, col = '#5e82c9')\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill='#5e82c9') +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\nAlthough the code is simpler when R Graphics is used, according to Hadley Wickham, “The transferrable skills from ggplot2 are not the idiosyncracies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive”."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_bar",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_bar",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.1 Geometric Objects: geom_bar()",
    "text": "1.7.1 Geometric Objects: geom_bar()\nThe code chunk below plots a bar chart by using geom_bar( ).\naes(x=RACE) defines “RACE” as the x-axis.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_dotplot",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_dotplot",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.2 Geometric Objects: geom_dotplot()",
    "text": "1.7.2 Geometric Objects: geom_dotplot()\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nBelow, we use geom_dotplot() to plot a dot plot of math scores.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe y scale may not be useful and could be misleading\n\n\nWe insert 2 additional arguments in geom_dotplot()\n\n‘binwidth’ which refers to group ranges\n‘dotsize’ which scales the size of the dots.\n\nThe function scale_y_continuous() is also added to turn off the y-axis by setting it to NULL.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5,\n               color=\"black\", \n               fill='#5e82c9') +\n  scale_y_continuous(NULL,           \n                     breaks = NULL)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_histogram",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_histogram",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.3 Geometric Objects: geom_histogram()",
    "text": "1.7.3 Geometric Objects: geom_histogram()\ngeom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()       \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default bin is 30."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#modifying-a-geometric-object-by-changing-geom",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#modifying-a-geometric-object-by-changing-geom",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.4 Modifying a geometric object by changing geom()",
    "text": "1.7.4 Modifying a geometric object by changing geom()\nWe modify the previous plot by using 3 arguments:-\n\nbins to change the number of bins to 20,\nfill to shade the histogram with another colour, and\ncolor to change the outline colours of the bars\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill='#5e82c9')"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#modifying-a-geometric-object-by-changing-aes",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#modifying-a-geometric-object-by-changing-aes",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.5 Modifying a geometric object by changing aes()",
    "text": "1.7.5 Modifying a geometric object by changing aes()\nThe code below changes the interior colour of the histogram (i.e. fill) by using a sub-group of aes().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to change colour, fill and alpha of the geometric object."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_density",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_density",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.6 Geometric Objects: geom_density()",
    "text": "1.7.6 Geometric Objects: geom_density()\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\n\n\n\nThe code below plots two kernel density lines by specifically using colour or fill arguments of aes()\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_boxplot",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_boxplot",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.7 Geometric Objects: geom_boxplot",
    "text": "1.7.7 Geometric Objects: geom_boxplot\ngeom_boxplot() displays continuous value list. It enables us to visualize five statistics (the median, two hinges and two whiskers), and all other “outlying” points individually.\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()            \n\n\n\n\nNotches are used to help visually assess whether the medians of distributions differ. If the notches do not overlap, it is more than likely that the medians are different.\nThe code below plots the distribution of Maths scores by gender in notched plots instead of boxplots.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_violin",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_violin",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.8 Geometric Objects: geom_violin()",
    "text": "1.7.8 Geometric Objects: geom_violin()\nViolin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plots the distribution of Maths score by gender using violin plots.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_point",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#geometric-objects-geom_point",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.9 Geometric Objects: geom_point()",
    "text": "1.7.9 Geometric Objects: geom_point()\ngeom_point() is used to create scatter plots. The code below plots a scatter plot of Maths VS English grades of pupils.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#combining-geometric-objects",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#combining-geometric-objects",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7.10 Combining geometric objects",
    "text": "1.7.10 Combining geometric objects\nThe code below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-stat",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-stat",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.8.1 Working with stat()",
    "text": "1.8.1 Working with stat()\nThe boxplots below are incomplete because the means are not shown.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-stat---stat_summary",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-stat---stat_summary",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.8.2 Working with stat - stat_summary()",
    "text": "1.8.2 Working with stat - stat_summary()\nThe code below adds mean values by using stat_summary().\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"blue\",        \n               size=4)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-stat---geom-method",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-stat---geom-method",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.8.3 Working with stat - geom() method",
    "text": "1.8.3 Working with stat - geom() method\nThe code below adds mean values by using geom().\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"blue\",          \n             size=4)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#adding-a-best-fit-curve-on-a-scatterplot",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#adding-a-best-fit-curve-on-a-scatterplot",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.8.4 Adding a best fit curve on a scatterplot",
    "text": "1.8.4 Adding a best fit curve on a scatterplot\nThe interpretability of scatterplots can be improved by adding a best fit curve.\nIn the code below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used is loess\n\n\nThe default smoothing method can be overridden as shown below.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-facet_wrap",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-facet_wrap",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.9.1 Working with facet_wrap()",
    "text": "1.9.1 Working with facet_wrap()\nThe code below produces a 2D matrix of ‘MATHS’ histograms grouped by variable ‘CLASS’ using facet_wrap(~ CLASS) .\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#facet_grid-function",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#facet_grid-function",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.9.2 facet_grid() function",
    "text": "1.9.2 facet_grid() function\nfacet_grid() forms a matrix of panels defined by row and column facetting variables.\nIt is most useful when we have two discrete variables, and all combinations of the variables exist in the data.\nThe code below plots a trellis plot using facet_grid().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-coordinate",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-coordinate",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.10.1 Working with Coordinate",
    "text": "1.10.1 Working with Coordinate\nBy default, bar charts in ggplot2 are vertical.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\nUsing coord_flip() we can flip the vertical bar chart to a horizontal one.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#changing-the-x-and-y-axis-range",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#changing-the-x-and-y-axis-range",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.10.2 Changing the x and y axis range",
    "text": "1.10.2 Changing the x and y axis range\nThe scatterplot below is misleading as the x and y axes are not equal.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n\n\nInstead, we can set both the x and y axis ranges to 0-100.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-themes",
    "href": "Hands-on-ex/Hands-on-Ex1/Hands-on_Ex1.html#working-with-themes",
    "title": "Hands-on Exercise 1 - A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.11.1 Working with Themes",
    "text": "1.11.1 Working with Themes\nThe code below plots a horizontal bar chart using theme_gray().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\nWe can see the difference when we change the theme to theme_classic().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\nWe can further customize the chart by adding additional arguments using theme().\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal() +\n   theme(panel.background = element_rect(fill = \"lightblue\",\n                                         color = \"white\"))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nWelcome to Imran’s home page on Visual Analytics.\nIn this website, you will find my coursework for this intense yet insightful Analytics module from Singapore Management University for Academic year 23-24, Jan.\n\nStudents are assessed based on pre & in-class exercises, take home assignments and a final group project."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "In this exercise, we will learn how to create interactive data visualization using functions provided by ggiraph and plotly packages."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#tooltip-effect-with-tooltip-aesthetic",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#tooltip-effect-with-tooltip-aesthetic",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.4.1 Tooltip effect with tooltip aesthetic",
    "text": "3.4.1 Tooltip effect with tooltip aesthetic\nWe can use the below code to plot an interactive statistical graph by using ggiraph package.\nThe code chunk consists of two parts:-\n\nFirst, a ggplot object will be created.\nNext, girafe() of ggiraph will be used to create an interactive svg object.\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\n\nNote\n\n\n\nFirst, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#displaying-multiple-information-on-tooltip",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#displaying-multiple-information-on-tooltip",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.5.1 Displaying multiple information on tooltip",
    "text": "3.5.1 Displaying multiple information on tooltip\nThe specific content or box for the tooltip can be customized using a list object as shown in the code below\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Race = \", exam_data$RACE)) \n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p1,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\nThe first three lines of codes in the code creates a new field called tooltip. At the same time, it populates text in ID, CLASS and RACE fields into the newly created field. Then, this newly created field is used as tooltip field."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#customizing-tooltip-style",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#customizing-tooltip-style",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.6.1 Customizing Tooltip style",
    "text": "3.6.1 Customizing Tooltip style\nThe code below uses opts_tooltip() of ggiraph to customize tooltip rendering by adding css declarations.\n\ntooltip_css &lt;- \"background-color:blue; #&lt;&lt;\nfont-style:bold; color:white;\" #&lt;&lt;\n\np3 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = exam_data$tooltip),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p3,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\nThe background colour of the tooltip is blue and the font is white and bold."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#displaying-statistics-on-tooltip",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#displaying-statistics-on-tooltip",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.6.2 Displaying statistics on tooltip",
    "text": "3.6.2 Displaying statistics on tooltip\nThe code below shows an advanced way to customize the tooltip. A function is used to compute 90% confidence interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean English scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = ENGLISH, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light green\"\n  ) +\n  stat_summary(aes(y = ENGLISH),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#hover-effect-with-data_id-aesthetic",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#hover-effect-with-data_id-aesthetic",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.6.3 Hover effect with data_id aesthetic",
    "text": "3.6.3 Hover effect with data_id aesthetic\nThe code below shows another interactive feature of ggiraph, data_id.\n\np4 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p4,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\nInteractivity: Elements associated with a data_id (CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\nThe default value of the hover css is hover_css = “fill:orange;”."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#styling-hover-effect",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#styling-hover-effect",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.6.4 Styling hover effect",
    "text": "3.6.4 Styling hover effect\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #800080;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent from previous example, in this example the ccs customization is encoded directly."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#combining-tooltip-and-hover-effect",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#combining-tooltip-and-hover-effect",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.6.5 Combining tooltip and hover effect",
    "text": "3.6.5 Combining tooltip and hover effect\nWe can use the code below to combine both tooltip and hover effects on the interactive stastistical graph below.\n\np5 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p5,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #800080;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#click-effect-with-onclick",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#click-effect-with-onclick",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.6.6 Click effect with onclick",
    "text": "3.6.6 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web\nThe code below shows an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that click actions must be a string column in the data set containing valid javascript instructions"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#coordinated-multiple-views-with-ggiraph",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#coordinated-multiple-views-with-ggiraph",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.6.7 Coordinated Multiple Views with ggiraph",
    "text": "3.6.7 Coordinated Multiple Views with ggiraph\nWe can also implement multiple views for data visualization, like below.\n\n\n\n\n\n\nWhen a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualization will also be highlighted as well.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\np_1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np_2 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p_1 + p_2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #800080;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when we mouse over a point."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#creating-an-interactive-scatter-plot-plot_ly-method",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#creating-an-interactive-scatter-plot-plot_ly-method",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.7.1 Creating an interactive scatter plot: plot_ly() method",
    "text": "3.7.1 Creating an interactive scatter plot: plot_ly() method\nThe tabset below shows a basic interactive plot.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#working-with-visual-variable-plot_ly-method",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#working-with-visual-variable-plot_ly-method",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.7.2 Working with visual variable: plot_ly() method",
    "text": "3.7.2 Working with visual variable: plot_ly() method\nIn the code below, colour argument is mapped to a qualitative visual variable (eg RACE).\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#creating-an-interactive-scatter-plot-ggplotly-method",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#creating-an-interactive-scatter-plot-ggplotly-method",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.7.3 Creating an interactive scatter plot: ggplotly() method",
    "text": "3.7.3 Creating an interactive scatter plot: ggplotly() method\nThe code below plots an interactive scatter plot using ggplotly().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#coordinated-multiple-views-with-plotly",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#coordinated-multiple-views-with-plotly",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.7.4 Coordinated multiple views with plotly",
    "text": "3.7.4 Coordinated multiple views with plotly\nCreation of a coordinated linked plot using plotly has three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\nThing to learn from the code:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk,"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#interactive-data-table-dt-package",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#interactive-data-table-dt-package",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.8.1 Interactive Data Table: DT package",
    "text": "3.8.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#linked-brushing-crosstalk-method",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#linked-brushing-crosstalk-method",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.8.2 Linked brushing: crosstalk method",
    "text": "3.8.2 Linked brushing: crosstalk method\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code below is used to implement the coordinated brushing shown above\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#ggiraph",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#ggiraph",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.10.1 ggiraph",
    "text": "3.10.1 ggiraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#plotly-for-r",
    "href": "Hands-on-ex/Hands-on-Ex3a/Hands-on_Ex3a.html#plotly-for-r",
    "title": "Hands-on Exercise 3a - Programming Interactive Data Visualisation with R",
    "section": "3.10.2 plotly for R",
    "text": "3.10.2 plotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels\n\nMain reference: Kam, T.S. (2023). Programming Interactive Data Visualisation with R."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "",
    "text": "In this chapter, we will learn two new statistical graphic methods for visualising distribution, namely ridgeline plot and raincloud plot"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#loading-r-package",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#loading-r-package",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.2.1 Loading R package",
    "text": "4.2.1 Loading R package\nThe following R packages will be used:\n\ntidyverse, a family of R packages for data science processes,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#importing-data",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#importing-data",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.2.2 Importing data",
    "text": "4.2.2 Importing data\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#plotting-ridgeline-graph-ggridges-method",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#plotting-ridgeline-graph-ggridges-method",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.3.1 Plotting ridgeline graph: ggridges method",
    "text": "4.3.1 Plotting ridgeline graph: ggridges method\nHere, we will plot ridgeline plots using ggridges package.\nThere are 2 main geoms to plot ridgeline plots: geom_ridgeline() and geom_density_ridges().\nThe former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#varying-fill-colours-along-the-x-axis",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#varying-fill-colours-along-the-x-axis",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.3.2 Varying fill colours along the x axis",
    "text": "4.3.2 Varying fill colours along the x axis\nWe can have the area under a ridgeline filled with colors that vary along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill.\nWe can have changing fill colors OR transparency but not both.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Grades\",\n                       option = \"D\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#mapping-the-probabilities-directly-onto-colour",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#mapping-the-probabilities-directly-onto-colour",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.3.2 Mapping the probabilities directly onto colour",
    "text": "4.3.2 Mapping the probabilities directly onto colour\nggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges()."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#ridgeline-plots-with-quantile-lines",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#ridgeline-plots-with-quantile-lines",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.3.4 Ridgeline plots with quantile lines",
    "text": "4.3.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nWe can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#plotting-a-half-eye-graph",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#plotting-a-half-eye-graph",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.4.1 Plotting a Half Eye graph",
    "text": "4.4.1 Plotting a Half Eye graph\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nThings to learn from the code above\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#adding-the-boxplot-with-geom_boxplot",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#adding-the-boxplot-with-geom_boxplot",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "9.4.2 Adding the boxplot with geom_boxplot()",
    "text": "9.4.2 Adding the boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#adding-the-dot-plots-with-stat_dots",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#adding-the-dot-plots-with-stat_dots",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.4.3 Adding the Dot Plots with stat_dots()",
    "text": "4.4.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#finishing-touch",
    "href": "Hands-on-ex/Hands-on-Ex4a/Hands-on_Ex4a.html#finishing-touch",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.4.2 Finishing Touch",
    "text": "4.4.2 Finishing Touch\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "",
    "text": "In this chapter, we will gain practise creating statistical graphics for visualising uncertainty. By the end of this chapter we will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#installing-and-loading-the-packages",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.2.1 Installing and loading the packages",
    "text": "4.2.1 Installing and loading the packages\nThe following R packages will be used:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#importing-data",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#importing-data",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.2.2 Importing Data",
    "text": "4.2.2 Importing Data\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#plotting-standard-error-bars-of-point-estimates",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#plotting-standard-error-bars-of-point-estimates",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.3.1 Plotting standard error bars of point estimates",
    "text": "4.3.1 Plotting standard error bars of point estimates\nNext, we plot the standard error bars of mean maths score by race as shown below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\nThings to learn from the code above\n\n\n\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#plotting-confidence-interval-of-point-estimates",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#plotting-confidence-interval-of-point-estimates",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.3.2 Plotting confidence interval of point estimates",
    "text": "4.3.2 Plotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\nThings to learn from the code above\n\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-with-interactive-error-bars",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-with-interactive-error-bars",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.3.3 Visualizing the uncertainty of point estimates with interactive error bars",
    "text": "4.3.3 Visualizing the uncertainty of point estimates with interactive error bars\nIn this section, we will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.4.1 Visualizing the uncertainty of point estimates: ggdist methods",
    "text": "4.4.1 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nThis function comes with many arguments. In the code below, the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\nFor more information on the arguments available, please refer to this link.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods-1",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods-1",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.4.2 Visualizing the uncertainty of point estimates: ggdist methods",
    "text": "4.4.2 Visualizing the uncertainty of point estimates: ggdist methods\nWe will makeover the previous plot by showing 99% confidence intervals.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.99,  # Change .width to 0.99 for 99% confidence interval\n                    .point = median,\n                    .interval = qi) +\n  labs(\n    title = \"Visualising 99%  confidence interval of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe .width argument in the stat_pointinterval function controls the width of the confidence interval to be displayed around the median point (or any other summary statistic) in your plot. It determines the coverage probability of the confidence interval."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods-2",
    "href": "Hands-on-ex/Hands-on-Ex4c/Hands-on_Ex4c.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods-2",
    "title": "Hands-on Exercise 4c - Visualising Uncertainty",
    "section": "4.4.3 Visualizing the uncertainty of point estimates: ggdist methods",
    "text": "4.4.3 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html",
    "href": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html",
    "title": "Hands-on Exercise 5a- Creating Ternary Plots with R",
    "section": "",
    "text": "Ternary plots are a way to display the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.)\nIt’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThis hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html#plotting-a-static-ternary-diagram",
    "href": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html#plotting-a-static-ternary-diagram",
    "title": "Hands-on Exercise 5a- Creating Ternary Plots with R",
    "section": "5.4.1 Plotting a static ternary diagram",
    "text": "5.4.1 Plotting a static ternary diagram\nUsing ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=ACTIVE,y=OLD, z=YOUNG)) +\n  geom_point()\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=ACTIVE,y=OLD, z=YOUNG)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html#plotting-an-interactive-ternary-diagram",
    "href": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html#plotting-an-interactive-ternary-diagram",
    "title": "Hands-on Exercise 5a- Creating Ternary Plots with R",
    "section": "5.4.2 Plotting an interactive ternary diagram",
    "text": "5.4.2 Plotting an interactive ternary diagram\nUsing plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Active\"), \n  baxis = axis(\"Old\"), \n  caxis = axis(\"Young\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~ACTIVE, \n  b = ~OLD, \n  c = ~YOUNG, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html#plotting-practise",
    "href": "Hands-on-ex/Hands-on-Ex5a/Hands-on_Ex5a.html#plotting-practise",
    "title": "Hands-on Exercise 5a- Creating Ternary Plots with R",
    "section": "5.4.3 Plotting Practise",
    "text": "5.4.3 Plotting Practise\nBelow are some additional plots created for practise."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rows and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, we will use R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#heatmap-of-r-stats",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#heatmap-of-r-stats",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.4.1 heatmap() of R stats",
    "text": "5.4.1 heatmap() of R stats\nWe will plot a heatmap by using heatmap() of Base Stats, using the code below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 5))\n\n\n\n\nThe values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#default",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#default",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.1 Default",
    "text": "5.5.1 Default\n\nheatmaply(mtcars)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#correlation-heatmaps",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#correlation-heatmaps",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.2 Correlation heatmaps",
    "text": "5.5.2 Correlation heatmaps\nWith reference from Introduction to Heatmaply, we can also use the margins parameter with correlation heatmaps. heatmaply includes the heatmaply_cor function, which is a wrapper around heatmaply with arguments optimised for use with correlation matrices. Notice how we color the branches.\n\nheatmaply_cor(\n  cor(mtcars),\n  xlab = \"Features\",\n  ylab = \"Features\",\n  k_col = 2,\n  k_row = 2\n)\n\n\n\n\n\nWe can also do a more advanced correlation heatmap where the p-value from the correlation test is mapped to point size.\n\nr &lt;- cor(mtcars)\n## We use this function to calculate a matrix of p-values from correlation tests\n## https://stackoverflow.com/a/13112337/4747043\ncor.test.p &lt;- function(x){\n    FUN &lt;- function(x, y) cor.test(x, y)[[\"p.value\"]]\n    z &lt;- outer(\n      colnames(x), \n      colnames(x), \n      Vectorize(function(i,j) FUN(x[,i], x[,j]))\n    )\n    dimnames(z) &lt;- list(colnames(x), colnames(x))\n    z\n}\np &lt;- cor.test.p(mtcars)\n\nheatmaply_cor(\n  r,\n  node_type = \"scatter\",\n  point_size_mat = -log10(p), \n  point_size_name = \"-log10(p-value)\",\n  label_names = c(\"x\", \"y\", \"Correlation\")\n)\n\n\n\n\n\nThe code below shows the basic syntax needed to create an interactive heatmap using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the right hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the left hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#data-transformation",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#data-transformation",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.3 Data Transformation",
    "text": "5.5.3 Data Transformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n5.5.3.1 Scaling method\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n5.5.3.2 Normalising method\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n5.5.3.3 Percentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#clustering-algorithm",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#clustering-algorithm",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.4 Clustering algorithm",
    "text": "5.5.4 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#manual-approach",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#manual-approach",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.5 Manual approach",
    "text": "5.5.5 Manual approach\nIn the code below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#statistical-approach",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#statistical-approach",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.6 Statistical approach",
    "text": "5.5.6 Statistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#seriation",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#seriation",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.7 Seriation",
    "text": "5.5.7 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#working-with-colour-palettes",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#working-with-colour-palettes",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.8 Working with colour palettes",
    "text": "5.5.8 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code below, the Blues colour palette of rColorBrewer is used.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\nheatmaply(\n  normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n  colors = heat.colors(100)\n)\n\n\n\n\n\n\nheatmaply(\n  mtcars,\n  scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(\n    low = \"yellow\", \n    high = \"red\", \n    midpoint = 200, \n    limits = c(0, 500)\n  )\n)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#the-finish-touch",
    "href": "Hands-on-ex/Hands-on-Ex5c/Hands-on_Ex5c.html#the-finish-touch",
    "title": "Hands-on Exercise 5c- Visual Multivariate Analysis with Heatmap",
    "section": "5.5.9 The finish touch",
    "text": "5.5.9 The finish touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "",
    "text": "In this hands-on exercise, we will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, we will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, we will learn how to plot static treemap by using treemap package. In the third section, we will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#importing-the-data-set",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#importing-the-data-set",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.3.1 Importing the data set",
    "text": "5.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#data-wrangling-and-manipulation",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#data-wrangling-and-manipulation",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.3.2 Data Wrangling and Manipulation",
    "text": "5.3.2 Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#grouped-summaries-without-the-pipe",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#grouped-summaries-without-the-pipe",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.3.3 Grouped summaries without the Pipe",
    "text": "5.3.3 Grouped summaries without the Pipe\nThe code below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#grouped-summaries-with-the-pipe",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#grouped-summaries-with-the-pipe",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.3.4 Grouped summaries with the pipe",
    "text": "5.3.4 Grouped summaries with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#designing-a-static-treemap",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#designing-a-static-treemap",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.1 Designing a static treemap",
    "text": "5.4.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#using-the-basic-arguments",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#using-the-basic-arguments",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.2 Using the basic arguments",
    "text": "5.4.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#working-with-vcolor-and-type-arguments",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#working-with-vcolor-and-type-arguments",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.3 Working with vColor and type arguments",
    "text": "5.4.3 Working with vColor and type arguments\nIn the code below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#colours-in-treemap-package",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#colours-in-treemap-package",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.4 Colours in treemap package",
    "text": "5.4.4 Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#type-value-treemap",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#type-value-treemap",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.5 type = “value” treemap",
    "text": "5.4.5 type = “value” treemap\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#type-manual-treemap",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#type-manual-treemap",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.6 type = “manual” treemap",
    "text": "5.4.6 type = “manual” treemap\nWith “manual” type, the value range is mapped linearly to the colour palette.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#treemap-layout",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#treemap-layout",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.7 Treemap layout",
    "text": "5.4.7 Treemap layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#working-with-algorithm-argument",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#working-with-algorithm-argument",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.8 Working with algorithm argument",
    "text": "5.4.8 Working with algorithm argument\nThe code below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#using-sortid",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#using-sortid",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.4.9 Using sortID",
    "text": "5.4.9 Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#designing-a-basic-treemap",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#designing-a-basic-treemap",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.5.1 Designing a basic treemap",
    "text": "5.5.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#defining-heirarchy",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#defining-heirarchy",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.5.2 Defining heirarchy",
    "text": "5.5.2 Defining heirarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#installing-d3treer-package",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#installing-d3treer-package",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.6.1 Installing d3treeR package",
    "text": "5.6.1 Installing d3treeR package\n\nlibrary(devtools)\n#install_github(\"timelyportfolio/d3treeR\")1\n\n\nlibrary(d3treeR)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#designing-an-interactive-treemap",
    "href": "Hands-on-ex/Hands-on-Ex5e/Hands-on_Ex5e.html#designing-an-interactive-treemap",
    "title": "Hands-on Exercise 5e- Treemap Visualisation with R",
    "section": "5.6.2 Designing an interactive Treemap",
    "text": "5.6.2 Designing an interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is saved as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, we will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#importing-geospatial-data-into-r",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#importing-geospatial-data-into-r",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.3.1 Importing Geospatial Data into R",
    "text": "7.3.1 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\imranmi\\ISSS608-VAA\\Hands-on-ex\\Hands-on-Ex7a\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWe can examine the content of mpsz by using the code below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29..."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#importing-attribute-data-into-r",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#importing-attribute-data-into-r",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.3.2 Importing Attribute Data into R",
    "text": "7.3.2 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"data/respopagesextod2011to2020.csv\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#data-preparation",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#data-preparation",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.3.3 Data Preparation",
    "text": "7.3.3 Data Preparation\nBefore a thematic map can be prepared, we will need to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n7.3.3.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n7.3.3.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\nwrite_rds(mpsz_pop2020, \"data/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.1 Plotting a choropleth map quickly by using qtm()",
    "text": "7.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nThings to learn from the code above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.2 Creating a choropleth map by using tmap’s elements",
    "text": "7.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Reds\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will go through the tmap functions that used to plot these elements.\n\n7.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n7.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. We will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n7.4.2.3 Drawing choropleth map using tm_fill() and tm_border()\ntm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#data-classification-methods-of-tmap",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#data-classification-methods-of-tmap",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.3 Data classification methods of tmap",
    "text": "7.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n7.4.3.1 Plotting chropleth maps with built-in classification methods\nThe code below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\nQuantile, Equal, Jenks, kmeans, sd and hclust data classification with 2 classes:-\n\nQuantileEqualJenksKmeansSdHclust\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nQuantile, Equal, Jenks, kmeans, sd and hclust data classification with 10 classes:-\n\nQuantileEqualJenksKmeansSdHclust\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n7.4.3.2 Plotting choropleth map with custom breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. The Code below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#colour-scheme",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#colour-scheme",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.4 Colour scheme",
    "text": "7.4.4 Colour scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n7.4.4.1 Using ColourBrewer palette\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Reds\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#map-layouts",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#map-layouts",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.5 Map layouts",
    "text": "7.4.5 Map layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n7.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n7.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n7.4.5.3 Cartographic Furniture\nBeside map style, tmap also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code below.\n\ntmap_style(\"white\")"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.6 Drawing Small Multiple Choropleth Maps",
    "text": "7.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arranged side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n7.4.6.1 By assigning multiple values to at least one of the aesthetics arguments\nIn the example below, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn the example below, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n7.4.6.2 By defining a group-by variable in tm_facets()\nIn the example below, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n7.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn the example below, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#mapping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#mapping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.7 Mapping Spatial Object Meeting a Selection Criterion",
    "text": "7.4.7 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection function to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#plotting-practise",
    "href": "Hands-on-ex/Hands-on-Ex7a/Hands-on_Ex7a.html#plotting-practise",
    "title": "Hands-on Exercise 7a - Chropleth Mapping in R",
    "section": "7.4.8 Plotting Practise",
    "text": "7.4.8 Plotting Practise\nBelow are a few additional plots for practise"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "",
    "text": "In this in-class exercise, we will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\n\nBy the end of this in-class exercise, we will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#objectives",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#objectives",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "",
    "text": "In this in-class exercise, we will gain hands-on experience on using appropriate R methods to plot analytical maps."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#learning-outcome",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#learning-outcome",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "",
    "text": "By the end of this in-class exercise, we will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "7.4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points",
    "text": "7.4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA.\nIn the following code, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#plotting-map-using-rates-instead-of-counts",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#plotting-map-using-rates-instead-of-counts",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "7.4.2 Plotting map using rates, instead of counts",
    "text": "7.4.2 Plotting map using rates, instead of counts\nWe will use the code below to plot a choropleth map showing the distribution of percentage functional water point by LGA.\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#percentile-map",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#percentile-map",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "7.5.1 Percentile Map",
    "text": "7.5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n7.5.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values.\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n7.5.1.2 Writing functions\nWriting a function has three big advantages over using copy-and-paste:\n\nWe can give a function an evocative name that makes our code easier to understand.\nAs requirements change, we only need to update code in one place, instead of many.\nWe can eliminate the chance of making incidental mistakes when using copy and paste (i.e. updating a variable name in one place, but not in another).\n\n\n\n7.5.1.3 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n7.5.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n7.5.1.5 Trying the percentile mapping function\nTo run the function, we type the code below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nAdditional arguments such as the title, legend positioning just to name a few, could also be passed to customise various features of the map."
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#box-map",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#box-map",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "7.5.2 Box Map",
    "text": "7.5.2 Box Map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n7.5.2.1 Creating the boxbreaks function\nThe code below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n7.5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n7.5.2.3 Trying the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n7.5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#plotting-practise",
    "href": "Hands-on-ex/Hands-on-Ex7c/Hands-on_Ex7c.html#plotting-practise",
    "title": "Hands-on Exercise 7c - Analytical Mapping",
    "section": "7.5.3 Plotting Practise",
    "text": "7.5.3 Plotting Practise\nBelow are some additional plots for practise"
  },
  {
    "objectID": "In-class-ex/In-class-Ex2/In-class_Ex2.html",
    "href": "In-class-ex/In-class-Ex2/In-class_Ex2.html",
    "title": "In-Class Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggiraph, plotly, listviewer,\n               patchwork, DT, tidyverse,ggdist, ggridges, ggthemes,\n               colorspace) \n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nAdding a Tool tip\n\np4 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS, tooltip=ID), \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p4,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\nAdding tool tip to patchwork\n\np_1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID, tooltip=ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np_2 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID, tooltip=ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p_1 + p_2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #800080;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "In-class-ex/In-class-Ex2/In-class_Ex2.html#loading-r-package",
    "href": "In-class-ex/In-class-Ex2/In-class_Ex2.html#loading-r-package",
    "title": "In-Class Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggiraph, plotly, listviewer,\n               patchwork, DT, tidyverse,ggdist, ggridges, ggthemes,\n               colorspace)"
  },
  {
    "objectID": "In-class-ex/In-class-Ex2/In-class_Ex2.html#importing-data",
    "href": "In-class-ex/In-class-Ex2/In-class_Ex2.html#importing-data",
    "title": "In-Class Exercise 2",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nAdding a Tool tip\n\np4 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS, tooltip=ID), \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p4,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\nAdding tool tip to patchwork\n\np_1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID, tooltip=ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np_2 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID, tooltip=ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p_1 + p_2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #800080;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "In-class-ex/In-class-Ex4/data/geospatial/MPSZ-2019.html",
    "href": "In-class-ex/In-class-Ex4/data/geospatial/MPSZ-2019.html",
    "title": "Imran's Visual Analytics page",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-Visual Analytics & Applications",
    "section": "",
    "text": "Welcome to Imran’s Visual Analytics home page.\n\nIn this website, you will find my coursework for this module.\n\nExpectationsRealityBut eventually….\n\n\n\n\n\nIt can’t be that hard…\n\n\n\n\n\n\n\nWhat did I sign up for???!!!\n\n\n\n\n\n\n\nHopefully……"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex1a/Take-home_Ex1a.html",
    "href": "Take-home-ex/Take-home-Ex1a/Take-home_Ex1a.html",
    "title": "Take-home Exercise 1a- 5 EDAs",
    "section": "",
    "text": "Setting the Scene\nOECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several of Singapore’s ministers for Education also started an “every school a good school” slogan. The general public, however, believes that there are still disparities that exist, especially between “elite” and neighborhood schools, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families.\n\n\nThe Task\nThe 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA’s global education survey runs every three years to assess education systems worldwide through the testing 15 year old students in the subjects of mathematics, reading, and science.\nIn this take-home exercise, we will use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students.\n\n\n\nGetting Started\nLoading R packages\n\npacman::p_load(tidyverse, haven, ggdist, ggridges, ggthemes,\n               colorspace)\n\nImporting the Data\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\nLoading the instvy package.\n\n# install.packages(\"intsvy\",repos = \"http://cran.us.r-project.org\")\n\nlibrary(\"intsvy\")\n\nExtracting the overall student mean score values for each subject\n\nMath_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", data=stu_qqq_SG)\n\nRead_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", data=stu_qqq_SG)\n\nSCIE_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", data=stu_qqq_SG)\n\n\n\n1) Distribution of scores in the student cohort\nWe will use the code below to plot histograms to show the distribution of scores across the 3 subjects.\n\n\nShow the code\n# Create the histogram plot with an annotated mean line using Math_mean_SG\nplt1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightblue') +\n  labs(x = \"Math Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = Math_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Math_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create the histogram plot with an annotated mean line using Read_mean_SG\nplt2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightgreen') +\n  labs(x = \"Reading Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = Read_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Read_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n\n# Create the histogram plot with an annotated mean line using Science_mean_SG\nplt3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightpink') +\n  labs(x = \"Science Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = SCIE_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = SCIE_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(SCIE_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create a single plot with density plots for Math, Reading, and Science scores\nplt4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +\n  geom_density(alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +\n  labs(x = \"Subject Scores\",\n       y = \"Density\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  guides(fill = FALSE) +  # Remove the legend\n  theme_minimal()\n\n\nWe use the code below to create a composite plot.\n\n\nShow the code\nlibrary(patchwork)\n\npatch1 &lt;- (plt1+plt2) / (plt3+plt4)  + \n              plot_annotation(\n                title = \"Distribution of student performance in Math, Reading and Science\")\n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 1\n\n\n\nThe distribution of scores seem to resemble a normal distribution across all 3 subjects. Singaporean students seem to have a higher mean score In Mathematics relative to Reading and Science.\nFurther statistical tests like the Anderson-Darling or Shapiro-Wilk tests will need to be conducted to confirm the normality in distribution.\n\n\n\n\n2) Relationship between Scores and School ID\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the School ID (CNTSCHID).\nIn the code below, we will create separate tables for the mean scores for each subject by different School Ids.\n\nSchoolid_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nNext, we use the below code to plot bubble plots to examine the number of students and their mean scores for each school. We will also use the plotly package for added interactivity.\n\nMean Math scores across SchoolsMean Reading scores across SchoolsMean Science scores across Schools\n\n\n\n\nShow the code\nlibrary(plotly)\n\nbest_sch_math &lt;- Schoolid_math %&gt;% filter(Mean == max(Mean))\nworst_sch_math &lt;- Schoolid_math %&gt;% filter(Mean == min(Mean))\n\n\np_1 &lt;- ggplot(Schoolid_math, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"skyblue\", high = \"darkblue\") +\n  labs(title = \"Mean Math Scores per School\",\n    y = \"Mean Math Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_1 &lt;- p_1 + \n  geom_text(data = best_sch_math, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_math, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n\n# Convert to an interactive plot\nggplotly(p_1, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_sch_read &lt;- Schoolid_read %&gt;% filter(Mean == max(Mean))\nworst_sch_read &lt;- Schoolid_read %&gt;% filter(Mean == min(Mean))\n\np_2 &lt;- ggplot(Schoolid_read, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"yellow\", high = \"darkorchid\") +\n  labs(title = \"Mean Reading Scores per School\",\n    y = \"Mean Reading Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_2 &lt;- p_2 + \n  geom_text(data = best_sch_read, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_read, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n# Convert to an interactive plot\nggplotly(p_2, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_sch_scie &lt;- Schoolid_scie %&gt;% filter(Mean == max(Mean))\nworst_sch_scie &lt;- Schoolid_scie %&gt;% filter(Mean == min(Mean))\n\n\np_3 &lt;- ggplot(Schoolid_scie, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"lightpink\", high = \"darkred\") +\n  labs(title = \"Mean Science Scores per School\",\n    y = \"Mean Science Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_3 &lt;- p_3 + \n  geom_text(data = best_sch_scie, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_scie, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n# Convert to an interactive plot\nggplotly(p_3, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 2\n\n\n\nThe ability to extract and assign Mean scores to individual schools enables us to further explore and examine the disparity in performance between schools. For example, looking at the two extremes of score results, we note that Schools (70200001 & 70200003) out perform other schools in Math and Science. On the other hand, Schools (7020115 & 70200149) under perform other schools in Math and Science.\nThis seems to indicate that there are still marked differences between the ‘’best” schools and the’‘worst’’ schools. Additional analysis could be done to identify the differences between these two sets of schools in terms of resources, teaching quality, and students attitudes or motivation etc, in order to fully understand the reason behind the difference in the scores.\n\n\n\n\n3) Relationship Between Gender and Scores\nFirst we create a subset of Gender and PV1 scores using the below code. We also convert the levels from 1 and 2, to Female and Male respectively.\n\n# Create a subset of the data with gender and PV1 score columns\nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)\n\n# Convert the \"ST004D01T\" column to a factor \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nNext we plot the ridgeline plots with quantile lines.\n\n\nShow the code\nrp1 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1MATH, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Math Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\nrp2 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1READ, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Reading Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\nrp3 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1SCIE, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Science Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\n\nWe use the code below to create a composite plot via patchwork\n\n\nShow the code\nlibrary(patchwork)\n\npatch6 &lt;- rp1 / rp2 / rp3 + \n              plot_annotation(\n                title = \"Male students seem to perform better in Math and Science\")\n\npatch6 & theme(panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 3\n\n\n\nMales students seem to outperform Female students in both Maths and Science. Female students seem to outperform Male students in Reading.\n\n\n\n\n4) Relationship between Scores and Socioeconomic status of students\nWe will create a new subset with ESCS and the PV1 scores for this visualization.\n\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)\n\n#omiting NA values\nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\nUsing our new table subset_ESCS_PV1, we will create scatter plots for ESCS versus each PV1 score for each subject using the code below.\n\n\nShow the code\nc_coeff_ESCS_Math &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1MATH)\n\nC_plt1 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1MATH)) +\n  geom_point(color = \"lightblue\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1MATH),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Math, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n    y = \"Math Scores\") +\n  theme_minimal()\n\nc_coeff_ESCS_Read &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1READ)\n\nC_plt2 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1READ)) +\n  geom_point(color = \"lightgreen\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1READ),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Read, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n    y = \"Reading Scores\") +\n  theme_minimal()\n\nc_coeff_ESCS_Scie &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1SCIE)\n\nC_plt3 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1SCIE)) +\n  geom_point(color = \"lightpink\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1SCIE),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Scie, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n       y = \"Science Scores\") +\n  theme_minimal()\n\n\nWe will use patchwork to create a composite plot for our scatter plots.\n\n\nShow the code\npatch4 &lt;- C_plt1 / C_plt2 / C_plt3 + \n              plot_annotation(\n                title = \"Weak positive relationship between Scores and ESCS\")\n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nObservation 4\n\n\n\nThere is a weak positive relationship between subject scores and Socioeconomic statuses. The ESCS score is a composite score calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS).\nFurther analysis could be conducted on the individual components of the ESCS score to check for their individual influence on student performance.\n\n\n\n\n5) Examining the Breakdown of scores per Subject\nWe can further examine the percentage of students per score range for each subject. This might help us examine whether there are specific strengths or weaknesses in the student cohort.\nFirst, we use the pisa.ben.pv() function from the instvy package which calculates student scores from the 10 plausible values and calculates the percentage of students at each proficiency level (Score range) as defined by PISA.\nIn the code below, we will create separate tables for the percentage breakdown of scores for each subject.\n\n\nShow the code\nMath_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nRead_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nScie_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\n\nNext, we can combine these tables and plots into one plot to show the percentage of students per Score Range for all subjects.\n\n\nShow the code\n# Creating a new combined table\n\nMath_Breakdown$Subject &lt;- 'Math'\nRead_Breakdown$Subject &lt;- 'Reading'\nScie_Breakdown$Subject &lt;- 'Science'\n\nCombined_Breakdown &lt;- bind_rows(Math_Breakdown, Read_Breakdown, Scie_Breakdown)\n\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_Breakdown &lt;- Combined_Breakdown %&gt;%\n  mutate(Benchmarks = fct_inorder(Benchmarks))\n\n# Now plot using ggplot\nggplot(Combined_Breakdown, aes(x = Benchmarks, y = Percentage, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +  # Dodge position for the bars\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percentage)),  # This will format the label to have 1 decimal place and a percentage sign\n    position = position_dodge(width = 0.9),  # Match the position of the text with the dodged bars\n    vjust = -0.25,   \n    size = 2  \n  ) +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"For lower Score ranges, students seem to do better in Reading\",\n       x = \"Score Range\",\n       y = \"Percentage of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nObservation 5\n\n\n\nCombined bar plots can allow us to obtain insights on relative performance. For example, for scores below 544.68, we can see that at lower score ranges, students seem to do better in Reading relative to Math and Science. However, at higher score ranges, students do worse in Reading relative to Math and Science."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "",
    "text": "DataVis Makeover\n\n\n\nRemaking a peer’s original design by improving the clarity and aesthetics of charts by creating an alternative design."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#loading-r-packages",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#loading-r-packages",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\n\npacman::p_load(tidyverse, haven, ggrepel, ggthemes, hrbrthemes, patchwork, intsvy, ggdist, ggridges,colorspace, plotly)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#importing-the-data-set",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#importing-the-data-set",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "2.2 Importing the Data set",
    "text": "2.2 Importing the Data set\n\nstu_qqq_SG &lt;- \n  read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#removing-missing-values-and-converting-data-types",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#removing-missing-values-and-converting-data-types",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "2.3 Removing missing values and Converting data types",
    "text": "2.3 Removing missing values and Converting data types\nRows with missing values for ESCS were removed before further analysis.\n\n\nShow the code\nstu_qqq_SG_clean &lt;- stu_qqq_SG[complete.cases(stu_qqq_SG[, \"ESCS\"]), ]\n\n\nCNTSCHID and CNTSTUID’s data types were changed to character.\n\n\nShow the code\nstu_qqq_SG_clean$CNTSCHID &lt;- as.character(stu_qqq_SG_clean$CNTSCHID)\nstu_qqq_SG_clean$CNTSTUID &lt;- as.character(stu_qqq_SG_clean$CNTSTUID)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#binning-variable-escs",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#binning-variable-escs",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "2.4 Binning Variable ESCS",
    "text": "2.4 Binning Variable ESCS\nThe ESCS variable was binned into quantiles using the mutate() and cut() function\n\n\nShow the code\nstu_qqq_SG_clean &lt;- stu_qqq_SG_clean %&gt;%\n  mutate(ESCS_recoded = cut(ESCS,breaks=quantile(ESCS,c(0,0.25,0.5,0.75,1)),labels=c(\"Very Low\",\"Low\",\"Medium\",\"High\"),include.lowest=TRUE))"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#recoding-st004d01t-gender-variable",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#recoding-st004d01t-gender-variable",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "2.5 Recoding ST004D01T (Gender) variable",
    "text": "2.5 Recoding ST004D01T (Gender) variable\nVariable ST004D01T, was recoded to labels to “Female” and “Male” respectively.\n\n\nShow the code\nstu_qqq_SG_clean$ST004D01T &lt;- recode(stu_qqq_SG_clean$ST004D01T, \"1\" = \"Female\", \"2\" = \"Male\")"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#general-distribution-of-students-performance-in-math-science-and-reading",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#general-distribution-of-students-performance-in-math-science-and-reading",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "3.1 General distribution of students’ performance in Math, Science and Reading",
    "text": "3.1 General distribution of students’ performance in Math, Science and Reading\nFirst, the author had created a data folder path to save any data files generated:\n\n\nShow the code\ndata_folder_path &lt;- file.path(getwd(), \"data\")\n\n\nThe code below uses the instvy package to extract the composite Means and Standard Deviations of PV values across the population of students that took the assessment.\n\n\nShow the code\npvmathgeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_general\",folder=data_folder_path)\n\npvmathgeneral$subject &lt;- \"Math\"\n\npvreadgeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_general\",folder=data_folder_path)\n\npvreadgeneral$subject &lt;- \"Reading\"\n\npvsciegeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_general\",folder=data_folder_path)\n\npvsciegeneral$subject &lt;- \"Science\"\n\nmergedgeneral &lt;- rbind(pvmathgeneral,pvreadgeneral,pvsciegeneral)\nmergedgeneral %&gt;% relocate(subject)\n\n\n  subject Freq   Mean s.e.     SD  s.e\n1    Math 6559 575.27 1.26 102.68 0.93\n2 Reading 6559 543.25 1.91 105.73 1.16\n3 Science 6559 561.97 1.33  99.02 1.10\n\n\nA plot to show the general distribution of Math, Reading and Science scores was generated using the code below.\n\nThe original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the distribution of Student scores across the 3 subjects.\n\n\n\np1&lt;- ggplot(mergedgeneral, aes(x = subject, y = Mean,colour=subject)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    aes(colour=subject),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD),\n    width = 0.5,\n    size=0.8,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Scores across Math, Reading and Science across all 15-year olds who took PISA 2022 in SGP\") +\n  ylab(\"Scores\") +\n  xlab(\"Subject\")+\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, by = 50))+\n  coord_flip() +\n  theme_minimal(base_size = 12)\np1\n\n\n\n\n\n3.1.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across subjects, I am unclear as to which score this is showing. Is it PV1 or average of the PV values?\nGraph is titled as “Scores across Math, Reading and Science”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this plot only has 3 observations and 6 variables. Upon inspecting the data set, it is actually the composite mean score for each subject for the entire student cohort.\n\n\nThe code and subset used is not suitable to plot a Distribution of student scores across subjects. I will extract a new subset with all PV1 scores at the individual student level. This will then be used to plot the distribution of student scores for the different subjects.\n\n\n\nAesthetics\n\nThere is already a legend on the right which identifies subjects by colour, thus making the Y axis labels seem redundant.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated. This contributes to “extra” space and additional non-data ink.\nUnable to decipher the actual scores even with the X-axis tick marks.\nSince the subjects are clearly labelled in the Y axis, the label “Subject” can be removed, to minimize non-data ink.\n\n\nTo remove the legend or Y-Axis labels, to reduce non-data ink.\nShorten the X-axis, or extract new data points to populate the correct range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\nTo remove the label “Subject”, since it does not provide additional information to readers.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nUpon closer examination of the code used, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD),\n    #width = 0.5,\n    #size=0.8,\n    #position = position_dodge(width = 0.5)\n\nI would conclude that this plot is actually comparing the mean subject scores across subjects.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(mergedgeneral)\n\n  Freq   Mean s.e.     SD  s.e subject\n1 6559 575.27 1.26 102.68 0.93    Math\n2 6559 543.25 1.91 105.73 1.16 Reading\n3 6559 561.97 1.33  99.02 1.10 Science\n\n\nThe mergedgeneral subset table only has 3 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe code and data subset used by the author will not be able to plot a Distribution of student scores across subjects.\nI will revert to the original large data set to extract PV1 scores which are available for all students to create new plots to visualize the distribution of scores.\n\n\n\n\n3.1.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.1.3 Remake\nThe original data sub set and plot is unable to exhibit the distribution of student scores across subjects, due to the lack of scores at an individual student level.\nFirst, I will use the code below derive some reference lines (mean and median) for our histograms.\nTo populate the distribution of scores for the student cohort, I will use PV1 scores, which are available for all students.\n\n\nShow the code\nmean_pv1math &lt;- mean(stu_qqq_SG$PV1MATH)\nmedian_pv1math &lt;- median(stu_qqq_SG$PV1MATH)\nmean_pv1read &lt;- mean(stu_qqq_SG$PV1READ)\nmedian_pv1read &lt;- median(stu_qqq_SG$PV1READ)\nmean_pv1scie &lt;- mean(stu_qqq_SG$PV1SCIE)\nmedian_pv1scie &lt;- median(stu_qqq_SG$PV1SCIE)\n\n\nNext, I will use the code below and extract data points (PV1 scores) from the original large data set to create Histograms with Density plots for each subject.\n\n\nShow the code\np_1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightblue') +\n  geom_density(color = \"purple\") + \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Math Scores\", y = \"Density\") + # Change label to 'Density'\n  geom_vline(xintercept = mean_pv1math, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1math, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1math, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1math, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1math, y = 0.0025, label = paste(\"Median:\", round(median_pv1math, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightgreen') +\n  geom_density(color = \"purple\") + \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Reading Scores\", y = \"Density\") + \n  geom_vline(xintercept = mean_pv1read, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1read, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1read, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1read, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1read, y = 0.0025, label = paste(\"Median:\", round(median_pv1read, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightpink') +\n  geom_density(color = \"purple\") +  \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Science Scores\", y = \"Density\") + \n  geom_vline(xintercept = mean_pv1scie, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1scie, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1scie, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1scie, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1scie, y = 0.0025, label = paste(\"Median:\", round(median_pv1scie, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +\n  geom_density(alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +\n  labs(x = \"Combined Scores\",\n       y = \"Density\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  guides(fill = FALSE) +  # Remove the legend\n  theme_minimal()\n\n\nFinally, using the code below, I will create a composite graph for all the histograms and density plots.\n\n\nShow the code\npatch1 &lt;- (p_1 + p_2) / (p_3 + p_4)  + \n              plot_annotation(\n                title = \"Distribution of student performance in Math, Reading and Science\",\n                subtitle = \"All subjects exhibit a slight left-skewed distribution\" )\n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe revised plot consists of a patchwork of 3 histograms with density curves, and a combined density curve overlay for the 3 subjects.\n\nReference lines for the mean and median scores are added to enable readers to infer the skewness of the distribution of student scores.\n\nThe revised plot will enable readers to quickly infer the nature of the distributions of the subject scores for the student cohort."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-students-performance-in-math-science-and-reading-across-schools",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-students-performance-in-math-science-and-reading-across-schools",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "3.2 Distribution of students’ performance in Math, Science and Reading across schools",
    "text": "3.2 Distribution of students’ performance in Math, Science and Reading across schools\nThe author has used the pisa.mean.pv function from intsvypackage, to obtain the composite Mean and Standard Deviation values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each school.\nThe code below generates the composite PV values.\n\n\nShow the code\npvmathsch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bysch\",folder=data_folder_path)\n\npvreadsch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bysch\",folder=data_folder_path)\n\npvsciesch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bysch\",folder=data_folder_path)\n\n\nA plot to show the distribution of students performance in Math, Reading and Science scores across schools was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the subject scores by each school id (164 schools).\n\n\n\np2&lt;- ggplot(pvmathsch, aes(x = as.factor(CNTSCHID), y = Mean)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=1.5) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    position = position_dodge(width = 0.75),\n    color = \"red\"\n  ) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD),\n    width = 0.8,\n    size=1,\n    position = position_dodge(width = 10)\n  ) +\n  labs(title = \"Math Scores by School\") +\n  ylab(\"Math Score\") +\n  xlab(\"School ID\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=100))+\n  coord_flip() +\n  theme_minimal(base_size=20)             \n\n\n\n\n\n3.2.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across schools, I am unclear as to which score this is showing. Is it showing me the range of student scores by school ID?\nGraph is titled as “Subject Scores by school”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this visualization only has 164 observations and 6 variables. Upon inspecting the data set, this subset is basically the composite mean subject scores for each school.\n\n\nThe original subset can still be used, but the code will need to be changed for the revised visualization. This new visualization will retain the original intent to show the different mean scores for each of the 164 schools to visualize the relative performance between schools.\n\n\n\nAesthetics\n\nThe use of a vertically long plot to compare the scores across schools makes it visually challenging and difficult to spot differences in scores, and identify any clusters.\nSince there are 164 schools, the use of a vertically long format makes it difficult to read the school IDs on the Y-axis. There are too many schools to make identification off the Y-axis easy.\nThe distance between the X-axis, which shows the scores is too far apart from the error bars. I am unable to decipher what are the actual scores or score range for each error bar.\nThe graph is stiched side by side, resulting in the Y-axis being repeated 3 times. The order of the School IDs are all the same, hence this creates additional non-data ink, that makes the graph too “busy”.\n\n\nTo revise the design of a vertically long format and re-make it to a horizontally compact visualization, by using bubble plots.\nTo minimize axis labels, we can drop axis labels for school id, and only identify and directly annotate the top & bottom 5 schools instead.\nTo remove the 2 other repetitive Y-axes to reduce non-data ink.\n\n\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(head(pvmathsch))\n\n  CNTSCHID Freq   Mean  s.e.    SD   s.e\n1 70200001   55 725.21  9.34 59.23  6.38\n2 70200002   37 536.19 17.09 90.27 14.50\n3 70200003   36 739.92 12.30 59.23  7.70\n4 70200004   56 509.61 12.84 86.63  7.71\n5 70200005   37 548.39 13.10 86.30  9.18\n6 70200006   36 485.30 13.90 76.47  8.86\n\n\nThe pvmathsch subset table only has 164 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe data subset can only be used to visualize the mean scores per school ID.\nThe original graph is not of a suitable design to help readers easily and quickly understand the disparity in performances between schools.\nI will use a new compact visualization to enable the reader to quickly see the differences in school performance.\n\n\n\n\n3.2.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.2.3 Remake\nThe original plot does not enable the reader to quickly capture information from the data points.\nUsing the exact same data set, I will use the code below to re-make the original graph into a Bubble plots instead. I will employ color as a means to differentiate the mean scores.\nI have also added reference lines to show the 10th and 90th percentile scores to help readers easily identify the schools belonging to these two opposite segments.\nLastly, I have also added annotations to help readers to quickly identify the top 5 and bottom 5 schools with the best and worst mean scores.\n\n\nShow the code\nlibrary(ggrepel)\n\n# Identify the top 5 and bottom 5 schools\ntop_5 &lt;- pvmathsch %&gt;% \n  arrange(desc(Mean)) %&gt;%\n  slice(1:5)\n\nbottom_5 &lt;- pvmathsch %&gt;% \n  arrange(Mean) %&gt;%\n  slice(1:5)\n\n# Base plot with points colored by the mean score\np1 &lt;- ggplot(pvmathsch, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(color = Mean), alpha = 0.5, size = 7) +\n  scale_color_gradient(low = \"red\", high = \"green\") +\n  labs(title = \"Mean Math Scores per School\",\n       subtitle = \"5 Schools with the Best & Worst mean scores\",\n       x = \"School ID\",\n       y = \"Mean Math Scores\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.grid.major = element_blank())  # Remove major grid lines\n       \n\n# Annotate the top 5 and bottom 5 schools\np1 &lt;- p1 +\n  geom_text_repel(\n    data = top_5, \n    aes(label = CNTSCHID, y = Mean), \n    color = \"blue\", \n    size = 3, \n    nudge_y = 12,   # Adjust nudge_y if necessary to move text up or down\n    direction = \"y\"\n  ) +\n  geom_text_repel(\n    data = bottom_5, \n    aes(label = CNTSCHID, y = Mean), \n    color = \"blue\", \n    size = 3, \n    nudge_y = -12,  # Adjust nudge_y if necessary to move text up or down\n    direction = \"y\"\n  )\n\n# Calculate the 10th and 90th percentiles\npercentile10 &lt;- quantile(pvmathsch$Mean, probs = 0.10, na.rm = TRUE)\npercentile90 &lt;- quantile(pvmathsch$Mean, probs = 0.90, na.rm = TRUE)\n\n# Add horizontal lines for the 10th and 90th percentiles to your plot\np1 &lt;- p1 + \n  geom_hline(yintercept = percentile10, linetype = \"dashed\", color = \"blue\", size = 0.5) +\n  geom_hline(yintercept = percentile90, linetype = \"dashed\", color = \"blue\", size = 0.5) +\n  geom_text(aes(x = Inf, y = percentile10, label = paste(\"10th Percentile:\", round(percentile10, 2))), \n            hjust = 1.05, vjust = 0, color = \"blue\", size = 3) +\n  geom_text(aes(x = Inf, y = percentile90, label = paste(\"90th Percentile:\", round(percentile90, 2))), \n            hjust = 1.05, vjust = 1, color = \"blue\", size = 3)\n\np1\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe use of colour can help readers quickly differentiate the scores between schools.\n\nI have annotated school IDs for the top and bottom 5 schools for readers to quickly identify the “best” and “worst” schools.\nI have also added reference lines for the 10th and 90th Percentile. Here we can quickly see that while most of the 164 schools attain the same range of mean scores, there are a number of schools that outperform the rest.\nAnnotating all the 164 school IDs will add too much ink to the plot. Hence, we can consider having another interactive plot (below) for readers to examine the data points in more detail at their descretion.\n\n\nFor interactivity, I have used ggplotly() to convert this to an interactive graph. With the added interactivity, readers can use the tooltip function to get more information on the data points.\n\n\nShow the code\nlibrary(plotly)\n\n# Convert to an interactive plot\nggplotly(p1, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome of the previous annotations generated in the static plot may not appear after turning it into an interactive plot. Hence, we can use both static and interactive plots together if needed. For example, static charts to quickly summarize and communicate information to reader, and an interactive plot for readers to drill down in more detail, eg use of zoom, identifying specific data points via tooltip etc."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-students-performance-in-math-science-and-reading-by-gender",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-students-performance-in-math-science-and-reading-by-gender",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "3.3 Distribution of students’ performance in Math, Science and Reading by gender",
    "text": "3.3 Distribution of students’ performance in Math, Science and Reading by gender\nThe author had used the pisa.mean.pv function from intsvy package to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each gender.\nThe code below was used to generate the composite PV values for Math, Reading and Science by gender.\n\n\nShow the code\npvmathgenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bygenderonly\",folder=data_folder_path)\n\npvreadgenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bygenderonly\",folder=data_folder_path)\n\npvsciegenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bygenderonly\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores by gender was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the subject scores by gender.\n\n\n\nggplot(pvmathgenderonly, aes(x = as.factor(ST004D01T), y = Mean,fill=ST004D01T)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    aes(colour=ST004D01T),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    width = 0.5,\n    size=0.8,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Math Scores by Gender\") +\n  ylab(\"Math Score\") +\n  xlab(\"Gender\")+\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, by = 100))+\n  scale_fill_discrete(name = \"Gender\") +\n  scale_color_discrete(name = \"Gender\") +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n3.3.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to see the differences in scores between the genders, I am unclear as to which score this is showing. Is it showing the range of PV1 scores or average of PV values?\nGraph is titled as “Subject Scores by Gender”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for the plot only has 2 observations and 6 variables. Upon inspecting the data set, this subset is actually the composite mean score for each gender across the subjects.\n\n\nThe code and subset used is not suitable to plot a range of scores for each gender across subjects.\nI will extract a new subset with all PV1 scores at the individual student level. This will then be used to plot the distribution and range of scores for the different genders across all subjects.\n\n\n\nAesthetics\n\nThe graphs are already well labelled in the x and y axes, and identifies the different genders clearly. The use of different colors per gender together with the legend is not necessary.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated.\nUnable to decipher the actual scores even with the granularity in X-axis tick marks.\n\n\nTo remove the legend since there are only two categories of gender and they are already differentiated by colour.\nShorten the X-axis, or extract new data points to correctly populate the range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\nThe new visualization will need to enable readers to quickly differentiate the performance between genders across subjects.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nupon closer examination of the code below, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    #width = 0.5,\n    #size=0.8,\n    #position = position_dodge(width = 0.5)\n\nThe original data set and plot is instead comparing the mean subject scores across different genders.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(pvmathgenderonly)\n\n  ST004D01T Freq   Mean s.e.     SD  s.e\n1    Female 3227 569.01 1.70  97.49 1.14\n2      Male 3332 581.30 1.74 107.09 1.36\n\n\nThe pvmathgenderonly data set only has 2 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThis data set can only be used to compare the overall mean scores for genders across the 3 subjects.\nThe visualization needs to be done differently to enable the reader to obtain more information regarding the performance between genders.\nI will extract a new subset and create new graphs to show the range and distribution of scores between the genders across subjects.\nI will use PV1 scores which are available for all students to create new plots to visualize the distribution of scores.\n\n\n\n\n3.3.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.3.3 Remake\nThe original data subset and plot is unable to exhibit the range and distribution of subject scores across genders, due to the lack of scores at an individual student level.\nTherefore, I will extract data from the original data set to create new subsets for the new plots using the code below.\nIn terms of data, I will use the PV1 scores which are available for all students.\n\n\nShow the code\n# Create a subset of the data with gender and PV1 score columns\nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)\n\n# Convert the \"ST004D01T\" column to a factor \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nMath_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1MATH, na.rm = TRUE)  # Calculate the mean of PV1MATH, removing NA values\n  )\n\nRead_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1READ, na.rm = TRUE)  # Calculate the mean of PV1READ, removing NA values\n  )\n\nSCIE_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1SCIE, na.rm = TRUE)  # Calculate the mean of PV1SCIE, removing NA values\n  )\n\n\nNext, I will use the code below to create new box plots for each subject. This will show the range of scores for the genders in different subjects.\n\n\nShow the code\n# Create the plot using the subset_data\nbxp1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust =4, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Math Scores\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +  \n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\nbxp2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust = 4.25, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Reading Scores\") +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightgreen\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\") \n  \n\nbxp3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust = 4, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Science Scores\") +\n  scale_fill_manual(values = c(\"lightpink\", \"lightpink\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\nBesides showing the differences in scores through box plots, I will also plot ridgeline plots, segmented into 4 quantiles, to display the distribution and difference in subject scores between genders across the subjects.\n\n\nShow the code\nrp1 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1MATH, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  )  +\n  labs(title = \"Math Scores\\nacross genders\")\n\nrp2 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1READ, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  )  +\n  labs(title = \"Reading Scores \\nacross genders\")\n\nrp3 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1SCIE, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE,\n    alpha = 0.5 ) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  ) +\n  labs(title = \"Science Scores\\nacross genders\")\n\n\nI will use the code below to create a composite graph.\n\n\nShow the code\npatch3 &lt;- (rp1+ bxp1)/(rp2+bxp2)/(rp3+bxp3) + \n              plot_annotation(\n                title = \"Male students outperform in Maths and Science\",\n                subtitle = \"Female students outperform in Reading\")\n\npatch3 & theme(panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe revised plot consists of a patchwork of ridgeline plots and box plots for each subject.\nThe ridgeline plot been further segmented into 4 quantiles, differentiated by colour. This not only provides insights into the distribution of the scores but also enables readers to quickly understand the differences in performance between genders.\nThe box plots, further help readers to understand the differences in scores between genders. The range of scores can be seen along with the mean scores."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-math-reading-and-science-scores-by-socioeconomic-status",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-math-reading-and-science-scores-by-socioeconomic-status",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "3.4 Distribution of Math, Reading and Science scores by socioeconomic status",
    "text": "3.4 Distribution of Math, Reading and Science scores by socioeconomic status\nThe author had previously created a new variable, “ESCS_recoded”, which bins the socio-economic (ESCS) score into four quantiles - “Very Low”, “Low”, “Medium” and “High”. The author had used the pisa.mean.pv function from intsvypackage to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” and grouped by the ESCS score.\nThe code below was used to generate the composite PV values for Math, Reading and Science.\n\n\nShow the code\npvmathescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmathescs\",folder=data_folder_path)\n\npvreadescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvreadescs\",folder=data_folder_path)\n\npvscieescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscieescs\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores across schools by socioeconomic status was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars to represent the scores across the 4 categories of the ESCS score (High, Medium, Low, Very Low).\n\n\n\nggplot(pvscieescs, aes(x = ESCS_recoded, y = Mean,fill=ESCS_recoded))+\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.5),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 10,\n    size = 2,\n    aes(colour=ESCS_recoded),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ESCS_recoded),\n    width = 0.5,\n    size=1,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Science Scores by Socioeconomic Status\") +\n  ylab(\"Science Score\") +\n  xlab(\"Socioeconomic Status\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=200))+\n  coord_flip() +\n  theme_minimal(base_size=10)\n\n\n\n\n\n3.4.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores between the ESCS categories, I am unclear as to which score this is showing. Is it showing me the range of PV1 scores or average of PV values?\nGraph is titled as “Subject Scores by Socioeconomic Status”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for the plots only has 4 observations and 6 variables. Upon inspecting the data set, this is actually just the overall mean scores for each of the newly binned ESCS score category.\n\n\nThe code and subset used is not suitable to plot a range or distribution of scores for each ESCS score category.\nI will extract a new subset with all PV1 scores at the individual student level together with ESCS scores per student. I will then plot the relationship between the two continuous variables (subject scores and ESCS score) via scatter plots.\nA scatter plots will enable readers to quickly infer the relationships between subject scores and socioeconomic statuses.\n\n\n\nAesthetics\n\nThe Y-axis is already well labelled and identifies the different socioeconomic segments clearly, the use of different colors per segment together with legend is not necessary.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated.\nEven with the background grid lines and X axis tick marks, I am unable to decipher the actual scores clearly.\n\n\nTo remove the legend since the ESCS segments are already differentiated by colour and identifiable through the Y-axis labels.\nShorten the X-axis, or extract new data points to correctly populate the range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nUpon closer examination of the code below, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD,color=ESCS_recoded),\n    #width = 0.5,\n    #size=1,\n    #position = position_dodge(width = 0.5)\n\nHence the original plot can only be used to compare the mean subject scores across different ESCS segments.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(pvscieescs)\n\n  ESCS_recoded Freq   Mean s.e.    SD  s.e\n1     Very Low 1640 503.73 3.14 97.91 1.97\n2          Low 1640 546.33 2.61 93.23 1.75\n3       Medium 1640 584.61 2.31 86.07 2.04\n4         High 1639 610.62 3.23 84.01 2.21\n\n\nThe data subset used for the plots only has 4 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThis subset and the code used is not able to plot the range of scores per ESCS segment, and can only be used to compare the composite mean scores of each of the 4 ESCS segments.\nAlthough it is useful to compare the mean scores between different ESCS segments, a different set of data and plots can be used instead to communicate more information about the strength or weakness of the relationship between subject scores and socioeconomic statuses.\n\n\nAdditionally, the binning of the ESCS score, while helpful may be limited in usefulness, for the following reasons:-\n\nBinning the ESCS scores adds in an element of subjectivity. For example, why should there be 4 segments of socioeconomic statuses instead of 3, or 6?\nCould the number of socioeconomic segments be applicable for other countries? For example, would a “Low segment” in Singapore be equivalent to a “Low segment” in another country?\n\n\n\n3.4.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.4.4 Remake\nInstead of binning the ESCS score and transforming it to a categorical type, I will keep the ESCS score as a continuous variable.\nI will use scatter plots to visualize the strength or weaknesses of the relationships between the socioeconomic statuses and subject scores.\nDrawing inspiration from Prof Kam’s Lesson 3 and well as additional useful references, I will proceed to plot Scatter plots with Marginal Histograms.\nIn terms of data, I will use the PV1 scores which at the individual student level.\nFor this visualization, I will use the ggstatplot package. (Patil 2021)\nFor more information on this package and how it can be used to create graphics from statistical tests included in the plots themselves, please refer to this link.\n\nlibrary(ggstatsplot)\n\nCreating new data subset for visualization.\n\n\nShow the code\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)\n\n#omiting NA values\nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\n\nWe will use the code below from ggstatplot package to generate the plot.\n\n\nShow the code\nggscatterstats(\n  data = subset_ESCS_PV1,                                          \n  x = ESCS,                                                  \n  y = PV1MATH,\n  xlab = \"Socioeconomic score (ESCS)\",\n  ylab = \"Math scores\",\n  marginal = TRUE,\n  marginal.type = \"histogram\",\n  centrality.para = \"mean\",\n  margins = \"both\",\n  title = \"Relationship between Socio-economic and Math scores\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe benefit of this plot is that it shows both the correlation between the two continous variables as well as their respective distributions, and includes important statistics like the Pearson coefficient.\nFrom the plot above, the pearson coefficient of 0.42 indicates that there is a weak positive relationship between ESCS scores and Math scores.\nThe marginal histograms for both variables also enables readers to additionally infer the distribution of these variables. For example, Math scores resemble a normal distribution, while ESCS scores resemble a left-skewed distribution."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-students-performance-in-math-science-and-reading-by-gender-across-schools",
    "href": "Take-home-ex/Take-home-Ex2/Take-home_Ex2.html#distribution-of-students-performance-in-math-science-and-reading-by-gender-across-schools",
    "title": "Take-Home Exercise 2 : DataVis Makeover",
    "section": "3.5 Distribution of students’ performance in Math, Science and Reading by gender across schools",
    "text": "3.5 Distribution of students’ performance in Math, Science and Reading by gender across schools\nThe author had intended to analyse whether there are differences in performances in Math, Reading and Science between genders at a more granular level - within schools.\nThe author had used the pisa.mean.pv function from intsvypackage to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each school and by gender.\nThe below code was used to generate the composite PV values.\n\n\nShow the code\npvmathgender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bygender\",folder=data_folder_path)\n\npvreadgender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bygender\",folder=data_folder_path)\n\npvsciegender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),by= c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bygender\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores across schools by gender, was generated using the code below.\n\nThe Original PlotThe code\n\n\n\nThe author had created a series of 2 error bars for each school ID across the 3 subjects to represent the subject scores for each gender for all schools (164 schools).\n\n\n\np8&lt;- ggplot(pvmathgender, aes(x = as.factor(CNTSCHID), y = Mean,fill=ST004D01T))+\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=1.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 10,\n    aes(colour=ST004D01T),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    width = 1,\n    size=2,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Math Scores by Gender across Schools\") +\n  ylab(\"Math Score\") +\n  xlab(\"School ID\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=200))+\n  coord_flip() +\n  theme_minimal(base_size=50)+\n  theme(legend.position=\"none\")\n\n\n\n\n\n3.5.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across schools, I am unclear as to which score this is showing. Is it showing me the range of gender scores by school ID?\nGraph is titled as “Subject Scores by Gender across school”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this visualization has 301 observations and 7 variables. Upon inspecting the data set, it is basically the overall mean subject scores for male and female students per school\n\n\nPrevious visualization had already analysed the differences per school. To further make it more granular by introducing gender, does not seem to add further value in terms of insights.\nWe can retain the original intent to show the granular difference in performance for the schools, by examining the actual disparity between the best and the worst schools instead.\n\n\n\nAesthetics\n\nThe use of a vertically long plot to compare the scores across schools makes it visually challenging and difficult to spot differences between schools, and identify any clusters.\nSince there are 164 schools, the use of a vertically long format makes it difficult to read the Y-axis labels. There are too many schools to make identification off the Y-axis easy. This is further compounded with the additional splitting of genders for every school.\nThe distance between the X-axis, which shows the scores is too far apart from the error bars. I am unable to decipher what are the actual scores or score range for each error bar.\nThe graph is patched side by side, resulting in the Y-axis being repeated 3 times. The order of the School IDs are all the same, hence this creates additional non-data ink, that makes the graph too “busy”.\n\n\nTo revise the design of a vertically long format and re-make it to a horizontally compact visualization, by using ridge line plots to show the differences in performance only between the best and worst schools.\nTo minimize axis labels, we will only identify and plot the top & bottom 5 schools instead.\n\n\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(head(pvmathgender))\n\n  CNTSCHID ST004D01T Freq   Mean  s.e.    SD   s.e\n1 70200001      Male   55 725.21  9.34 59.23  6.38\n2 70200002    Female   15 537.70 27.07 80.64 15.79\n3 70200002      Male   22 535.13 21.92 96.03 20.81\n4 70200003    Female    7 739.65 22.04 39.42 14.39\n5 70200003      Male   29 739.99 14.58 62.67  8.56\n6 70200004    Female   28 505.32 19.68 91.11 11.94\n\n\nThe pvmathgender subset table has 307 observations and 7 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe data subset can only be used to visualize the mean scores of the two genders for every school.\nThe original graph is not of a suitable design to help readers easily and quickly understand the disparity in performances between schools. By adding a further split into genders per school further complicates this visually.\nI will use a new compact visualization to enable the reader to quickly identify the disparity in school performance. Instead of analyzing all schools, we can narrow our focus just to the “best” and “worst” schools.\n\n\n\n\n3.5.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.5.3 Remake\nThe author had previously extracted the mean scores per school across the 3 subjects. I will use the same sub sets to extract the top and bottom 5 School IDs for each subject.\n\n\nShow the code\n# Identify the top and bottom 5 schools\ntop_bottom_math &lt;- pvmathsch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\ntop_bottom_read &lt;- pvreadsch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\ntop_bottom_scie &lt;- pvsciesch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\n\nNext I will filter the data set and obtain PV1 student scores from the top and bottom 5 schools only.\n\n\nShow the code\n# Filter the original dataset to include only the selected schools\n\ntop_bottom_mathsch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_math)\n\ntop_bottom_readsch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_read)\n\ntop_bottom_sciesch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_scie)\n\n\ntop_bottom_mathsch &lt;- top_bottom_mathsch %&gt;%\n  mutate(CNTSCHID = factor(CNTSCHID, levels = top_bottom_math))\n\n\nLastly, I will use the code below to create Ridgeline plots for the top and bottom 5 schools.\n\n\nShow the code\nmath_5 &lt;- ggplot(top_bottom_mathsch, aes(x = PV1MATH, y = factor(CNTSCHID), fill = after_stat(x))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_gradientn(\n    name = \"Math Scores\",\n    colors = c(\"red\", \"green\"),\n    limits = c(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE)),\n    breaks = seq(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE), length.out = 5)\n  ) +\n  coord_cartesian(xlim=c(0,1000)) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 15)  \n  )  +\n  labs(y = \"School ID\", x = NULL, title = \"Math Scores for the Top and Bottom 5 Schools\") +\n  theme(\n    axis.text.y = element_text(angle = 0, hjust = 0.5)  \n  )\n\n\nmath_5\n\n\n\n\n\n\n\n\n\n\n\nRevised plot\n\n\n\nBy narrowing our focus to compare only the top and bottom 5 schools, we are able to clearly infer the actual difference between the “best” and “worst” schools.\nThe new revised plot is now more visually compact, and allows readers to quickly examine the difference in performance between the top and bottom 5 schools.\n\n\nBesides this, we can also examine the differences in performance between Male and Female students in these top and bottom 5 schools, using the code below.\n\n\nShow the code\n# Convert ST004D01T to a factor with more meaningful level names\ntop_bottom_mathsch$ST004D01T &lt;- factor(top_bottom_mathsch$ST004D01T, levels = c(1, 2), labels = c(\"Girls\", \"Boys\"))\n\n# Plot\nmath_gender_ridges &lt;- ggplot(top_bottom_mathsch, \n                             aes(x = PV1MATH, \n                                 y = interaction(CNTSCHID, ST004D01T),  # Create interaction between school ID and gender\n                                 fill = after_stat(x))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE\n  ) +\n  scale_fill_gradientn(\n    name = \"Math Scores\",\n    colors = c(\"red\", \"green\"),\n    limits = c(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE)),\n    breaks = seq(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE), length.out = 5)\n  ) +\n  coord_cartesian(xlim = c(0, 1000)) +\n  theme_ridges() +\n  labs(y = \"School ID and Gender\", x = \"Math Scores\", title = \"Math Scores for the Top and Bottom 5 Schools by Gender\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 15),\n    axis.text.y = element_text(angle = 0, hjust = 0.5)  \n  )\n\nmath_gender_ridges\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSchool IDs 70200001, 70200139 and 70200110 does not have any data on Female students and are likely a single-gender school."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the module of our proposed Shiny application and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for our Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returns the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications,\nTo select the appropriate Shiny UI components for exposing the parameters determine above, and\nWe are required to include a section called UI design for the different components of the UIs for the proposed design."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#loading-r-packages",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#loading-r-packages",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Loading R packages",
    "text": "Loading R packages\n\npacman::p_load(sf, tidyverse, tmap, dplyr,\n               raster, spatstat, spdep,\n               lubridate, leaflet,\n               plotly, DT, viridis,\n               ggplot2, sfdep)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#importing-the-acled-data",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#importing-the-acled-data",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Importing the ACLED data",
    "text": "Importing the ACLED data\nCountry specific data from the Armed Conflict Location & Event Data Project (ACLED) can be downloaded at https://acleddata.com/data-export-tool/\n\nACLED_MMR &lt;- read_csv(\"data/MMR.csv\")\n\n\nclass(ACLED_MMR)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#downloading-and-loading-the-shape-files-for-country",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#downloading-and-loading-the-shape-files-for-country",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Downloading and loading the shape files for country",
    "text": "Downloading and loading the shape files for country\nShape files were downloaded from the Myanmmar Information Management Unit (MIMU) website at https://geonode.themimu.info/layers/?limit=100&offset=0\nThis source was chosen over GADM and GeoBoundaries due to its updated administrative region information and map levels.\n\n\n\n\n\n\nImportant- Data Quality Issue with ACLED data\n\n\n\nACLED captures event data from national, sub-national and other credible media sources, and populates event locations based on the last known information.\n\nHowever, due to the dynamic nature of conflict and politics, country/administrative boundaries and borders can sometimes be fluid. Names of administrative areas were found to have changed; either disaggregated into new countries/administrative areas or previously active but now defunct. Further, some administrative areas were agglomerated and upgraded into higher tier administrative areas.\nAs part of our data cleaning and preparation process, I had to identify discrepancies in both admin1 and admin2 data levels and re-name some administrative areas to sync with the downloaded shape files from MIMU."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#loading-admin1administrative-regionarea-shape-files",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#loading-admin1administrative-regionarea-shape-files",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Loading Admin1(administrative region/area) shape files",
    "text": "Loading Admin1(administrative region/area) shape files\n\nmmr_shp_mimu_1 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda2_adm1_250k_mimu_1\")\n\nReading layer `mmr_polbnda2_adm1_250k_mimu_1' from data source \n  `C:\\imranmi\\ISSS608-VAA\\Take-home-ex\\Take-home-Ex4\\data\\geospatial3' \n  using driver `ESRI Shapefile'\nSimple feature collection with 18 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nclass(mmr_shp_mimu_1)\n\n[1] \"sf\"         \"data.frame\"\n\n\nThe Shape file for admin1 level map, is an SF object, with geometry type: Multipolygon\n\nst_geometry(mmr_shp_mimu_1)\n\nGeometry set for 18 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\n\nunique_regions_mimu1 &lt;- unique(mmr_shp_mimu_1$ST)\n\nunique_regions_mimu1\n\n [1] \"Ayeyarwady\"   \"Bago (East)\"  \"Bago (West)\"  \"Chin\"         \"Kachin\"      \n [6] \"Kayah\"        \"Kayin\"        \"Magway\"       \"Mandalay\"     \"Mon\"         \n[11] \"Nay Pyi Taw\"  \"Rakhine\"      \"Sagaing\"      \"Shan (East)\"  \"Shan (North)\"\n[16] \"Shan (South)\" \"Tanintharyi\"  \"Yangon\"      \n\n\nThere are 18 admin1 levels or states/regions in mmr_shp_mimu_1\nLets compare with our admin1 levels in our main dataset ACLED_MMR\n\nunique_acled_regions1 &lt;- unique(ACLED_MMR$admin1)\n\nunique_acled_regions1\n\n [1] \"Rakhine\"     \"Bago-East\"   \"Sagaing\"     \"Shan-North\"  \"Mandalay\"   \n [6] \"Mon\"         \"Yangon\"      \"Shan-South\"  \"Kayin\"       \"Kachin\"     \n[11] \"Magway\"      \"Ayeyarwady\"  \"Nay Pyi Taw\" \"Kayah\"       \"Chin\"       \n[16] \"Bago-West\"   \"Tanintharyi\" \"Shan-East\"  \n\n\nI will write a simple function to identify the discrepancies between the shape file and the region names in our main dataset.\n\n# Find the unique region names that are in 'unique_acled_regions1' but not in 'unique_regions_mimu1'\n\nmismatched_admin1 &lt;- setdiff(unique_acled_regions1, unique_regions_mimu1)\n\nif (length(mismatched_admin1) &gt; 0) {\n  print(\"The following region names from 'acled_mmr' do not match any in 'mimu1':\")\n  print(mismatched_admin1)\n} else {\n  print(\"All unique region names in 'acled_mmr' match the unique region names in 'mimu1.'\")\n}\n\n[1] \"The following region names from 'acled_mmr' do not match any in 'mimu1':\"\n[1] \"Bago-East\"  \"Shan-North\" \"Shan-South\" \"Bago-West\"  \"Shan-East\" \n\n\nLets harmonize the names in both data files. I will resave it to a new data set called ACLED_MMR_1\n\nACLED_MMR_1 &lt;- ACLED_MMR %&gt;%\n  mutate(admin1 = case_when(\n    admin1 == \"Bago-East\" ~ \"Bago (East)\",\n    admin1 == \"Bago-West\" ~ \"Bago (West)\",\n    admin1 == \"Shan-North\" ~ \"Shan (North)\",\n    admin1 == \"Shan-South\" ~ \"Shan (South)\",\n    admin1 == \"Shan-East\" ~ \"Shan (East)\",\n    TRUE ~ as.character(admin1)\n  ))\n\nChecking if our changes are successful.\n\n# Get unique admin 1 region names from 'ACLED_MMR_1'\nunique_acled_regions1 &lt;- unique(ACLED_MMR_1$admin1)\n\n# Get unique region names from 'mmr_shp_mimu_1'\nunique_map_regions_mimu1 &lt;- unique(mmr_shp_mimu_1$ST)\n\n# Find the unique region names that are in 'unique_acled_regions1' but not in 'unique_map_regions_mimu1'\n\nmismatched_regions &lt;- setdiff(unique_acled_regions1, unique_map_regions_mimu1)\n\nif (length(mismatched_regions) &gt; 0) {\n  print(\"The following region names from 'acled_mmr_1' do not match any in 'mmr_shp_mimu_1':\")\n  print(mismatched_regions)\n} else {\n  print(\"All unique region names in 'acled_mmr_1' match the unique region names in 'mmmr_shp_mimu_1.'\")\n}\n\n[1] \"All unique region names in 'acled_mmr_1' match the unique region names in 'mmmr_shp_mimu_1.'\"\n\n\nLets do a sample plot to see how our country map looks like at admin1 level\n\nplot(mmr_shp_mimu_1)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#loading-admin2-administrative-regionarea-shape-files",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#loading-admin2-administrative-regionarea-shape-files",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Loading Admin2 (administrative region/area) shape files",
    "text": "Loading Admin2 (administrative region/area) shape files\n\nmmr_shp_mimu_2 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\imranmi\\ISSS608-VAA\\Take-home-ex\\Take-home-Ex4\\data\\geospatial3' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nclass(mmr_shp_mimu_2)\n\n[1] \"sf\"         \"data.frame\"\n\n\nThe Shape file for admin2 level map, is an SF object, with geometry type: Multipolygon\n\nst_geometry(mmr_shp_mimu_2)\n\nGeometry set for 80 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\n\nunique_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\nunique_regions_mimu2\n\n [1] \"Hinthada\"                        \"Labutta\"                        \n [3] \"Maubin\"                          \"Myaungmya\"                      \n [5] \"Pathein\"                         \"Pyapon\"                         \n [7] \"Bago\"                            \"Taungoo\"                        \n [9] \"Pyay\"                            \"Thayarwady\"                     \n[11] \"Falam\"                           \"Hakha\"                          \n[13] \"Matupi\"                          \"Mindat\"                         \n[15] \"Bhamo\"                           \"Mohnyin\"                        \n[17] \"Myitkyina\"                       \"Puta-O\"                         \n[19] \"Bawlake\"                         \"Loikaw\"                         \n[21] \"Hpa-An\"                          \"Hpapun\"                         \n[23] \"Kawkareik\"                       \"Myawaddy\"                       \n[25] \"Gangaw\"                          \"Magway\"                         \n[27] \"Minbu\"                           \"Pakokku\"                        \n[29] \"Thayet\"                          \"Kyaukse\"                        \n[31] \"Maungdaw\"                        \"Mrauk-U\"                        \n[33] \"Sittwe\"                          \"Thandwe\"                        \n[35] \"Hkamti\"                          \"Kale\"                           \n[37] \"Kanbalu\"                         \"Katha\"                          \n[39] \"Kawlin\"                          \"Mawlaik\"                        \n[41] \"Monywa\"                          \"Naga Self-Administered Zone\"    \n[43] \"Sagaing\"                         \"Shwebo\"                         \n[45] \"Tamu\"                            \"Yinmarbin\"                      \n[47] \"Kengtung\"                        \"Monghsat\"                       \n[49] \"Tachileik\"                       \"Hopang\"                         \n[51] \"Kokang Self-Administered Zone\"   \"Kyaukme\"                        \n[53] \"Lashio\"                          \"Matman\"                         \n[55] \"Mongmit\"                         \"Muse\"                           \n[57] \"Pa Laung Self-Administered Zone\" \"Danu Self-Administered Zone\"    \n[59] \"Langkho\"                         \"Loilen\"                         \n[61] \"Pa-O Self-Administered Zone\"     \"Taunggyi\"                       \n[63] \"Dawei\"                           \"Kawthoung\"                      \n[65] \"Mandalay\"                        \"Meiktila\"                       \n[67] \"Myingyan\"                        \"Nyaung-U\"                       \n[69] \"Pyinoolwin\"                      \"Yamethin\"                       \n[71] \"Mawlamyine\"                      \"Thaton\"                         \n[73] \"Det Khi Na\"                      \"Oke Ta Ra\"                      \n[75] \"Kyaukpyu\"                        \"Myeik\"                          \n[77] \"Yangon (East)\"                   \"Yangon (North)\"                 \n[79] \"Yangon (South)\"                  \"Yangon (West)\"                  \n\n\nThere are 80 admin2 levels or states/districts in mmr_shp_mimu_2\nLets compare with our admin2 levels in our main dataset ACLED_MMR\n\nunique_acled_regions2 &lt;- unique(ACLED_MMR$admin2)\n\nunique_acled_regions2\n\n [1] \"Maungdaw\"                        \"Bago\"                           \n [3] \"Shwebo\"                          \"Kyaukme\"                        \n [5] \"Pyinoolwin\"                      \"Muse\"                           \n [7] \"Sittwe\"                          \"Yinmarbin\"                      \n [9] \"Thaton\"                          \"Yangon-North\"                   \n[11] \"Pa-O Self-Administered Zone\"     \"Hpapun\"                         \n[13] \"Kyaukpyu\"                        \"Yangon-West\"                    \n[15] \"Mongmit\"                         \"Bhamo\"                          \n[17] \"Mrauk-U\"                         \"Yangon-East\"                    \n[19] \"Yangon-South\"                    \"Monywa\"                         \n[21] \"Gangaw\"                          \"Pathein\"                        \n[23] \"Katha\"                           \"Taungoo\"                        \n[25] \"Kanbalu\"                         \"Lashio\"                         \n[27] \"Mawlamyine\"                      \"Myitkyina\"                      \n[29] \"Kawkareik\"                       \"Loilen\"                         \n[31] \"Mandalay\"                        \"Kawlin\"                         \n[33] \"Kyaukse\"                         \"Magway\"                         \n[35] \"Meiktila\"                        \"Pakokku\"                        \n[37] \"Taunggyi\"                        \"Tamu\"                           \n[39] \"Nay Pyi Taw\"                     \"Mohnyin\"                        \n[41] \"Kale\"                            \"Det Khi Na\"                     \n[43] \"Myingyan\"                        \"Loikaw\"                         \n[45] \"Matupi\"                          \"Pyay\"                           \n[47] \"Sagaing\"                         \"Myeik\"                          \n[49] \"Dawei\"                           \"Thayarwady\"                     \n[51] \"Thandwe\"                         \"Mawlaik\"                        \n[53] \"Bawlake\"                         \"Pyapon\"                         \n[55] \"Hinthada\"                        \"Thayet\"                         \n[57] \"Pa Laung Self-Administered Zone\" \"Mindat\"                         \n[59] \"Hkamti\"                          \"Kokang Self-Administered Zone\"  \n[61] \"Hpa-An\"                          \"Danu Self-Administered Zone\"    \n[63] \"Myawaddy\"                        \"Maubin\"                         \n[65] \"Hakha\"                           \"Falam\"                          \n[67] \"Minbu\"                           \"Monghsat\"                       \n[69] \"Puta-O\"                          \"Hopang\"                         \n[71] \"Nyaung-U\"                        \"Kawthoung\"                      \n[73] \"Yamethin\"                        \"Yangon\"                         \n[75] \"Myaungmya\"                       \"Mong Pawk (Wa SAD)\"             \n[77] \"Oke Ta Ra\"                       \"Matman\"                         \n[79] \"Kengtung\"                        \"Naga Self-Administered Zone\"    \n[81] \"Labutta\"                         \"Langkho\"                        \n[83] \"Tachileik\"                      \n\n\nI will write a simple function to identify the discrepancies between the shape file and our state/district names in our main dataset.\n\n# Find the unique region names that are in 'unique_acled_regions2' but not in 'unique_regions_mimu2'\n\nmismatched_admin2 &lt;- setdiff(unique_acled_regions2, unique_regions_mimu2)\n\nif (length(mismatched_admin2) &gt; 0) {\n  print(\"The following region names from 'acled_mmr' do not match any in 'mimu2':\")\n  print(mismatched_admin2)\n} else {\n  print(\"All unique region names in 'acled_mmr' match the unique region names in 'mimu2.'\")\n}\n\n[1] \"The following region names from 'acled_mmr' do not match any in 'mimu2':\"\n[1] \"Yangon-North\"       \"Yangon-West\"        \"Yangon-East\"       \n[4] \"Yangon-South\"       \"Nay Pyi Taw\"        \"Yangon\"            \n[7] \"Mong Pawk (Wa SAD)\"\n\n\nLets harmonize the names in both data files. I will resave it to the previous data set called ACLED_MMR_1\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  mutate(admin2 = case_when(\n    admin2 == \"Yangon-East\" ~ \"Yangon (East)\",\n    admin2 == \"Yangon-West\" ~ \"Yangon (West)\",\n    admin2 == \"Yangon-North\" ~ \"Yangon (North)\",\n    admin2 == \"Yangon-South\" ~ \"Yangon (South)\",\n    admin2 == \"Mong Pawk (Wa SAD)\" ~ \"Tachileik\",\n    admin2 == \"Nay Pyi Taw\" ~ \"Det Khi Na\",\n    admin2 == \"Yangon\" ~ \"Yangon (West)\",\n    TRUE ~ as.character(admin2)\n  ))\n\nChecking if our changes are successful.\n\n# Get unique admin 2 district names from 'ACLED_MMR_1'\nunique_acled_regions2 &lt;- unique(ACLED_MMR_1$admin2)\n\n# Get unique district names from 'mmr_shp_mimu_2'\nunique_map_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\n# Find the unique district names that are in 'unique_acled_regions2' but not in 'unique_map_regions_mimu2'\n\nmismatched_regions2 &lt;- setdiff(unique_acled_regions2, unique_map_regions_mimu2)\n\nif (length(mismatched_regions2) &gt; 0) {\n  print(\"The following district names from 'acled_mmr_1' do not match any in 'mmr_shp_mimu_2':\")\n  print(mismatched_regions2)\n} else {\n  print(\"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\")\n}\n\n[1] \"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\"\n\n\n\n# Assuming your data frame is named 'my_data_frame'\nwrite.csv(ACLED_MMR_1, file = \"ACLED_MMR.csv\", row.names = FALSE)\n\nLets do a sample plot to see how our country map looks like at admin2 (districts) level.\n\nplot(mmr_shp_mimu_2)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#choropleth-map-of-incidents-fatalities-by-admin1-regionstate",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#choropleth-map-of-incidents-fatalities-by-admin1-regionstate",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Choropleth Map of Incidents & Fatalities by Admin1 (Region/State)",
    "text": "Choropleth Map of Incidents & Fatalities by Admin1 (Region/State)\nIn the Shiny App, the below choropleth maps can be plotted, with users being able to choose the following:-\n\nVariable: Count of Incidents or Fatalities\nspecific year or year range\nevent type and sub event type\ndata classification type, and\nnumber of clusters (n)\n\n\nFatalities in Battles in 2022, by QuantilesFatalities in Violence against civilians in 2022, by quantilesIncidents of Riots in 2021, by jenksFatalities in Battles, by sub event = Armed clash, in 2023 (Quantile)Fatalities in Explosions, by sub event = Shelling/Artillery, in 2023 (Quantile)\n\n\n\nACLED_MMR_admin1 %&gt;%\n  filter(year == 2022, event_type == \"Battles\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nACLED_MMR_admin1 %&gt;%\n  filter(year == 2022, event_type == \"Violence against civilians\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nACLED_MMR_admin1 %&gt;%\n  filter(year == 2021, event_type == \"Riots\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Incidents\",\n          n = 5,\n          style = \"jenks\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nACLED_MMR_admin1 %&gt;%\n  filter(year == 2023, event_type == \"Battles\", sub_event_type == \"Armed clash\" ) %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nACLED_MMR_admin1 %&gt;%\n  filter(year == 2023, event_type == \"Explosions/Remote violence\", sub_event_type == \"Shelling/artillery/missile attack\" ) %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Choropleth map of Incidents & Fatalities by Admin2 level (by District)",
    "text": "Choropleth map of Incidents & Fatalities by Admin2 level (by District)\nSimilarly, in the Shiny App, the below choropleth maps can be plotted, with users being able to choose the following:-\n\nVariable: Count of Incidents or Fatalities\nspecific year or year range\nevent type and sub event type\ndata classification type, and\nnumber of clusters (n)\n\n\nFatalities in Battles in 2023, by districts (Quantile)Incidents in Violence against civilians by sub event = Attack in 2021 (Quantile)Fatalities in Explosions, sub event= Air/drone strike in 2023\n\n\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Battles\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2021, event_type == \"Violence against civilians\", sub_event_type == \"Attack\" ) %&gt;%\n  tm_shape() +\n  tm_fill(\"Incidents\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Explosions/Remote violence\", sub_event_type == \"Air/drone strike\" ) %&gt;%\n  tm_shape() +\n  tm_fill(\"Incidents\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#incidents-and-fatalities-by-individual-regions-using-tm_facet",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#incidents-and-fatalities-by-individual-regions-using-tm_facet",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Incidents and Fatalities by individual regions using tm_facet()",
    "text": "Incidents and Fatalities by individual regions using tm_facet()\n\nFatalities in Battles, sub event = Armed Clash in 2023 by RegionIncidents of Violence against civilians in 2022 by Region\n\n\n\nACLED_MMR_admin1 %&gt;%\n  filter(year == 2023, event_type == \"Battles\", sub_event_type == \"Armed clash\" ) %&gt;%\ntm_shape( ) +\n  tm_fill(\"Fatalities\",\n          style = \"quantile\",\n          palette = \"Reds\",\n          thres.poly = 0) + \n  tm_facets(by=\"ST\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nACLED_MMR_admin1 %&gt;%\n  filter(year == 2022, event_type == \"Violence against civilians\") %&gt;%\ntm_shape( ) +\n  tm_fill(\"Incidents\",\n          style = \"quantile\",\n          palette = \"Reds\",\n          thres.poly = 0) + \n  tm_facets(by=\"ST\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nThe above plots using tm_facet() may not necessarily add value to users, so I will KIV for the app."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#visualising-of-fatalities-by-event-type",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#visualising-of-fatalities-by-event-type",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Visualising of Fatalities by Event Type",
    "text": "Visualising of Fatalities by Event Type\nIn the shiny App , we can enable users to filter by\n\nspecific year,\nyear range, and\nevent_type\nrange of number of fatalities\n\nBy using subsets of event types which have been converted to sf objects.\nI will use the Geometry points from our data sets to plot the points of events in the maps using the leaflet package.\nIn this case, i will use admin 1 level regions, to achieve a better aesthetics for users. ie dividing the country map into more districts would likely look too busy, and not value add to users. Further, additional information on the specific event type, region, year, no of fatalities and actors involved can be communicated by means of the tooltip.\n\nBattles from 2010 to presentViolence against civilians from 2010 to presentProtests from 2010 to presentRiots from 2010 to presentExplosions/Remote Devices from 2010 to present\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Battles) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Battles&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Violence_CV) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Violence on Civillians&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Protests) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Protests&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Riots) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Riots&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Explosions) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Explosions&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#computing-local-morans-i",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#computing-local-morans-i",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips &lt;- order(Battles_2023$DT)\nlocalMI &lt;- localmoran(Battles_2023$Incidents, rswm_q)\nhead(localMI)\n\n           Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.46274887 -9.006432e-03 0.1247560902  1.3356292      0.1816705\n2  0.71317847 -1.016877e-02 0.7448368248  0.8381394      0.4019524\n3  0.69218038 -9.386041e-03 0.3392457987  1.2045132      0.2283913\n4  0.68518101 -9.386041e-03 0.3392457987  1.1924960      0.2330668\n5  0.00627746 -1.483579e-05 0.0001702656  0.4822206      0.6296493\n6 -0.23004674 -4.814215e-03 0.0464274459 -1.0453066      0.2958813\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code below lists the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=Battles_2023$DT[fips]),\n  check.names=FALSE)\n\n                                         Ii        E.Ii      Var.Ii        Z.Ii\nBago                             6.2775e-03 -1.4836e-05  1.7027e-04  4.8222e-01\nBawlake                         -1.7909e-02 -4.6941e-04  1.1252e-02 -1.6441e-01\nBhamo                            2.4621e-01 -1.7367e-03  1.9897e-02  1.7577e+00\nDanu Self-Administered Zone      3.3657e-01 -8.2707e-03  1.9670e-01  7.7752e-01\nDawei                            9.4559e-01 -1.4130e-02  5.0825e-01  1.3462e+00\nDet Khi Na                       2.3024e-01 -6.5686e-03  6.3234e-02  9.4170e-01\nFalam                           -7.5203e-03 -2.4381e-03  5.8326e-02 -2.1044e-02\nGangaw                           6.2609e-01 -6.9289e-03  6.6679e-02  2.4514e+00\nHakha                           -1.1347e-02 -6.1003e-05  1.0815e-03 -3.4318e-01\nHinthada                         4.6275e-01 -9.0064e-03  1.2476e-01  1.3356e+00\nHkamti                          -5.3716e-02 -3.0598e-03  3.5009e-02 -2.7074e-01\nHopang                          -2.3647e-01 -9.7735e-03  3.5311e-01 -3.8150e-01\nHpa-An                          -1.3231e-01 -3.0598e-03  1.9751e-02 -9.1968e-01\nHpapun                          -4.1969e-02 -4.2526e-03  5.9189e-02 -1.5503e-01\nKale                             1.2532e-01 -7.7384e-04  6.4571e-03  1.5692e+00\nKanbalu                         -3.2905e-02 -3.4002e-05  3.9022e-04 -1.6640e+00\nKatha                           -8.8073e-03 -1.8682e-02  1.5309e-01  2.5238e-02\nKawkareik                        2.9062e-02 -1.3663e-02  3.2318e-01  7.5156e-02\nKawlin                          -7.2541e-02 -1.4063e-03  3.3678e-02 -3.8762e-01\nKawthoung                       -6.7102e-01 -3.2826e-03  2.4212e-01 -1.3570e+00\nKokang Self-Administered Zone    9.6449e-04 -1.1447e-08  2.7452e-07  1.8408e+00\nKyaukme                         -6.9939e-02 -9.0471e-03  8.6877e-02 -2.0659e-01\nKyaukpyu                         4.7922e-01 -6.8933e-03  1.2137e-01  1.3953e+00\nKyaukse                         -1.4472e-01 -9.0064e-03  7.4533e-02 -4.9713e-01\nLabutta                          7.1318e-01 -1.0169e-02  7.4484e-01  8.3814e-01\nLangkho                         -9.6769e-03 -8.6347e-03  2.0528e-01 -2.3004e-03\nLashio                           2.0219e-01 -4.2805e-03  4.8917e-02  9.3352e-01\nLoikaw                          -4.7652e-01 -2.3869e-02  3.2568e-01 -7.9318e-01\nLoilen                          -2.8804e-02 -5.6413e-03  7.8408e-02 -8.2718e-02\nMagway                           9.4077e-02 -5.9426e-03  4.9330e-02  4.5033e-01\nMandalay                        -2.2955e-01 -3.0598e-03  7.3153e-02 -8.3742e-01\nMatupi                          -1.7986e-02 -3.2117e-04  4.4878e-03 -2.6370e-01\nMaungdaw                         1.2575e-01 -2.6375e-03  6.3084e-02  5.1117e-01\nMawlaik                         -3.2557e-02 -2.6375e-03  3.0190e-02 -1.7220e-01\nMawlamyine                       2.4766e-01 -3.3072e-03  5.8440e-02  1.0381e+00\nMeiktila                         3.8976e-01 -8.2707e-03  7.9484e-02  1.4118e+00\nMinbu                           -1.9321e-01 -6.2516e-03  6.0203e-02 -7.6195e-01\nMindat                           2.2211e-02 -8.7518e-04  1.5503e-02  1.8541e-01\nMohnyin                          1.1002e-01 -8.8788e-04  1.5727e-02  8.8440e-01\nMongmit                         -4.9751e-01 -8.6347e-03  1.1965e-01 -1.4133e+00\nMonywa                           3.7540e+00 -3.3925e-02  5.8106e-01  4.9692e+00\nMrauk-U                          2.1574e-01 -3.9983e-03  4.5705e-02  1.0278e+00\nMuse                             2.6578e-01 -1.6313e-01  2.4204e+00  2.7569e-01\nMyawaddy                         1.8035e-02 -6.4391e-05  2.3492e-03  3.7341e-01\nMyeik                            3.6057e-01 -2.5739e-02  9.1496e-01  4.0386e-01\nMyingyan                         4.3732e-02 -1.0008e-04  1.3987e-03  1.1720e+00\nMyitkyina                       -1.9413e-01 -6.2855e-03  8.7306e-02 -6.3572e-01\nNaga Self-Administered Zone     -8.8212e-02 -1.0169e-02  3.6725e-01 -1.2878e-01\nNyaung-U                        -5.4115e-01 -8.6347e-03  1.5176e-01 -1.3669e+00\nOke Ta Ra                        5.7519e-01 -9.0064e-03  1.5824e-01  1.4686e+00\nPa-O Self-Administered Zone      1.9941e-01 -5.6413e-03  5.4359e-02  8.7950e-01\nPa Laung Self-Administered Zone -5.3309e-01 -5.0623e-03  7.0402e-02 -1.9901e+00\nPakokku                          1.9341e+00 -2.2766e-01  1.4683e+00  1.7840e+00\nPathein                          6.9218e-01 -9.3860e-03  3.3925e-01  1.2045e+00\nPuta-O                          -4.8052e-01 -6.8933e-03  5.0659e-01 -6.6543e-01\nPyapon                           6.8518e-01 -9.3860e-03  3.3925e-01  1.1925e+00\nPyay                             1.0545e-01 -1.2618e-03  1.7615e-02  8.0407e-01\nPyinoolwin                       4.6151e-01 -2.7025e-02  2.1958e-01  1.0426e+00\nSagaing                          9.5824e-01 -1.0212e-02  9.7948e-02  3.0944e+00\nShwebo                           2.1412e+00 -5.0075e-02  5.4593e-01  2.9657e+00\nSittwe                           2.3135e-01 -3.0598e-03  1.1130e-01  7.0266e-01\nTamu                             3.2174e-02 -1.8902e-04  3.3506e-03  5.5911e-01\nTaunggyi                        -1.0168e-01 -1.1395e-03  7.3697e-03 -1.1711e+00\nTaungoo                         -2.3005e-01 -4.8142e-03  4.6427e-02 -1.0453e+00\nThandwe                          5.6456e-01 -1.0169e-02  1.4069e-01  1.5322e+00\nThaton                          -6.7767e-02 -3.0835e-03  5.4499e-02 -2.7708e-01\nThayarwady                       9.0973e-03 -1.4836e-05  2.0737e-04  6.3277e-01\nThayet                           2.9530e-01 -5.3479e-03  5.1546e-02  1.3242e+00\nYamethin                         4.3920e-01 -9.7735e-03  1.3528e-01  1.2207e+00\nYangon (East)                    6.1100e-01 -7.5664e-03  1.8008e-01  1.4576e+00\nYangon (North)                   4.4954e-01 -9.3860e-03  1.0671e-01  1.4049e+00\nYangon (South)                   5.2023e-01 -8.6347e-03  1.1965e-01  1.5289e+00\nYangon (West)                    6.6585e-01 -9.7735e-03  2.3209e-01  1.4024e+00\nYinmarbin                        4.5788e+00 -9.9115e-02  1.2481e+00  4.1872e+00\n                                Pr.z....E.Ii..\nBago                                    0.6296\nBawlake                                 0.8694\nBhamo                                   0.0788\nDanu Self-Administered Zone             0.4369\nDawei                                   0.1782\nDet Khi Na                              0.3463\nFalam                                   0.9832\nGangaw                                  0.0142\nHakha                                   0.7315\nHinthada                                0.1817\nHkamti                                  0.7866\nHopang                                  0.7028\nHpa-An                                  0.3577\nHpapun                                  0.8768\nKale                                    0.1166\nKanbalu                                 0.0961\nKatha                                   0.9799\nKawkareik                               0.9401\nKawlin                                  0.6983\nKawthoung                               0.1748\nKokang Self-Administered Zone           0.0656\nKyaukme                                 0.8363\nKyaukpyu                                0.1629\nKyaukse                                 0.6191\nLabutta                                 0.4020\nLangkho                                 0.9982\nLashio                                  0.3506\nLoikaw                                  0.4277\nLoilen                                  0.9341\nMagway                                  0.6525\nMandalay                                0.4024\nMatupi                                  0.7920\nMaungdaw                                0.6092\nMawlaik                                 0.8633\nMawlamyine                              0.2992\nMeiktila                                0.1580\nMinbu                                   0.4461\nMindat                                  0.8529\nMohnyin                                 0.3765\nMongmit                                 0.1576\nMonywa                                  0.0000\nMrauk-U                                 0.3040\nMuse                                    0.7828\nMyawaddy                                0.7088\nMyeik                                   0.6863\nMyingyan                                0.2412\nMyitkyina                               0.5250\nNaga Self-Administered Zone             0.8975\nNyaung-U                                0.1716\nOke Ta Ra                               0.1419\nPa-O Self-Administered Zone             0.3791\nPa Laung Self-Administered Zone         0.0466\nPakokku                                 0.0744\nPathein                                 0.2284\nPuta-O                                  0.5058\nPyapon                                  0.2331\nPyay                                    0.4214\nPyinoolwin                              0.2972\nSagaing                                 0.0020\nShwebo                                  0.0030\nSittwe                                  0.4823\nTamu                                    0.5761\nTaunggyi                                0.2415\nTaungoo                                 0.2959\nThandwe                                 0.1255\nThaton                                  0.7817\nThayarwady                              0.5269\nThayet                                  0.1854\nYamethin                                0.2222\nYangon (East)                           0.1449\nYangon (North)                          0.1601\nYangon (South)                          0.1263\nYangon (West)                           0.1608\nYinmarbin                               0.0000"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-the-local-morans-i",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-the-local-morans-i",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Mapping the local Moran’s I",
    "text": "Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, we will need to append the local Moran’s I dataframe (i.e. localMI) onto the Battles_2023’s SF DataFrame.\n\nBattles_2023.localMI &lt;- cbind(Battles_2023,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\nBattles_2023.localMI\n\nSimple feature collection with 74 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   OBJECTID          ST ST_PCODE         DT   DT_PCODE    DT_MMR PCode_V year\n1         1  Ayeyarwady   MMR017   Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023\n2         2  Ayeyarwady   MMR017    Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023\n3         5  Ayeyarwady   MMR017    Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023\n4         6  Ayeyarwady   MMR017     Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023\n5         7 Bago (East)   MMR007       Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023\n6         8 Bago (East)   MMR007    Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023\n7         9 Bago (West)   MMR008       Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023\n8        10 Bago (West)   MMR008 Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023\n9        11        Chin   MMR004      Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023\n10       12        Chin   MMR004      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023\n   event_type Incidents Fatalities           Ii          E.Ii       Var.Ii\n1     Battles         4          3  0.462748867 -9.006432e-03 0.1247560902\n2     Battles         1          1  0.713178468 -1.016877e-02 0.7448368248\n3     Battles         3          1  0.692180375 -9.386041e-03 0.3392457987\n4     Battles         3          2  0.685181011 -9.386041e-03 0.3392457987\n5     Battles        50        270  0.006277460 -1.483579e-05 0.0001702656\n6     Battles        87        493 -0.230046740 -4.814215e-03 0.0464274459\n7     Battles        34         59  0.105454317 -1.261774e-03 0.0176145487\n8     Battles        50         93  0.009097304 -1.483579e-05 0.0002073682\n9     Battles        27         89 -0.007520292 -2.438086e-03 0.0583263538\n10    Battles        48        210 -0.011346560 -6.100301e-05 0.0010814666\n          Z.Ii     Pr.Ii                       geometry\n1   1.33562921 0.1816705 MULTIPOLYGON (((95.12637 18...\n2   0.83813940 0.4019524 MULTIPOLYGON (((95.04462 15...\n3   1.20451317 0.2283913 MULTIPOLYGON (((94.27572 15...\n4   1.19249602 0.2330668 MULTIPOLYGON (((95.20798 15...\n5   0.48222056 0.6296493 MULTIPOLYGON (((95.90674 18...\n6  -1.04530664 0.2958813 MULTIPOLYGON (((96.17964 19...\n7   0.80407054 0.4213562 MULTIPOLYGON (((95.70458 19...\n8   0.63277490 0.5268807 MULTIPOLYGON (((95.85173 18...\n9  -0.02104359 0.9832109 MULTIPOLYGON (((93.36931 24...\n10 -0.34317564 0.7314663 MULTIPOLYGON (((93.35213 23..."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-local-morans-i-values",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-local-morans-i-values",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Mapping local Moran’s I values",
    "text": "Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code below.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +  # Makes the fill semi-transparent\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-local-morans-i-p-values",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-local-morans-i-p-values",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Mapping local Moran’s I p-values",
    "text": "Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, we will also need to consider the p-values for each of these values.\nThe code below produces a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +  # Makes the fill semi-transparent\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-both-local-morans-i-values-and-p-values",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-both-local-morans-i-values-and-p-values",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Mapping both local Moran’s I values and p-values",
    "text": "Mapping both local Moran’s I values and p-values\nFor the Shiny App, it will be better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +  # Makes the fill semi-transparent\n  tm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +  # Makes the fill semi-transparent\n  \n  tm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#plotting-moran-scatterplot",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#plotting-moran-scatterplot",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Plotting Moran scatterplot",
    "text": "Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code below plots the Moran scatterplot of Battles in 2023 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(Battles_2023$Incidents, rswm_q,\n                  labels=as.character(Battles_2023$DT), \n                  xlab=\"Battles_2023\", \n                  ylab=\"Spatially Lagged Events,Year\")\n\n\n\n\nThe plot is split in 4 quadrants. The top right corner belongs to areas that have high incidents of events and are surrounded by other areas that have higher than the average level/number of battles This is the high-high locations.\n\n\n\n\n\n\nNote\n\n\n\nThe Moran scatterplot is divided into four areas, with each quadrant corresponding with one of four categories: (1) High-High (HH) in the top-right quadrant; (2) High-Low (HL) in the bottom right quadrant; (3) Low-High (LH) in the top-left quadrant; (4) Low- Low (LL) in the bottom left quadrant."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Plotting Moran scatterplot with standardised variable",
    "text": "Plotting Moran scatterplot with standardised variable\nFirst I will use scale() to centre and scale the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centred) variable by their standard deviations.\n\nBattles_2023$Z.Incidents &lt;- scale(Battles_2023$Incidents) %&gt;% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that maps neatly into our dataframe.\nNext, we plot the Moran scatterplot again by using the code below.\n\nnci2 &lt;- moran.plot(Battles_2023$Z.Incidents, rswm_q,\n                   labels=as.character(Battles_2023$DT),\n                   xlab=\"z-Battles in 2023\", \n                   ylab=\"Spatially Lag z-Battles in 2023\")\n\n\n\n\n\nMoran Scatterplots from 2020-2023 , Battles\n2020\n\n2021\n\n2022\n\n2023\n\nBoth types of scatter plots can be implemented for the Shiny app, with users being able to select between standardised or non-standardised variables.\n\n\n\n\n\n\nNote\n\n\n\nHigh-High (HH): indicates high spatial correlation where incidents of Battles are clustered closely together. 2) High-Low (HL): where areas of high frequency of incidents of Battles occurred are located next to areas where there is low frequency of incidents of Battles occurred. 3) Low-High (LH): these are areas of low frequency of incidents where Battles occurred that are located next to areas where high frequency of Battles. 4) Low-Low (LL): these are clusters of low frequency of incidents of Battles occurred."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#preparing-lisa-map-classes",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#preparing-lisa-map-classes",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Preparing LISA map classes",
    "text": "Preparing LISA map classes\nThe Moran Scatterplot has one drawback - it does not indicate whether these regions are significant or not. We can address this limitation by using the LISA method. LISA will not only allow us to identify the hotspot locations, but also the statistical significance of the hot spots in the map.\nAccording to Anselin (1995), LISA can be used to locate “hot spots” or local spatial clusters where the occurrence of Event types is statistically significant.\nIn addition to the four categories described in the Moran Scatterplot, the LISA analysis includes an additional category: (5) Insignificant: where there are no spatial autocorrelation or clusters where event types have occurred.\nThe code below shows the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we derive the spatially lagged variable of interest (i.e. Incidents) and centers the spatially lagged variable around its mean.\n\nBattles_2023$lag_Incidents &lt;- lag.listw(rswm_q, Battles_2023$Incidents)\nDV &lt;- Battles_2023$lag_Incidents - mean(Battles_2023$lag_Incidents)     \n\nThis is followed by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, we place non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nWe can also combine all the steps above into one single code chunk as shown below:\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nBattles_2023$lag_Incidents &lt;- lag.listw(rswm_q, Battles_2023$Incidents)\nDV &lt;- Battles_2023$lag_Incidents - mean(Battles_2023$lag_Incidents)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#plotting-lisa-map",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#plotting-lisa-map",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Plotting LISA Map",
    "text": "Plotting LISA Map\nNow, we can build the LISA map by using the code below.\n\nBattles_2023.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"grey\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +  # Makes the fill semi-transparent\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#getis-and-ords-g-statistics",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#getis-and-ords-g-statistics",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Getis and Ord’s G-Statistics",
    "text": "Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995).\nIt looks at neighbours within a defined proximity to identify where either high or low values cluster spatially.\nHere, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\nDeriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\nDeriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph.\nWe need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length.\nOur input vector will be the geometry column of Battles_2023 dataset. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid() function over the geometry column of our Battles_2023 dataset and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[1]])\n\n\nclass(longitude)\n\n[1] \"numeric\"\n\n\n\nlongitude\n\n [1] 95.19035 94.99369 94.74008 95.53705 96.58767 96.34709 95.30186 95.75119\n [9] 93.71488 93.56355 93.22544 93.81953 97.16218 96.49805 97.41891 97.78205\n[17] 97.42880 97.33311 97.45091 97.36866 98.22657 98.51991 94.18202 95.31595\n[25] 94.45732 94.73063 95.05170 96.22693 92.48276 93.35447 92.90601 94.50024\n[33] 95.47237 94.40944 95.49546 96.05576 95.53962 94.74604 95.26376 95.66196\n[41] 95.64449 95.40740 94.38739 94.71565 98.96838 98.65407 97.13646 98.19561\n[49] 96.66042 98.01229 97.21860 96.52970 98.02872 97.92760 97.03704 96.92822\n[57] 98.47076 98.77515 96.16459 95.97341 95.54920 95.15564 96.25009 96.06338\n[65] 97.84218 97.26005 96.16788 96.13921 93.92534 98.93770 96.24776 96.03014\n[73] 96.28511 96.13431\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[2]])\n\n\nclass(latitude)\n\n[1] \"numeric\"\n\n\n\nlatitude\n\n [1] 17.87515 16.16236 16.86214 16.17363 17.74287 18.81260 18.76076 18.10024\n [9] 23.42199 22.51244 21.57539 21.26333 24.25814 25.30215 26.01108 27.27547\n[17] 18.91417 19.48546 17.75720 18.07659 16.00305 16.54942 21.80335 20.26548\n[25] 20.38303 21.43135 19.43717 21.61640 21.02760 20.47696 20.40055 18.53159\n[33] 25.34589 23.07978 23.32423 24.25502 23.93661 23.96919 22.25730 26.41610\n[41] 21.98775 22.74644 24.17969 22.24865 23.02589 23.83364 22.50271 22.76825\n[49] 23.45728 23.68574 23.21080 21.22388 20.26507 21.39398 20.46234 20.85826\n[57] 14.08664 10.98880 21.96431 20.95960 21.47201 20.94652 22.62753 20.49193\n[65] 15.79499 17.13186 19.63929 20.04539 19.67628 12.36708 16.89759 17.26904\n[73] 16.66221 16.82926\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\nclass(coords)\n\n[1] \"matrix\" \"array\" \n\n\n\ncoords\n\n      longitude latitude\n [1,]  95.19035 17.87515\n [2,]  94.99369 16.16236\n [3,]  94.74008 16.86214\n [4,]  95.53705 16.17363\n [5,]  96.58767 17.74287\n [6,]  96.34709 18.81260\n [7,]  95.30186 18.76076\n [8,]  95.75119 18.10024\n [9,]  93.71488 23.42199\n[10,]  93.56355 22.51244\n[11,]  93.22544 21.57539\n[12,]  93.81953 21.26333\n[13,]  97.16218 24.25814\n[14,]  96.49805 25.30215\n[15,]  97.41891 26.01108\n[16,]  97.78205 27.27547\n[17,]  97.42880 18.91417\n[18,]  97.33311 19.48546\n[19,]  97.45091 17.75720\n[20,]  97.36866 18.07659\n[21,]  98.22657 16.00305\n[22,]  98.51991 16.54942\n[23,]  94.18202 21.80335\n[24,]  95.31595 20.26548\n[25,]  94.45732 20.38303\n[26,]  94.73063 21.43135\n[27,]  95.05170 19.43717\n[28,]  96.22693 21.61640\n[29,]  92.48276 21.02760\n[30,]  93.35447 20.47696\n[31,]  92.90601 20.40055\n[32,]  94.50024 18.53159\n[33,]  95.47237 25.34589\n[34,]  94.40944 23.07978\n[35,]  95.49546 23.32423\n[36,]  96.05576 24.25502\n[37,]  95.53962 23.93661\n[38,]  94.74604 23.96919\n[39,]  95.26376 22.25730\n[40,]  95.66196 26.41610\n[41,]  95.64449 21.98775\n[42,]  95.40740 22.74644\n[43,]  94.38739 24.17969\n[44,]  94.71565 22.24865\n[45,]  98.96838 23.02589\n[46,]  98.65407 23.83364\n[47,]  97.13646 22.50271\n[48,]  98.19561 22.76825\n[49,]  96.66042 23.45728\n[50,]  98.01229 23.68574\n[51,]  97.21860 23.21080\n[52,]  96.52970 21.22388\n[53,]  98.02872 20.26507\n[54,]  97.92760 21.39398\n[55,]  97.03704 20.46234\n[56,]  96.92822 20.85826\n[57,]  98.47076 14.08664\n[58,]  98.77515 10.98880\n[59,]  96.16459 21.96431\n[60,]  95.97341 20.95960\n[61,]  95.54920 21.47201\n[62,]  95.15564 20.94652\n[63,]  96.25009 22.62753\n[64,]  96.06338 20.49193\n[65,]  97.84218 15.79499\n[66,]  97.26005 17.13186\n[67,]  96.16788 19.63929\n[68,]  96.13921 20.04539\n[69,]  93.92534 19.67628\n[70,]  98.93770 12.36708\n[71,]  96.24776 16.89759\n[72,]  96.03014 17.26904\n[73,]  96.28511 16.66221\n[74,]  96.13431 16.82926\n\n\n\n\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\n\n\nclass(k1)\n\n[1] \"nb\"\n\n\n\nk1\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 74 \nPercentage nonzero weights: 1.351351 \nAverage number of links: 1 \n22 disjoint connected subgraphs\nNon-symmetric neighbours list\n\n\n\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\n\n\nclass(k1dists)\n\n[1] \"numeric\"\n\n\n\nk1dists\n\n [1]  64.41257  58.12290  82.03640  58.12290  79.09076  93.42950  79.36533\n [8]  64.41257  80.54794 101.91538  70.62347  70.60677 102.37138 103.38338\n[15] 121.31189 144.68282  64.03431  64.03431  36.40798  36.40798  47.16407\n[22]  68.10781  70.60677  77.22939  90.60751  69.49147  79.36533  39.05961\n[29]  82.23837  47.55720  47.55720  88.30259 103.38338  80.54794  64.61998\n[36]  63.22788  63.22788  43.28748  49.33444 120.07732  49.33444  56.14593\n[43]  43.28748  56.50570  84.26339  67.44212  78.86753  84.26339  63.27412\n[50]  67.44212  63.27412  53.61076 105.81463 125.42864  45.27628  45.27628\n[57] 196.85312 153.49063  39.05961  52.61862  57.95152  69.49147  73.96718\n[64]  50.06522  47.16407  72.11747  45.05727  45.05727  96.01699 153.49063\n[71]  14.25876  47.18525  24.50179  14.25876\n\n\n\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  14.26   49.33   66.03   71.79   82.19  196.85 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 196.85 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\nwm_d197 &lt;- dnearneigh(coords, 0, 197, longlat = TRUE)\nwm_d197\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm197_lw &lt;- nb2listw(wm_d197, style = 'B')\nsummary(wm197_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 3  1  2  3  3  5  3  3  7  3  4  4  6  5  5  4  1  5  4  3 \n3 least connected regions:\n16 57 58 with 1 link\n3 most connected regions:\n39 42 62 with 20 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 828 1656 45160\n\n\n\nclass(wm197_lw)\n\n[1] \"listw\" \"nb\"   \n\n\nThe output spatial weights object is called wm197_lw.\n\n\nComputing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours.\nHaving many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\n\nclass(knn)\n\n[1] \"nb\"\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n74 \n74 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n74 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 592 1036 19636\n\n\n\nclass(knn_lw)\n\n[1] \"listw\" \"nb\""
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#computing-gi-statistics",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#computing-gi-statistics",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Computing Gi statistics",
    "text": "Computing Gi statistics\n\nGi statistics using fixed distance\n\nfips &lt;- order(Battles_2023$DT)\nfips\n\n [1]  5 17 13 52 57 67  9 23 10  1 33 45 19 20 34 35 36 21 37 58 46 47 69 28  2\n[26] 53 48 18 54 24 59 11 29 38 65 60 25 12 14 49 39 30 50 22 70 61 15 40 62 68\n[51] 55 51 26  3 16  4  7 63 41 42 31 43 56  6 32 66  8 27 64 71 72 73 74 44\n\n\n\nclass(fips)\n\n[1] \"integer\"\n\n\n\ngi.fixed &lt;- localG(Battles_2023$Incidents, wm197_lw)\ngi.fixed\n\n [1] -2.16251076 -2.26945742 -2.36608309 -2.28060756 -1.40482297 -1.73401341\n [7] -1.78091543 -2.09517182  1.58404920  3.74739192  1.77357701  1.43327980\n[13]  1.76425408  0.23868264 -0.46980525  0.66543351 -0.74168975 -1.33781214\n[19] -0.51148627 -0.52707475  0.62973486  0.75874852  2.45091315 -0.79794591\n[25] -0.46714759  0.78360503 -2.26619716  1.90728203 -0.19741884  0.53880930\n[31] -1.05349256 -1.42802753 -0.14931467  3.57984196  2.13682831  0.42422030\n[37]  1.15690841  2.06391206  2.02017424  0.24781966  2.12585948  2.66825096\n[43]  0.59177302  2.14376526  1.90967745  1.33161281  1.56642737  0.66429027\n[49]  3.18467135 -0.08013071  2.14013808  0.26220228 -0.29653380 -0.70655951\n[55] -1.52151868 -1.55549239  1.38510282  1.35703308  1.80376535  0.80349295\n[61]  1.58567210  0.37028741  1.42615140 -0.19363169  0.39364611 -0.86461442\n[67] -1.28913769 -1.59126895 -1.85739540  0.40386411 -2.04421053 -1.82676914\n[73] -1.82291155 -2.01994548\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)       Z(Gi) Pr(z != E(Gi))\n [1,] 0.068229167 0.17808219 0.0025805215 -2.16251076   0.0305788284\n [2,] 0.007285974 0.09589041 0.0015242874 -2.26945742   0.0232405238\n [3,] 0.020046863 0.12328767 0.0019038942 -2.36608309   0.0179774092\n [4,] 0.006769071 0.09589041 0.0015270818 -2.28060756   0.0225716798\n [5,] 0.094095941 0.16438356 0.0025033091 -1.40482297   0.1600739255\n [6,] 0.110194304 0.20547945 0.0030195729 -1.73401341   0.0829157041\n [7,] 0.065091864 0.15068493 0.0023098862 -1.78091543   0.0749262673\n [8,] 0.091196626 0.20547945 0.0029752444 -2.09517182   0.0361557216\n [9,] 0.193083573 0.12328767 0.0019414335  1.58404920   0.1131825243\n[10,] 0.289515279 0.12328767 0.0019676510  3.74739192   0.0001786828\n[11,] 0.202220460 0.12328767 0.0019806822  1.77357701   0.0761331435\n[12,] 0.267664828 0.19178082 0.0028030997  1.43327980   0.1517778927\n[13,] 0.219305224 0.13698630 0.0021770936  1.76425408   0.0776892110\n[14,] 0.091077575 0.08219178 0.0013859604  0.23868264   0.8113516776\n[15,] 0.040245203 0.05479452 0.0009590683 -0.46980525   0.6384941605\n[16,] 0.023995827 0.01369863 0.0002394576  0.66543351   0.5057732553\n[17,] 0.090454904 0.12328767 0.0019596135 -0.74168975   0.4582753304\n[18,] 0.074313409 0.13698630 0.0021946699 -1.33781214   0.1809576830\n[19,] 0.139005236 0.16438356 0.0024618296 -0.51148627   0.6090105985\n[20,] 0.125490196 0.15068493 0.0022849420 -0.52707475   0.5981416802\n[21,] 0.058130190 0.04109589 0.0007317001  0.62973486   0.5288680718\n[22,] 0.078141499 0.05479452 0.0009468162  0.75874852   0.4480030085\n[23,] 0.340266667 0.20547945 0.0030244162  2.45091315   0.0142494332\n[24,] 0.200730880 0.24657534 0.0033008582 -0.79794591   0.4249018788\n[25,] 0.167275574 0.19178082 0.0027517563 -0.46714759   0.6403942853\n[26,] 0.303858068 0.26027397 0.0030935821  0.78360503   0.4332719050\n[27,] 0.062418386 0.17808219 0.0026049511 -2.26619716   0.0234393145\n[28,] 0.355729167 0.24657534 0.0032752773  1.90728203   0.0564840763\n[29,] 0.061812467 0.06849315 0.0011451559 -0.19741884   0.8434997910\n[30,] 0.146966527 0.12328767 0.0019313068  0.53880930   0.5900184491\n[31,] 0.043455497 0.08219178 0.0013519884 -1.05349256   0.2921153017\n[32,] 0.030184751 0.08219178 0.0013263280 -1.42802753   0.1532839350\n[33,] 0.076701571 0.08219178 0.0013519884 -0.14931467   0.8813053367\n[34,] 0.363684489 0.17808219 0.0026880602  3.57984196   0.0003438021\n[35,] 0.322002635 0.20547945 0.0029736197  2.13682831   0.0326119589\n[36,] 0.171367177 0.15068493 0.0023769086  0.42422030   0.6714051589\n[37,] 0.252951981 0.19178082 0.0027957315  1.15690841   0.2473097820\n[38,] 0.232058669 0.13698630 0.0021219065  2.06391206   0.0390260550\n[39,] 0.396593674 0.27397260 0.0036842794  2.02017424   0.0433653170\n[40,] 0.047619048 0.04109589 0.0006928579  0.24781966   0.8042739429\n[41,] 0.387329591 0.26027397 0.0035720591  2.12585948   0.0335149615\n[42,] 0.435444414 0.27397260 0.0036621834  2.66825096   0.0076247282\n[43,] 0.134509081 0.10958904 0.0017733202  0.59177302   0.5540025915\n[44,] 0.370217451 0.24657534 0.0033264297  2.14376526   0.0320517005\n[45,] 0.132483082 0.06849315 0.0011228022  1.90967745   0.0561747560\n[46,] 0.113924051 0.06849315 0.0011639833  1.33161281   0.1829874538\n[47,] 0.307425214 0.21917808 0.0031738082  1.56642737   0.1172486006\n[48,] 0.137802607 0.10958904 0.0018038490  0.66429027   0.5065045498\n[49,] 0.339932274 0.17808219 0.0025828347  3.18467135   0.0014491849\n[50,] 0.092809365 0.09589041 0.0014784223 -0.08013071   0.9361332989\n[51,] 0.287356322 0.17808219 0.0026070606  2.14013808   0.0323436091\n[52,] 0.261594581 0.24657534 0.0032811258  0.26220228   0.7931654986\n[53,] 0.071372753 0.08219178 0.0013311533 -0.29653380   0.7668224621\n[54,] 0.080156658 0.10958904 0.0017352153 -0.70655951   0.4798402603\n[55,] 0.123498695 0.20547945 0.0029031487 -1.52151868   0.1281297272\n[56,] 0.131920530 0.21917808 0.0031468082 -1.55549239   0.1198288467\n[57,] 0.035637728 0.01369863 0.0002508843  1.38510282   0.1660210309\n[58,] 0.034807642 0.01369863 0.0002419663  1.35703308   0.1747706992\n[59,] 0.366230366 0.26027397 0.0034505972  1.80376535   0.0712681006\n[60,] 0.292600313 0.24657534 0.0032811258  0.80349295   0.4216898674\n[61,] 0.354370214 0.26027397 0.0035214197  1.58567210   0.1128137120\n[62,] 0.295910393 0.27397260 0.0035100061  0.37028741   0.7111683500\n[63,] 0.299541655 0.21917808 0.0031753179  1.42615140   0.1538246447\n[64,] 0.222019781 0.23287671 0.0031438461 -0.19363169   0.8464642818\n[65,] 0.066967845 0.05479452 0.0009563271  0.39364611   0.6938423348\n[66,] 0.108660999 0.15068493 0.0023623728 -0.86461442   0.3872504579\n[67,] 0.124184712 0.19178082 0.0027494435 -1.28913769   0.1973502243\n[68,] 0.131770833 0.21917808 0.0030172252 -1.59126895   0.1115490605\n[69,] 0.041992697 0.12328767 0.0019156610 -1.85739540   0.0632549198\n[70,] 0.036378335 0.02739726 0.0004945225  0.40386411   0.6863126506\n[71,] 0.063607925 0.16438356 0.0024302999 -2.04421053   0.0409327537\n[72,] 0.096329081 0.19178082 0.0027302372 -1.82676914   0.0677344872\n[73,] 0.085438916 0.17808219 0.0025828347 -1.82291155   0.0683167878\n[74,] 0.065070276 0.16438356 0.0024173270 -2.01994548   0.0433890436\nattr(,\"cluster\")\n [1] Low  Low  Low  Low  Low  High Low  Low  Low  Low  High Low  High High High\n[16] Low  Low  High Low  Low  High High High Low  Low  High Low  Low  Low  Low \n[31] Low  Low  Low  High Low  High Low  Low  High Low  High High Low  High Low \n[46] High High High Low  High Low  Low  Low  Low  Low  High High Low  Low  Low \n[61] High Low  High Low  High High Low  Low  Low  High Low  Low  Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = Battles_2023$Incidents, listw = wm197_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\n\nclass(gi.fixed)\n\n[1] \"localG\"\n\n\nNext, we will join the Gi values to their corresponding sf data frame by using the code chunk below.\n\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nclass(Battles_2023.gi)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\nBattles_2023.gi\n\nSimple feature collection with 74 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   OBJECTID          ST ST_PCODE         DT   DT_PCODE    DT_MMR PCode_V year\n1         1  Ayeyarwady   MMR017   Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023\n2         2  Ayeyarwady   MMR017    Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023\n3         5  Ayeyarwady   MMR017    Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023\n4         6  Ayeyarwady   MMR017     Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023\n5         7 Bago (East)   MMR007       Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023\n6         8 Bago (East)   MMR007    Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023\n7         9 Bago (West)   MMR008       Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023\n8        10 Bago (West)   MMR008 Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023\n9        11        Chin   MMR004      Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023\n10       12        Chin   MMR004      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023\n   event_type Incidents Fatalities Z.Incidents lag_Incidents gstat_fixed\n1     Battles         4          3 -0.80534765      18.20000   -2.162511\n2     Battles         1          1 -0.85573862       3.00000   -2.269457\n3     Battles         3          1 -0.82214464       2.50000   -2.366083\n4     Battles         3          2 -0.82214464       3.00000   -2.280608\n5     Battles        50        270 -0.03268604      40.66667   -1.404823\n6     Battles        87        493  0.58880265      29.00000   -1.734013\n7     Battles        34         59 -0.30143790      31.40000   -1.780915\n8     Battles        50         93 -0.03268604      35.60000   -2.095172\n9     Battles        27         89 -0.41901684      53.00000    1.584049\n10    Battles        48        210 -0.06628002      62.00000    3.747392\n                         geometry\n1  MULTIPOLYGON (((95.12637 18...\n2  MULTIPOLYGON (((95.04462 15...\n3  MULTIPOLYGON (((94.27572 15...\n4  MULTIPOLYGON (((95.20798 15...\n5  MULTIPOLYGON (((95.90674 18...\n6  MULTIPOLYGON (((96.17964 19...\n7  MULTIPOLYGON (((95.70458 19...\n8  MULTIPOLYGON (((95.85173 18...\n9  MULTIPOLYGON (((93.36931 24...\n10 MULTIPOLYGON (((93.35213 23...\n\n\nThe code above performs three tasks. First, it converts the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join Battles_2023 and gi.fixed matrix to produce a new SpatialPolygonDataFrame called Battles_2023.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename()."
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-gi-values-with-fixed-distance-weights",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-gi-values-with-fixed-distance-weights",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Mapping Gi values with fixed distance weights",
    "text": "Mapping Gi values with fixed distance weights\nThe code below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\nGimap &lt;-tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +  # Makes the fill semi-transparent\n  \n  tm_shape(Battles_2023.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"Fixed Distance\\nlocal Gi\") +\n  tm_borders(alpha = 0.5)\n\nGimap\n\n\n\n\n\nGi statistics using adaptive distance\nThe code below is used to compute the Gi values for Incidents of Battles in 2023 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(Battles_2023$DT)\ngi.adaptive &lt;- localG(Battles_2023$Incidents, knn_lw)\n\n\ngi.adaptive\n\n [1] -1.69409549 -2.15575982 -2.16609113 -2.16609113 -0.89241976 -0.45565261\n [7] -1.45431059 -1.41744307  1.78786329  3.23771850  2.11557210  2.16224207\n[13]  1.48096455 -0.24335336  1.20898711  1.66729058 -0.47992492 -0.99329818\n[19] -0.78642145  0.28700849 -0.15164441 -0.23203660  2.85401683 -1.92978624\n[25]  0.42042420  1.95472277 -1.55181501 -0.11817471 -0.70352285  0.86383248\n[31]  0.84113052 -1.29509100 -0.02272399  1.88812927  1.75077945 -0.72558630\n[37]  0.52136202  0.69837889  4.01655295 -0.01979340  3.92038427  2.32672440\n[43]  0.59177302  3.77235135  1.13032190  1.33403907 -0.76161463  0.66429027\n[49]  2.27422154  0.41536707  2.55339763 -0.88873272 -0.82680985 -0.70655951\n[55] -1.17038676 -2.24322777  0.55403406  1.01583290  1.26679795 -1.63560522\n[61]  1.06875739  0.43494584  1.19463589 -1.84059117 -0.09004962 -1.53676005\n[67] -0.75911771 -1.78827402 -1.65680880  0.40725952 -1.40151113 -1.37483989\n[73] -1.40432987 -1.38835916\nattr(,\"internals\")\n              Gi    E(Gi)       V(Gi)       Z(Gi) Pr(z != E(Gi))\n [1,] 0.03932292 0.109589 0.001720348 -1.69409549   9.024714e-02\n [2,] 0.02029664 0.109589 0.001715648 -2.15575982   3.110242e-02\n [3,] 0.01978651 0.109589 0.001718793 -2.16609113   3.030422e-02\n [4,] 0.01978651 0.109589 0.001718793 -2.16609113   3.030422e-02\n [5,] 0.07195572 0.109589 0.001778307 -0.89241976   3.721680e-01\n [6,] 0.09023157 0.109589 0.001804802 -0.45565261   6.486398e-01\n [7,] 0.04855643 0.109589 0.001761204 -1.45431059   1.458602e-01\n [8,] 0.04981550 0.109589 0.001778307 -1.41744307   1.563534e-01\n [9,] 0.18443804 0.109589 0.001752683  1.78786329   7.379807e-02\n[10,] 0.24604847 0.109589 0.001776352  3.23771850   1.204896e-03\n[11,] 0.19904837 0.109589 0.001788116  2.11557210   3.438122e-02\n[12,] 0.20042028 0.109589 0.001764663  2.16224207   3.059952e-02\n[13,] 0.17236807 0.109589 0.001796966  1.48096455   1.386160e-01\n[14,] 0.09928515 0.109589 0.001792785 -0.24335336   8.077317e-01\n[15,] 0.16098081 0.109589 0.001806940  1.20898711   2.266678e-01\n[16,] 0.17892540 0.109589 0.001729416  1.66729058   9.545664e-02\n[17,] 0.08940310 0.109589 0.001769096 -0.47992492   6.312808e-01\n[18,] 0.06731287 0.109589 0.001811474 -0.99329818   3.205647e-01\n[19,] 0.07670157 0.109589 0.001748841 -0.78642145   4.316206e-01\n[20,] 0.12156863 0.109589 0.001742185  0.28700849   7.741058e-01\n[21,] 0.10313421 0.109589 0.001811829 -0.15164441   8.794674e-01\n[22,] 0.09978881 0.109589 0.001783857 -0.23203660   8.165096e-01\n[23,] 0.23093333 0.109589 0.001807697  2.85401683   4.317024e-03\n[24,] 0.02923519 0.109589 0.001733784 -1.92978624   5.363333e-02\n[25,] 0.12708768 0.109589 0.001732341  0.42042420   6.741756e-01\n[26,] 0.18698958 0.109589 0.001567897  1.95472277   5.061580e-02\n[27,] 0.04492034 0.109589 0.001736634 -1.55181501   1.207065e-01\n[28,] 0.10468750 0.109589 0.001720348 -0.11817471   9.059292e-01\n[29,] 0.08014667 0.109589 0.001751415 -0.70352285   4.817300e-01\n[30,] 0.14565900 0.109589 0.001743541  0.86383248   3.876799e-01\n[31,] 0.14476440 0.109589 0.001748841  0.84113052   4.002748e-01\n[32,] 0.05594588 0.109589 0.001715648 -1.29509100   1.952888e-01\n[33,] 0.10863874 0.109589 0.001748841 -0.02272399   9.818704e-01\n[34,] 0.18951826 0.109589 0.001792040  1.88812927   5.900860e-02\n[35,] 0.18339921 0.109589 0.001777336  1.75077945   7.998391e-02\n[36,] 0.07869997 0.109589 0.001812306 -0.72558630   4.680924e-01\n[37,] 0.13146156 0.109589 0.001760025  0.52136202   6.021146e-01\n[38,] 0.13881613 0.109589 0.001751415  0.69837889   4.849403e-01\n[39,] 0.28034604 0.109589 0.001807382  4.01655295   5.905560e-05\n[40,] 0.10876919 0.109589 0.001715648 -0.01979340   9.842082e-01\n[41,] 0.27639669 0.109589 0.001810400  3.92038427   8.840788e-05\n[42,] 0.20820875 0.109589 0.001796543  2.32672440   1.997994e-02\n[43,] 0.13450908 0.109589 0.001773320  0.59177302   5.540026e-01\n[44,] 0.26727223 0.109589 0.001747216  3.77235135   1.617163e-04\n[45,] 0.15642894 0.109589 0.001717227  1.13032190   2.583406e-01\n[46,] 0.16587553 0.109589 0.001780210  1.33403907   1.821911e-01\n[47,] 0.07719017 0.109589 0.001809627 -0.76161463   4.462900e-01\n[48,] 0.13780261 0.109589 0.001803849  0.66429027   5.065045e-01\n[49,] 0.20395936 0.109589 0.001721890  2.27422154   2.295267e-02\n[50,] 0.12653289 0.109589 0.001664025  0.41536707   6.778732e-01\n[51,] 0.21603971 0.109589 0.001738040  2.55339763   1.066776e-02\n[52,] 0.07269411 0.109589 0.001723420 -0.88873272   3.741467e-01\n[53,] 0.07528002 0.109589 0.001721890 -0.82680985   4.083448e-01\n[54,] 0.08015666 0.109589 0.001735215 -0.70655951   4.798403e-01\n[55,] 0.06083551 0.109589 0.001735215 -1.17038676   2.418454e-01\n[56,] 0.01456954 0.109589 0.001794233 -2.24322777   2.488213e-02\n[57,] 0.13317256 0.109589 0.001811942  0.55403406   5.795555e-01\n[58,] 0.15205444 0.109589 0.001747535  1.01583290   3.097090e-01\n[59,] 0.16256545 0.109589 0.001748841  1.26679795   2.052275e-01\n[60,] 0.04168838 0.109589 0.001723420 -1.63560522   1.019222e-01\n[61,] 0.15473990 0.109589 0.001784735  1.06875739   2.851790e-01\n[62,] 0.12763741 0.109589 0.001721890  0.43494584   6.636017e-01\n[63,] 0.16042060 0.109589 0.001810488  1.19463589   2.322293e-01\n[64,] 0.03331598 0.109589 0.001717227 -1.84059117   6.568149e-02\n[65,] 0.10576668 0.109589 0.001801776 -0.09004962   9.282478e-01\n[66,] 0.04436769 0.109589 0.001801223 -1.53676005   1.243521e-01\n[67,] 0.07800678 0.109589 0.001730885 -0.75911771   4.477821e-01\n[68,] 0.03541667 0.109589 0.001720348 -1.78827402   7.373181e-02\n[69,] 0.04068858 0.109589 0.001729416 -1.65680880   9.755814e-02\n[70,] 0.12691997 0.109589 0.001810928  0.40725952   6.838174e-01\n[71,] 0.05135558 0.109589 0.001726443 -1.40151113   1.610613e-01\n[72,] 0.05259047 0.109589 0.001718793 -1.37483989   1.691811e-01\n[73,] 0.05131545 0.109589 0.001721890 -1.40432987   1.602206e-01\n[74,] 0.05205622 0.109589 0.001717227 -1.38835916   1.650277e-01\nattr(,\"cluster\")\n [1] Low  Low  Low  Low  Low  High Low  Low  Low  Low  High Low  High High High\n[16] Low  Low  High Low  Low  High High High Low  Low  High Low  Low  Low  Low \n[31] Low  Low  Low  High Low  High Low  Low  High Low  High High Low  High Low \n[46] High High High Low  High Low  Low  Low  Low  Low  High High Low  Low  Low \n[61] High Low  High Low  High High Low  Low  Low  High Low  Low  Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = Battles_2023$Incidents, listw = knn_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\n\nclass(gi.adaptive)\n\n[1] \"localG\"\n\n\n\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\ndatatable(Battles_2023.gi)"
  },
  {
    "objectID": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-gi-values-with-adaptive-distance-weights",
    "href": "Take-home-ex/Take-home-Ex4/Take-home_Ex4.html#mapping-gi-values-with-adaptive-distance-weights",
    "title": "Take-Home Exercise 4 - Detailed version (not for submission)",
    "section": "Mapping Gi values with adaptive distance weights",
    "text": "Mapping Gi values with adaptive distance weights\nNow we visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\nGimap &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +  # Makes the fill semi-transparent\n  \n  tm_shape(Battles_2023.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"Adaptive Distance\\nlocal Gi\") + \n  tm_borders(alpha = 0.5)\n\nGimap\n\n\n\n\n\nComparing Fixed distance and Adaptive Distance\n\n\nThere is a few subtle differences between the Fixed and Adaptive distances."
  }
]