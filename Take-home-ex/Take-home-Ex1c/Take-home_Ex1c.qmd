---
title: "Take-home Exercise 4c"
date: March 17, 2024
date-modified: "last-modified"
author: "Imran Ibrahim"
toc: true
execute: 
  eval: true
  echo: true
  freeze: true
  warning: false
  message: false
---

# Emerging Hot Spot Analysis: sfdep methods

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:

-   Building a space-time cube,

-   Calculating Getis-Ord local Gi\* statistic for each bin by using an FDR correction,

-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test,

-   Categorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.

# Data Loading and Prep

```{r}
pacman::p_load(tidyverse, dplyr , 
               sf, lubridate,plotly,
               tmap, spdep, sfdep)
```

```{r}
ACLED_MMR <- read_csv("data/MMR.csv")
```

Shapes files for Myanmar at admin1 and admin2 levels

```{r}
mmr_shp_mimu_2 <-  st_read(dsn = "data/geospatial3",  
                  layer = "mmr_polbnda_adm2_250k_mimu")
```

```{r}
mmr_shp_mimu_1 <-  st_read(dsn = "data/geospatial3",  
                  layer = "mmr_polbnda2_adm1_250k_mimu_1")
```

Renaming the region and district names so that our dataset will be the same as the shape files

```{r}
ACLED_MMR_1 <- ACLED_MMR %>%
  mutate(admin1 = case_when(
    admin1 == "Bago-East" ~ "Bago (East)",
    admin1 == "Bago-West" ~ "Bago (West)",
    admin1 == "Shan-North" ~ "Shan (North)",
    admin1 == "Shan-South" ~ "Shan (South)",
    admin1 == "Shan-East" ~ "Shan (East)",
    TRUE ~ as.character(admin1)
  ))
```

```{r}
ACLED_MMR_1 <- ACLED_MMR_1 %>%
  mutate(admin2 = case_when(
    admin2 == "Yangon-East" ~ "Yangon (East)",
    admin2 == "Yangon-West" ~ "Yangon (West)",
    admin2 == "Yangon-North" ~ "Yangon (North)",
    admin2 == "Yangon-South" ~ "Yangon (South)",
    admin2 == "Mong Pawk (Wa SAD)" ~ "Tachileik",
    admin2 == "Nay Pyi Taw" ~ "Det Khi Na",
    admin2 == "Yangon" ~ "Yangon (West)",
    TRUE ~ as.character(admin2)
  ))
```

# Creating a Time Series Cube

In the code chunk below, [`spacetime()`](https://sfdep.josiahparry.com/reference/spacetime.html) of sfdep is used to create an spatio-temporal cube.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#hunan <- st_read(dsn = "data/geospatial4", 
                 #layer = "Hunan")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#class(GDPPC)
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#class(hunan)
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#GDPPC_st <- spacetime(GDPPC, hunan,
                      #.loc_col = "County",
                      #.time_col = "Year")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#is_spacetime_cube(GDPPC_st)
```

### Filtering for Battles for admin 1 & 2 and renaming the column names

loc_col identifier needs to be the same name for both data and shape file

Since 2024 data is not for the full year, we will remove it. We will the total up the count of Incidents for event type== Battles.

```{r}
Battles_admin1 <- ACLED_MMR_1 %>%
    group_by(year, admin1) %>%
    filter(year != 2024, event_type == "Battles") %>%
    summarise(Incidents = n()) %>%
    ungroup() %>%
    rename(ST = admin1)

```

```{r}
Battles_admin2 <- ACLED_MMR_1 %>%
    group_by(year, admin2) %>%
    filter(year != 2024, event_type == "Battles") %>%
    summarise(Incidents = n()) %>%
    ungroup() %>%
    rename(DT = admin2)
```

```{r}
battles_spacial1 <- spacetime(Battles_admin1, mmr_shp_mimu_1,
                      .loc_col = "ST",
                      .time_col = "year")
```

```{r}
is_spacetime_cube(battles_spacial1)
```

```{r}
battles_spacial2 <- spacetime(Battles_admin2, mmr_shp_mimu_2,
                      .loc_col = "DT",
                      .time_col = "year")
```

```{r}
is_spacetime_cube(battles_spacial2)
```

This is due to some years having zero incidents, this needs to be populated as zero, all state/region and district names have to be accounted for, and for each year

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#Battles_admin1_2124 <- read_csv("data/Battles_admin1_2124.csv")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#battles_test2 <- spacetime(Battles_admin1_2124, mmr_shp_mimu_1,
                   #.loc_col = "ST",
                   #.time_col = "year")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#is_spacetime_cube(battles_test2)
```

## Using `complete_spacetime_cube()`

As per above, we have an incomplete space time cube due to missing districts/regions, with no incidents in some years.

According to [Josiah Perry](https://sfdep.josiahparry.com/articles/spacetime-s3.html), if an object is a **sparse** spatio-temporal grid, we can make it a full one using `complete_spacetime_cube()`. This works similarly to \[tidyr::complete()\].

`complete_spacetime_cube()` ensures that there is a row for each combination of location and time. New rows will contain missing values

```{r}
spt1_complete <- complete_spacetime_cube(battles_spacial1)
```

```{r}
is_spacetime_cube(spt1_complete)
```

```{r}
spt2_complete <- complete_spacetime_cube(battles_spacial2)
```

```{r}
is_spacetime_cube(spt2_complete)
```

We have successfull converted to a space time object. However using `complete_spacetime_cube()` , leaves zero values as NAs, this needs to be converted to '0' value.

```{r}
# Replace NA with zero for the incidents where there were no incidents
spt1_complete$Incidents[is.na(spt1_complete$Incidents)] <- 0
spt2_complete$Incidents[is.na(spt2_complete$Incidents)] <- 0

```

```{r}
class(spt1_complete)
```

```{r}
class(spt2_complete)
```

# Computing Gi\*

Next, we will compute the local Gi\* statistics.

### Deriving the spatial weights

The code below will be used to identify neighbors and to derive an inverse distance weights.

```{r}
# for admin 1 regions/states
Incidents1_nb <- spt1_complete %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

```{r}
#for admin 2 districts
Incidents2_nb <- spt2_complete %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

::: callout-note
## Note

-   `activate()` of dplyr package is used to activate the geometry context

-   `mutate()` of dplyr package is used to create two new columns *nb* and *wt*.

-   Then we will activate the data context again and copy over the nb and wt columns to each time-slice using `set_nbs()` and `set_wts()`

    -   row order is very important so do not rearrange the observations after using `set_nbs()` or `set_wts()`.
:::

Note that the data sets now have neighbors and weights for each time-slice.

```{r}
head(Incidents1_nb)
```

```{r}
head(Incidents2_nb)
```

## Computing Gi\*

We can use these new columns to manually calculate the local Gi\* for each location. We can do this by grouping by *year* and using `local_gstar_perm()` of sfdep package. After which, we `use unnest()` to unnest *gi_star* column of the newly created *gi_starts* data.frame.

```{r}
#for admin 1
gi_stars1 <- Incidents1_nb %>% 
  group_by(year) %>% 
  mutate(gi_star = local_gstar_perm(
    Incidents, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

```{r}
#for admin 2
gi_stars2 <- Incidents2_nb %>% 
  group_by(year) %>% 
  mutate(gi_star = local_gstar_perm(
    Incidents, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

# Mann-Kendall Test

With these Gi\* measures we can then evaluate each location for a trend using the Mann-Kendall test.

The code chunk below uses Ayeyarwady region.

```{r}
cbg1 <- gi_stars1 %>% 
  ungroup() %>% 
  filter(ST == "Ayeyarwady") |> 
  select(ST, year, gi_star)
```

The code chunk below uses Hinthada region.

```{r}
cbg2 <- gi_stars2 %>% 
  ungroup() %>% 
  filter(DT == "Hinthada") |> 
  select(DT, year, gi_star)
```

Next, we plot the result by using ggplotly() of plotly package.

**Ayeyarwady region**

```{r}
p <- ggplot(data = cbg1, 
       aes(x = year, 
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

**Hinthada district**

```{r}
p <- ggplot(data = cbg2, 
       aes(x = year, 
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

Mann Kendall test for **Ayeyarwady region**

```{r}
cbg1 %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

Mann Kendall test for **Hinthada district**

```{r}
cbg2 %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

Values of Mann Kendall test.

|        |                         |
|--------|-------------------------|
| `tau`  | Kendall's tau statistic |
| `sl`   | two-sided p-value       |
| `S`    | Kendall Score           |
| `D`    | denominator, tau=S/D    |
| `varS` | variance of S           |

We can replicate this for each location by using `group_by()` of dplyr package.

**Admin 1 regions/states**

```{r}
ehsa1 <- gi_stars1 %>%
  group_by(ST) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

```{r}
ehsa1
```

**Admin 2 districts**

```{r}
ehsa2 <- gi_stars2 %>%
  group_by(DT) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

```{r}
ehsa2
```

## Arrange to show significant emerging hot/cold spots

**Admin 1 regions/states**

```{r}
emerging1 <- ehsa1 %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)
```

```{r}
emerging1
```

Admin 2 districts

```{r}
emerging2 <- ehsa2 %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)
```

```{r}
emerging2
```

## Performing Emerging Hotspot Analysis

Lastly, we will perform EHSA analysis by using [`emerging_hotspot_analysis()`](https://sfdep.josiahparry.com/reference/emerging_hotspot_analysis.html) of sfdep package. It takes a spacetime object x (i.e. spt1_complete, sp2_complete), and the quoted name of the variable of interest (i.e. Incidents) for .var argument.

The **k argument is used to specify the** **number of time lags** which is set to 1 by default.

Lastly, **nsim map** **numbers of simulation** to be performed.

```{r}
ehsa1 <- emerging_hotspot_analysis(
  x = spt1_complete, 
  .var = "Incidents", 
  k = 1, 
  nsim = 99
)
```

```{r}
ehsa2 <- emerging_hotspot_analysis(
  x = spt2_complete, 
  .var = "Incidents", 
  k = 1, 
  nsim = 99
)
```

### Visualising the distribution of EHSA classes

In the code chunk below, ggplot2 functions is used to reveal the distribution of EHSA classes as a bar chart.

**Admin 1 regions/states**

```{r}
ggplot(data = ehsa1,
       aes(x = classification)) +
  geom_bar()
```

**Admin2 districts**

```{r}
#| fig-width: 10
#| fig-height: 7
#| column: body-outset-right

ggplot(data = ehsa2,
       aes(x = classification)) +
  geom_bar()
```

### Visualising EHSA

In this section, we will learn how to visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both (mmr_shp_mimu_1 & *ehsa1) and* (mmr_shp_mimu_2 & *ehsa2)* together by using the code chunk below.

```{r}
mmr1_ehsa <- mmr_shp_mimu_1 %>%
  left_join(ehsa1,
            by = join_by(ST == location))

mmr2_ehsa <- mmr_shp_mimu_2 %>%
  left_join(ehsa2,
            by = join_by(DT == location))
```

Next, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.

```{r}
#| fig-width: 10
#| fig-height: 7
#| column: body-outset-right

ehsa_sig1 <- mmr1_ehsa  %>%
  filter(p_value < 0.05)

tmap_mode("plot")

tm_shape(mmr1_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig1) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```

```{r}
#| fig-width: 10
#| fig-height: 7
#| column: body-outset-right

ehsa_sig2 <- mmr2_ehsa  %>%
  filter(p_value < 0.05)

tmap_mode("plot")

tm_shape(mmr2_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig2) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```
